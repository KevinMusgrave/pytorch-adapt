{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1588e700",
      "metadata": {
        "id": "1588e700",
        "outputId": "1c95adf3-fd86-4099-fa7e-d4ad293bfa72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-adapt\n",
            "  Downloading pytorch_adapt-0.0.61-py3-none-any.whl (137 kB)\n",
            "\u001b[K     |████████████████████████████████| 137 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting torchmetrics\n",
            "  Downloading torchmetrics-0.7.2-py3-none-any.whl (397 kB)\n",
            "\u001b[K     |████████████████████████████████| 397 kB 47.0 MB/s \n",
            "\u001b[?25hCollecting pytorch-metric-learning>=1.1.0\n",
            "  Downloading pytorch_metric_learning-1.2.0-py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 53.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-adapt) (1.21.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-adapt) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pytorch-adapt) (0.11.1+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning>=1.1.0->pytorch-adapt) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning>=1.1.0->pytorch-adapt) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-adapt) (3.10.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning>=1.1.0->pytorch-adapt) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning>=1.1.0->pytorch-adapt) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning>=1.1.0->pytorch-adapt) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics->pytorch-adapt) (21.3)\n",
            "Collecting pyDeprecate==0.3.*\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics->pytorch-adapt) (3.0.7)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch-adapt) (7.1.2)\n",
            "Installing collected packages: pyDeprecate, torchmetrics, pytorch-metric-learning, pytorch-adapt\n",
            "Successfully installed pyDeprecate-0.3.2 pytorch-adapt-0.0.61 pytorch-metric-learning-1.2.0 torchmetrics-0.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-adapt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e26cc1b",
      "metadata": {
        "id": "1e26cc1b"
      },
      "source": [
        "### Create some fake data and models\n",
        "\n",
        "Model names:\n",
        "- G: feature generator\n",
        "- C: classifier\n",
        "- D: discriminator (for adversarial methods)\n",
        "\n",
        "Data names:\n",
        "- src_imgs/target_imgs: source or target data. The ```_imgs``` suffix is misleading, as the data doesn't have to be 2d, so this will probably be changed in a future version of the library.\n",
        "- src_labels: class labels for the source data.\n",
        "- src_domain/target_domain: integers representing the source and target domain. The convention is 0 for source, and 1 for target.\n",
        "- src_sample_idx/target_sample_idx: each sample's index in the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "11c9c78b",
      "metadata": {
        "id": "11c9c78b"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "import torch\n",
        "\n",
        "from pytorch_adapt.containers import Models, Optimizers\n",
        "from pytorch_adapt.hooks import validate_hook\n",
        "\n",
        "feature_size = 100\n",
        "G = torch.nn.Linear(1000, feature_size)\n",
        "C = torch.nn.Linear(feature_size, 10)\n",
        "D = torch.nn.Sequential(torch.nn.Linear(feature_size, 1), torch.nn.Flatten(start_dim=0))\n",
        "\n",
        "models = Models({\"G\": G, \"C\": C, \"D\": D})\n",
        "optimizers = Optimizers((torch.optim.Adam, {\"lr\": 0.00001}))\n",
        "optimizers.create_with(models)\n",
        "opts = list(optimizers.values())\n",
        "\n",
        "\n",
        "dataset_size = 10000\n",
        "# one batch worth of \"data\"\n",
        "data = {\n",
        "    \"src_imgs\": torch.randn(32, 1000),\n",
        "    \"target_imgs\": torch.randn(32, 1000),\n",
        "    \"src_labels\": torch.randint(0, 10, size=(32,)),\n",
        "    \"src_domain\": torch.zeros(32),\n",
        "    \"target_domain\": torch.ones(32),\n",
        "    \"src_sample_idx\": torch.randint(0, dataset_size, size=(32,)),\n",
        "    \"target_sample_idx\": torch.randint(0, dataset_size, size=(32,)),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55877506",
      "metadata": {
        "id": "55877506"
      },
      "source": [
        "### Register PyTorch forward hooks for demonstration\n",
        "\n",
        "This will keep track of how many times each model is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2c35c798",
      "metadata": {
        "id": "2c35c798",
        "outputId": "d97d0663-7dd8-4dd2-bbf0-9e1eed1376ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.hooks.RemovableHandle at 0x7efe22d23b10>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "def forward_count(self, *_):\n",
        "    self.count += 1\n",
        "\n",
        "\n",
        "G.register_forward_hook(forward_count)\n",
        "C.register_forward_hook(forward_count)\n",
        "D.register_forward_hook(forward_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5839570a",
      "metadata": {
        "id": "5839570a"
      },
      "source": [
        "### Helper function for this demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3ec3e0d1",
      "metadata": {
        "id": "3ec3e0d1"
      },
      "outputs": [],
      "source": [
        "def print_info(model_counts, outputs, losses, G, C, D=None):\n",
        "    def get_shape(v):\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            return v.shape\n",
        "        elif isinstance(v, list):\n",
        "            return [z.shape for z in v]\n",
        "\n",
        "    print(f\"Expected model counts = {dict(model_counts)}\")\n",
        "    true_str = f\"True model counts = G: {G.count}, C: {C.count}\"\n",
        "    if D:\n",
        "        true_str += f\", D: {D.count}\"\n",
        "    print(true_str)\n",
        "    pprint(losses)\n",
        "    pprint({k: get_shape(v) for k, v in outputs.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62eb92ee",
      "metadata": {
        "id": "62eb92ee"
      },
      "source": [
        "### Source Classifier\n",
        "\n",
        "This hook applies a cross entropy loss on the source data, so it requires source logits to be computed. \n",
        "\n",
        "Therefore, each model (G and C) will be used once:\n",
        "```src_features_logits = C(G(src_imgs))```.\n",
        "\n",
        "We can use ```validate_hook``` to verify that the hook will work with the given data. This function also returns the expected number of times each model will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "09080c17",
      "metadata": {
        "id": "09080c17",
        "outputId": "fcdbb156-65d7-40a0-b0f9-3820f90af052",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected model counts = {'G': 1, 'C': 1}\n",
            "True model counts = G: 1, C: 1\n",
            "{'total_loss': {'c_loss': 2.2329823970794678, 'total': 2.2329823970794678}}\n",
            "{'src_imgs_features': torch.Size([32, 100]),\n",
            " 'src_imgs_features_logits': torch.Size([32, 10])}\n"
          ]
        }
      ],
      "source": [
        "from pytorch_adapt.hooks import ClassifierHook\n",
        "\n",
        "# Reset counts\n",
        "G.count, C.count = 0, 0\n",
        "hook = ClassifierHook(opts)\n",
        "model_counts = validate_hook(hook, list(data.keys()))\n",
        "outputs, losses = hook({**models, **data})\n",
        "print_info(model_counts, outputs, losses, G, C)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cade585",
      "metadata": {
        "id": "0cade585"
      },
      "source": [
        "### Source Classifier + BSP + BNM\n",
        "\n",
        "Now we'll use the same ```ClassifierHook``` but add some hooks that are useful for domain adaptation.\n",
        "\n",
        "The ```BSPHook``` requires source and target features: \n",
        "\n",
        "- ```src_features = G(src_imgs)```\n",
        "\n",
        "- ```target_features = G(target_imgs)```\n",
        "\n",
        "The ```BNMHook``` requires target logits: ```target_features_logits = C(target_features)```\n",
        "\n",
        "The source logits still need to be computed for the source classification loss. So in total, each model will be used twice.\n",
        "\n",
        "To use these hooks, we pass them as a list into the ```post``` argument. This means that the losses will be computed in the following order: classification, BSP, BNM. The ```ClassifierHook``` takes in optimizers as its first argument, so after the loss is computed, it also computes gradients and updates model weights.\n",
        "\n",
        "The BSP loss tends to be very large, so we add a ```MeanWeighter```. This multiplies each loss by a scalar (1 by default), and then returns the mean of the scaled losses. In this case, we change the weight for ```bsp_loss``` to ```1e-5```.\n",
        "\n",
        "The hook outputs two dictionaries:\n",
        "\n",
        "- losses: a two-level dictionary where the outer level is associated with a particular optimization step (relevant for GAN architectures), and the inner level contains the loss components.\n",
        "- outputs: all the data that was generated by models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c4aa2a9f",
      "metadata": {
        "id": "c4aa2a9f",
        "outputId": "61288b54-fd21-4e11-83a5-e7ae1ef951dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected model counts = {'G': 2, 'C': 2}\n",
            "True model counts = G: 2, C: 2\n",
            "{'total_loss': {'bnm_loss': -0.10542324930429459,\n",
            "                'bsp_loss': 0.0017005496192723513,\n",
            "                'c_loss': 2.225362777709961,\n",
            "                'total': 0.7072134017944336}}\n",
            "{'src_imgs_features': torch.Size([32, 100]),\n",
            " 'src_imgs_features_logits': torch.Size([32, 10]),\n",
            " 'target_imgs_features': torch.Size([32, 100]),\n",
            " 'target_imgs_features_logits': torch.Size([32, 10])}\n"
          ]
        }
      ],
      "source": [
        "from pytorch_adapt.hooks import BNMHook, BSPHook\n",
        "from pytorch_adapt.weighters import MeanWeighter\n",
        "\n",
        "# Reset counts\n",
        "G.count, C.count = 0, 0\n",
        "weighter = MeanWeighter(weights={\"bsp_loss\": 1e-5})\n",
        "hook = ClassifierHook(opts, post=[BSPHook(), BNMHook()], weighter=weighter)\n",
        "model_counts = validate_hook(hook, list(data.keys()))\n",
        "outputs, losses = hook({**models, **data})\n",
        "print_info(model_counts, outputs, losses, G, C)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b16dbf50",
      "metadata": {
        "id": "b16dbf50"
      },
      "source": [
        "### DANN\n",
        "\n",
        "Let's try DANN next. DANN uses a discriminator that tries to distinguish between source and target features. The required data for computing the adversarial loss is:\n",
        "\n",
        "- ```src_features = G(src_imgs)```\n",
        "- ```target_features = G(target_imgs)```\n",
        "- ```src_features_dlogits = D(src_features)```\n",
        "- ```target_features_dlogits = D(target_features)```\n",
        "\n",
        "The ```_dlogits``` suffix represents the output of the discriminator model. In addition to these outputs, DANN uses a classification loss on source data:\n",
        "\n",
        "- ```src_features_logits = C(src_features)```\n",
        "\n",
        "Based on these requirements, the model counts should be G:2, D:2, C:1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ac4dbf48",
      "metadata": {
        "id": "ac4dbf48",
        "outputId": "2211dd70-4285-49a0-ac75-5bb5060eb8da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected model counts = {'G': 2, 'D': 2, 'C': 1}\n",
            "True model counts = G: 2, C: 1, D: 2\n",
            "{'total_loss': {'c_loss': 2.218740224838257,\n",
            "                'src_domain_loss': 0.6359639167785645,\n",
            "                'target_domain_loss': 0.7587134838104248,\n",
            "                'total': 1.204472541809082}}\n",
            "{'src_imgs_features': torch.Size([32, 100]),\n",
            " 'src_imgs_features_dlogits': torch.Size([32]),\n",
            " 'src_imgs_features_logits': torch.Size([32, 10]),\n",
            " 'target_imgs_features': torch.Size([32, 100]),\n",
            " 'target_imgs_features_dlogits': torch.Size([32])}\n"
          ]
        }
      ],
      "source": [
        "from pytorch_adapt.hooks import DANNHook\n",
        "\n",
        "G.count, C.count, D.count = 0, 0, 0\n",
        "hook = DANNHook(opts)\n",
        "model_counts = validate_hook(hook, list(data.keys()))\n",
        "outputs, losses = hook({**models, **data})\n",
        "print_info(model_counts, outputs, losses, G, C, D)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84fa74fd",
      "metadata": {
        "id": "84fa74fd"
      },
      "source": [
        "### DANN + MCC + ATDOC\n",
        "\n",
        "Now we'll add two hooks to DANN:\n",
        "\n",
        "- ```MCCHook``` requires target logits. This isn't normally required by DANN, so the count for C should increase by 1.\n",
        "- ```ATDOCHook``` requires source features and logits. These are already required by DANN, so the count for G and C should remain the same.\n",
        "\n",
        "We pass these hooks into the ```post_g``` argument, because we want them to use raw source and target features. (If you passed them in as ```post_d``` then they would use the output of the gradient reversal layer, which we don't want in this case.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a77daa99",
      "metadata": {
        "id": "a77daa99",
        "outputId": "e5d2e68e-6d46-4514-b7d7-34c988acbfc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected model counts = {'G': 2, 'D': 2, 'C': 2}\n",
            "True model counts = G: 2, C: 2, D: 2\n",
            "{'total_loss': {'c_loss': 2.212649345397949,\n",
            "                'mcc_loss': 0.8906670808792114,\n",
            "                'pseudo_label_loss': 0.24055415391921997,\n",
            "                'src_domain_loss': 0.636552095413208,\n",
            "                'target_domain_loss': 0.7594033479690552,\n",
            "                'total': 0.9479652643203735}}\n",
            "{'src_imgs_features': torch.Size([32, 100]),\n",
            " 'src_imgs_features_dlogits': torch.Size([32]),\n",
            " 'src_imgs_features_logits': torch.Size([32, 10]),\n",
            " 'target_imgs_features': torch.Size([32, 100]),\n",
            " 'target_imgs_features_dlogits': torch.Size([32]),\n",
            " 'target_imgs_features_logits': torch.Size([32, 10])}\n"
          ]
        }
      ],
      "source": [
        "from pytorch_adapt.hooks import ATDOCHook, MCCHook\n",
        "\n",
        "G.count, C.count, D.count = 0, 0, 0\n",
        "mcc = MCCHook()\n",
        "atdoc = ATDOCHook(dataset_size=dataset_size, feature_dim=100, num_classes=10)\n",
        "\n",
        "hook = DANNHook(opts, post_g=[mcc, atdoc])\n",
        "model_counts = validate_hook(hook, list(data.keys()))\n",
        "outputs, losses = hook({**models, **data})\n",
        "print_info(model_counts, outputs, losses, G, C, D)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c944e724",
      "metadata": {
        "id": "c944e724"
      },
      "source": [
        "### CDAN\n",
        "\n",
        "The ```CDANHook``` is adversarial like ```DANNHook```, but it doesn't use a gradient reversal layer. Thus, optimization occurs in two steps: one for updating the generator, and one for updating the discriminator. In each step, the discriminator has to recompute its logits, so it will be used 4 times instead of 2.\n",
        "\n",
        "```CDANHook``` also requires a separate ```feature_combiner``` model that we pass in along with all the other models and data.\n",
        "\n",
        "You'll notice the outputs have different names from DANN's outputs:\n",
        "\n",
        "- All of the ```feature_combiner``` outputs contain the ```_combined``` suffix, as well as the names of the tensors that were combined. \n",
        "- Tensors with the ```_detached``` suffix are detached from the autograd graph. This is done during the discriminator update, to avoid computing gradients for the generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "405957d8",
      "metadata": {
        "id": "405957d8",
        "outputId": "dc0ed12e-78b7-49c5-f436-44e648a18d91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected model counts = {'G': 2, 'C': 2, 'feature_combiner': 2, 'D': 4}\n",
            "True model counts = G: 2, C: 2, D: 4\n",
            "{'d_loss': {'d_src_domain_loss': 0.6567280888557434,\n",
            "            'd_target_domain_loss': 0.7346022725105286,\n",
            "            'total': 0.695665180683136},\n",
            " 'g_loss': {'c_loss': 2.2070629596710205,\n",
            "            'g_src_domain_loss': 0.7336423397064209,\n",
            "            'g_target_domain_loss': 0.6560751795768738,\n",
            "            'total': 1.1989268064498901}}\n",
            "{'src_imgs_features': torch.Size([32, 100]),\n",
            " 'src_imgs_features_AND_src_imgs_features_logits_combined': torch.Size([32, 100]),\n",
            " 'src_imgs_features_AND_src_imgs_features_logits_combined_detached': torch.Size([32, 100]),\n",
            " 'src_imgs_features_AND_src_imgs_features_logits_combined_detached_dlogits': torch.Size([32]),\n",
            " 'src_imgs_features_AND_src_imgs_features_logits_combined_dlogits': torch.Size([32]),\n",
            " 'src_imgs_features_logits': torch.Size([32, 10]),\n",
            " 'target_imgs_features': torch.Size([32, 100]),\n",
            " 'target_imgs_features_AND_target_imgs_features_logits_combined': torch.Size([32, 100]),\n",
            " 'target_imgs_features_AND_target_imgs_features_logits_combined_detached': torch.Size([32, 100]),\n",
            " 'target_imgs_features_AND_target_imgs_features_logits_combined_detached_dlogits': torch.Size([32]),\n",
            " 'target_imgs_features_AND_target_imgs_features_logits_combined_dlogits': torch.Size([32]),\n",
            " 'target_imgs_features_logits': torch.Size([32, 10])}\n"
          ]
        }
      ],
      "source": [
        "from pytorch_adapt.hooks import CDANHook\n",
        "from pytorch_adapt.layers import RandomizedDotProduct\n",
        "from pytorch_adapt.utils import common_functions as c_f\n",
        "\n",
        "G.count, C.count, D.count = 0, 0, 0\n",
        "d_opts = opts[2:]\n",
        "g_opts = opts[:2]\n",
        "misc = {\"feature_combiner\": RandomizedDotProduct([feature_size, 10], feature_size)}\n",
        "\n",
        "hook = CDANHook(d_opts=d_opts, g_opts=g_opts)\n",
        "model_counts = validate_hook(hook, list(data.keys()))\n",
        "outputs, losses = hook({**models, **misc, **data})\n",
        "print_info(model_counts, outputs, losses, G, C, D)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51f14fe2",
      "metadata": {
        "id": "51f14fe2"
      },
      "source": [
        "### CDAN + VAT\n",
        "\n",
        "Here we present a current failure case of ```validate_hook```. \n",
        "\n",
        "- The ```VATHook``` uses ```VATLoss```, and inside of ```VATLoss```, the ```combined_model``` is used twice. \n",
        "- ```VATHook``` uses ```VATLoss``` twice, so the ```combined_model``` is used a total of 4 times. \n",
        "- However, there is no way for ```validate_hook``` to know this, so its estimates for G and C are off by 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "025090e6",
      "metadata": {
        "id": "025090e6",
        "outputId": "df497a87-fed8-4acc-a4be-17e3d1259f0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected model counts = {'G': 2, 'C': 2, 'feature_combiner': 2, 'D': 4}\n",
            "True model counts = G: 6, C: 6, D: 4\n",
            "{'d_loss': {'d_src_domain_loss': 0.6568478941917419,\n",
            "            'd_target_domain_loss': 0.7346765995025635,\n",
            "            'total': 0.6957622766494751},\n",
            " 'g_loss': {'c_loss': 2.201413154602051,\n",
            "            'g_src_domain_loss': 0.7335168123245239,\n",
            "            'g_target_domain_loss': 0.6560076475143433,\n",
            "            'src_vat_loss': 0.3102036118507385,\n",
            "            'target_vat_loss': 0.29932552576065063,\n",
            "            'total': 0.8400933146476746}}\n",
            "{'src_imgs_features': torch.Size([32, 100]),\n",
            " 'src_imgs_features_AND_src_imgs_features_logits_combined': torch.Size([32, 100]),\n",
            " 'src_imgs_features_AND_src_imgs_features_logits_combined_detached': torch.Size([32, 100]),\n",
            " 'src_imgs_features_AND_src_imgs_features_logits_combined_detached_dlogits': torch.Size([32]),\n",
            " 'src_imgs_features_AND_src_imgs_features_logits_combined_dlogits': torch.Size([32]),\n",
            " 'src_imgs_features_logits': torch.Size([32, 10]),\n",
            " 'target_imgs_features': torch.Size([32, 100]),\n",
            " 'target_imgs_features_AND_target_imgs_features_logits_combined': torch.Size([32, 100]),\n",
            " 'target_imgs_features_AND_target_imgs_features_logits_combined_detached': torch.Size([32, 100]),\n",
            " 'target_imgs_features_AND_target_imgs_features_logits_combined_detached_dlogits': torch.Size([32]),\n",
            " 'target_imgs_features_AND_target_imgs_features_logits_combined_dlogits': torch.Size([32]),\n",
            " 'target_imgs_features_logits': torch.Size([32, 10])}\n"
          ]
        }
      ],
      "source": [
        "from pytorch_adapt.hooks import VATHook\n",
        "\n",
        "G.count, C.count, D.count = 0, 0, 0\n",
        "misc[\"combined_model\"] = torch.nn.Sequential(G, C)\n",
        "hook = CDANHook(d_opts=d_opts, g_opts=g_opts, post_g=[VATHook()])\n",
        "model_counts = validate_hook(hook, list(data.keys()))\n",
        "outputs, losses = hook({**models, **misc, **data})\n",
        "print_info(model_counts, outputs, losses, G, C, D)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e2b116",
      "metadata": {
        "id": "01e2b116"
      },
      "source": [
        "### MCD\n",
        "\n",
        "```MCDHook``` is an adversarial method with 3 optimization steps. The steps are run independently of each other, so all required tensors have to be recomputed in each step.\n",
        "\n",
        "1. Classification loss on source data. Model count = G:1, C:1\n",
        "2. Classification loss + adversarial loss on target logits. Model count = G:2, C:2\n",
        "3. Advesrial loss on target logits, repeated N times. Model count = G:N, C:N\n",
        "\n",
        "The default value of N is 4. So the total model count should be G:7, C:7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "79148420",
      "metadata": {
        "id": "79148420",
        "outputId": "b0517377-9113-4702-db71-0ee6f702bc7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected model counts = {'G': 7, 'C': 7}\n",
            "True model counts = G: 7, C: 7\n",
            "{'x_loss': {'c_loss0': 2.196049928665161,\n",
            "            'c_loss1': 2.2937872409820557,\n",
            "            'total': 2.2449185848236084},\n",
            " 'y_loss': {'c_loss0': 2.190664529800415,\n",
            "            'c_loss1': 2.292001724243164,\n",
            "            'discrepancy_loss': -0.034854356199502945,\n",
            "            'total': 1.482603907585144},\n",
            " 'z_loss': {'discrepancy_loss': 0.03478754311800003,\n",
            "            'total': 0.03478754311800003}}\n",
            "{'src_imgs_features': torch.Size([32, 100]),\n",
            " 'src_imgs_features_detached': torch.Size([32, 100]),\n",
            " 'src_imgs_features_detached_logits': [torch.Size([32, 10]),\n",
            "                                       torch.Size([32, 10])],\n",
            " 'src_imgs_features_logits': [torch.Size([32, 10]), torch.Size([32, 10])],\n",
            " 'target_imgs_features': torch.Size([32, 100]),\n",
            " 'target_imgs_features_detached': torch.Size([32, 100]),\n",
            " 'target_imgs_features_detached_logits': [torch.Size([32, 10]),\n",
            "                                          torch.Size([32, 10])],\n",
            " 'target_imgs_features_logits': [torch.Size([32, 10]), torch.Size([32, 10])]}\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "from pytorch_adapt.hooks import MCDHook\n",
        "from pytorch_adapt.layers import MultipleModels\n",
        "\n",
        "C2 = c_f.reinit(copy.deepcopy(C))\n",
        "C_multiple = MultipleModels(C, C2)\n",
        "models[\"C\"] = C_multiple\n",
        "\n",
        "C_multiple.register_forward_hook(forward_count)\n",
        "G.count, C_multiple.count = 0, 0\n",
        "g_opts = opts[0:1]\n",
        "c_opts = opts[1:2]\n",
        "\n",
        "hook = MCDHook(g_opts=g_opts, c_opts=c_opts)\n",
        "model_counts = validate_hook(hook, list(data.keys()))\n",
        "outputs, losses = hook({**models, **data})\n",
        "print_info(model_counts, outputs, losses, G, C_multiple)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ca090d",
      "metadata": {
        "id": "03ca090d"
      },
      "source": [
        "### MCD + AFN + MMD\n",
        "\n",
        "Here we'll add two hooks:\n",
        "\n",
        "- ```AFNHook``` requires source and target features. We're placing this in step 1 of MCD (```post_x```). Step 1 already computes source features, but not target features, so the model count for G should increase by 1.\n",
        "- ```AlignerHook``` requires source and target features. We're placing this in step 3 of MCD (```post_z```). Step 3 already computes target features, but not source features, so the model count for G should increase by N.\n",
        "\n",
        "In total, the count for G should be 7 + 1 + N = 12 (where N = 4 by default)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b1d27961",
      "metadata": {
        "id": "b1d27961",
        "outputId": "434eedac-3c5e-422b-f98e-d86b7359173f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected model counts = {'G': 12, 'C': 7}\n",
            "True model counts = G: 12, C: 7\n",
            "{'x_loss': {'afn_loss': 2.0,\n",
            "            'c_loss0': 2.174976110458374,\n",
            "            'c_loss1': 2.2866954803466797,\n",
            "            'total': 2.153890609741211},\n",
            " 'y_loss': {'c_loss0': 2.1721420288085938,\n",
            "            'c_loss1': 2.285510778427124,\n",
            "            'discrepancy_loss': -0.03475061058998108,\n",
            "            'total': 1.4743008613586426},\n",
            " 'z_loss': {'discrepancy_loss': 0.03473714366555214,\n",
            "            'features_confusion_loss': -0.02522990107536316,\n",
            "            'total': 0.00475362129509449}}\n",
            "{'src_imgs_features': torch.Size([32, 100]),\n",
            " 'src_imgs_features_detached': torch.Size([32, 100]),\n",
            " 'src_imgs_features_detached_logits': [torch.Size([32, 10]),\n",
            "                                       torch.Size([32, 10])],\n",
            " 'src_imgs_features_logits': [torch.Size([32, 10]), torch.Size([32, 10])],\n",
            " 'target_imgs_features': torch.Size([32, 100]),\n",
            " 'target_imgs_features_detached': torch.Size([32, 100]),\n",
            " 'target_imgs_features_detached_logits': [torch.Size([32, 10]),\n",
            "                                          torch.Size([32, 10])],\n",
            " 'target_imgs_features_logits': [torch.Size([32, 10]), torch.Size([32, 10])]}\n"
          ]
        }
      ],
      "source": [
        "from pytorch_adapt.hooks import AFNHook, AlignerHook\n",
        "\n",
        "G.count, C_multiple.count = 0, 0\n",
        "hook = MCDHook(g_opts=g_opts, c_opts=c_opts, post_x=[AFNHook()], post_z=[AlignerHook()])\n",
        "model_counts = validate_hook(hook, list(data.keys()))\n",
        "outputs, losses = hook({**models, **data})\n",
        "print_info(model_counts, outputs, losses, G, C_multiple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8f888fa6",
      "metadata": {
        "id": "8f888fa6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "CustomizingAlgorithms.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}