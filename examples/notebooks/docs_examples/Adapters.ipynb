{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-adapt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a605ea8c",
   "metadata": {},
   "source": [
    "### Helper function and data for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21930ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pytorch_adapt.utils.common_functions import get_lr\n",
    "\n",
    "\n",
    "def print_optimizers_slim(adapter):\n",
    "    for k, v in adapter.optimizers.items():\n",
    "        print(f\"{k}: {v.__class__.__name__} with lr={get_lr(v)}\")\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"src_imgs\": torch.randn(32, 1000),\n",
    "    \"target_imgs\": torch.randn(32, 1000),\n",
    "    \"src_labels\": torch.randint(0, 10, size=(32,)),\n",
    "    \"src_domain\": torch.zeros(32),\n",
    "    \"target_domain\": torch.zeros(32),\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2c7af",
   "metadata": {},
   "source": [
    "### Adapters Initialization\n",
    "\n",
    "Models are usually the only required argument when initializing adapters. Optimizers are created using the default that is defined in the adapter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9667c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_adapt.adapters import DANN\n",
    "from pytorch_adapt.containers import Models\n",
    "\n",
    "G = torch.nn.Linear(1000, 100)\n",
    "C = torch.nn.Linear(100, 10)\n",
    "D = torch.nn.Sequential(torch.nn.Linear(100, 1), torch.nn.Flatten(start_dim=0))\n",
    "models = Models({\"G\": G, \"C\": C, \"D\": D})\n",
    "\n",
    "adapter = DANN(models=models)\n",
    "print_optimizers_slim(adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07774e88",
   "metadata": {},
   "source": [
    "### Modifying optimizers using the Optimizers container\n",
    "\n",
    "We can use the Optimizers container if we don't want to use the defaults.\n",
    "\n",
    "For example: SGD with lr 0.1 for all 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc771fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_adapt.containers import Optimizers\n",
    "\n",
    "optimizers = Optimizers((torch.optim.SGD, {\"lr\": 0.1}))\n",
    "adapter = DANN(models=models, optimizers=optimizers)\n",
    "print_optimizers_slim(adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5293a9d",
   "metadata": {},
   "source": [
    "SGD with lr 0.1 for the G and C models only. The default optimizer will be used for D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = Optimizers((torch.optim.SGD, {\"lr\": 0.1}), keys=[\"G\", \"C\"])\n",
    "adapter = DANN(models=models, optimizers=optimizers)\n",
    "print_optimizers_slim(adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fdabd",
   "metadata": {},
   "source": [
    "SGD with lr 0.1 for G, and SGD with lr 0.5 for C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b0ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = Optimizers(\n",
    "    {\"G\": (torch.optim.SGD, {\"lr\": 0.1}), \"C\": (torch.optim.SGD, {\"lr\": 0.5})}\n",
    ")\n",
    "adapter = DANN(models=models, optimizers=optimizers)\n",
    "print_optimizers_slim(adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d57a5",
   "metadata": {},
   "source": [
    "You can also create the optimizers yourself and pass them into the Optimizers container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a17df",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = Optimizers({\"G\": torch.optim.SGD(G.parameters(), lr=0.123)})\n",
    "adapter = DANN(models=models, optimizers=optimizers)\n",
    "print_optimizers_slim(adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d62e48",
   "metadata": {},
   "source": [
    "### Adding LR Schedulers\n",
    "\n",
    "LR schedulers can be added with the LRSchedulers container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd34b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_adapt.containers import LRSchedulers\n",
    "\n",
    "optimizers = Optimizers((torch.optim.Adam, {\"lr\": 1}))\n",
    "lr_schedulers = LRSchedulers(\n",
    "    {\n",
    "        \"G\": (torch.optim.lr_scheduler.ExponentialLR, {\"gamma\": 0.99}),\n",
    "        \"C\": (torch.optim.lr_scheduler.StepLR, {\"step_size\": 2}),\n",
    "    },\n",
    "    scheduler_types={\"per_step\": [\"G\"], \"per_epoch\": [\"C\"]},\n",
    ")\n",
    "adapter = DANN(models=models, optimizers=optimizers, lr_schedulers=lr_schedulers)\n",
    "print(adapter.lr_schedulers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502b0e05",
   "metadata": {},
   "source": [
    "If you don't wrap the adapter with a framework, then you have to step the lr schedulers manually as shown below.\n",
    "\n",
    "(Here we're just demonstrating how the lr scheduler container works, so we're stepping it without computing a loss or stepping the optimizers etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5902afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(4):\n",
    "    for i in range(5):\n",
    "        adapter.lr_schedulers.step(\"per_step\")\n",
    "    adapter.lr_schedulers.step(\"per_epoch\")\n",
    "    print(f\"End of epoch={epoch}\")\n",
    "    print_optimizers_slim(adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5558a4c9",
   "metadata": {},
   "source": [
    "### Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2b1773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_adapt.utils import common_functions as c_f\n",
    "\n",
    "adapter.models.to(device)\n",
    "data = c_f.batch_to_device(data, device)\n",
    "loss = adapter.training_step(data)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f96f0",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inference = torch.randn(32, 1000).to(device)\n",
    "features, logits = adapter.inference(data_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f751acd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
