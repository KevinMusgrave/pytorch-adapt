{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6c162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-adapt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a579af",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FakeData\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "x = FakeData(size=320, transform=ToTensor())\n",
    "y = FakeData(size=320, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bce91f",
   "metadata": {},
   "source": [
    "### Dataset Wrappers\n",
    "\n",
    "These wrappers transform datasets so that they are compatible with Adapters and Hooks.\n",
    "\n",
    "Notice that ```CombinedSourceAndTargetDataset``` returns the target sample corresponding with the input index, but returns a random source sample, even with the same input index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_adapt.datasets import (\n",
    "    CombinedSourceAndTargetDataset,\n",
    "    SourceDataset,\n",
    "    TargetDataset,\n",
    ")\n",
    "\n",
    "src = SourceDataset(x)\n",
    "target = TargetDataset(y)\n",
    "print(\"SourceDataset\", src)\n",
    "print(src[0].keys())\n",
    "\n",
    "print(\"\\nTargetDataset\", target)\n",
    "print(target[0].keys())\n",
    "\n",
    "src_target = CombinedSourceAndTargetDataset(src, target)\n",
    "print(\"\\nCombinedSourceAndTarget\", src_target)\n",
    "for _ in range(2):\n",
    "    retrieved = src_target[0]\n",
    "    print(\"src index\", retrieved[\"src_sample_idx\"])\n",
    "    print(\"target_index\", retrieved[\"target_sample_idx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35fe25",
   "metadata": {},
   "source": [
    "### Using CombinedSourceAndTargetDataset as input to hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pytorch_adapt.hooks import FeaturesHook\n",
    "\n",
    "models = {\"G\": torch.nn.Conv2d(3, 32, 3)}\n",
    "dataloader = torch.utils.data.DataLoader(src_target, batch_size=32)\n",
    "hook = FeaturesHook()\n",
    "\n",
    "for data in dataloader:\n",
    "    outputs, losses = hook({**models, **data})\n",
    "    print(outputs.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5546c80",
   "metadata": {},
   "source": [
    "### DataloaderCreator\n",
    "\n",
    "```DataloaderCreator``` is a factory class. It allows you to specify how dataloaders should be made for multiple datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_adapt.datasets import DataloaderCreator\n",
    "\n",
    "\n",
    "def print_dataloaders(dataloaders):\n",
    "    print({k: (v.batch_size, v.num_workers) for k, v in dataloaders.items()})\n",
    "\n",
    "\n",
    "# set the batch_size and num_workers for all datasets\n",
    "dc = DataloaderCreator(batch_size=64, num_workers=2)\n",
    "dataloaders = dc(train=src_target, src_train=src, target_train=target)\n",
    "print_dataloaders(dataloaders)\n",
    "\n",
    "# set different params for train vs val datasets\n",
    "dc = DataloaderCreator(\n",
    "    train_kwargs={\"batch_size\": 64, \"num_workers\": 2},\n",
    "    val_kwargs={\"batch_size\": 256, \"num_workers\": 4},\n",
    ")\n",
    "dataloaders = dc(train=src_target, src_val=src, target_val=target)\n",
    "print_dataloaders(dataloaders)\n",
    "\n",
    "# specify the name of the validation datasets\n",
    "dc = DataloaderCreator(\n",
    "    val_kwargs={\"batch_size\": 256, \"num_workers\": 4}, val_names=[\"val1\", \"val2\"]\n",
    ")\n",
    "dataloaders = dc(train=src_target, val1=src, val2=target)\n",
    "print_dataloaders(dataloaders)\n",
    "\n",
    "# consider all inputs to be validation datasets\n",
    "dc = DataloaderCreator(val_kwargs={\"batch_size\": 256, \"num_workers\": 4}, all_val=True)\n",
    "dataloaders = dc(train=src_target, val=src, woof=target)\n",
    "print_dataloaders(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdcc167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
