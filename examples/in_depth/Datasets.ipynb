{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0d6c162d",
      "metadata": {
        "id": "0d6c162d",
        "outputId": "ab603bb4-ccb3-4817-8d8f-5ff9be43dc05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-adapt\n",
            "  Downloading pytorch_adapt-0.0.61-py3-none-any.whl (137 kB)\n",
            "\u001b[K     |████████████████████████████████| 137 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-adapt) (1.10.0+cu111)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.7.2-py3-none-any.whl (397 kB)\n",
            "\u001b[K     |████████████████████████████████| 397 kB 12.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-adapt) (1.21.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pytorch-adapt) (0.11.1+cu111)\n",
            "Collecting pytorch-metric-learning>=1.1.0\n",
            "  Downloading pytorch_metric_learning-1.2.0-py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning>=1.1.0->pytorch-adapt) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning>=1.1.0->pytorch-adapt) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-adapt) (3.10.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning>=1.1.0->pytorch-adapt) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning>=1.1.0->pytorch-adapt) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning>=1.1.0->pytorch-adapt) (1.4.1)\n",
            "Collecting pyDeprecate==0.3.*\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics->pytorch-adapt) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics->pytorch-adapt) (3.0.7)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch-adapt) (7.1.2)\n",
            "Installing collected packages: pyDeprecate, torchmetrics, pytorch-metric-learning, pytorch-adapt\n",
            "Successfully installed pyDeprecate-0.3.2 pytorch-adapt-0.0.61 pytorch-metric-learning-1.2.0 torchmetrics-0.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-adapt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4a579af",
      "metadata": {
        "id": "c4a579af"
      },
      "source": [
        "### Create Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7e1d5e0c",
      "metadata": {
        "id": "7e1d5e0c"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import FakeData\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "x = FakeData(size=320, transform=ToTensor())\n",
        "y = FakeData(size=320, transform=ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5bce91f",
      "metadata": {
        "id": "e5bce91f"
      },
      "source": [
        "### Dataset Wrappers\n",
        "\n",
        "These wrappers transform datasets so that they are compatible with Adapters and Hooks.\n",
        "\n",
        "Notice that ```CombinedSourceAndTargetDataset``` returns the target sample corresponding with the input index, but returns a random source sample, even with the same input index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "eb9b778f",
      "metadata": {
        "id": "eb9b778f",
        "outputId": "b20f8a3b-f9da-49f6-875c-830d55c7b3ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SourceDataset SourceDataset(\n",
            "  domain=0\n",
            "  (dataset): Dataset FakeData\n",
            "      Number of datapoints: 320\n",
            "      StandardTransform\n",
            "  Transform: ToTensor()\n",
            ")\n",
            "dict_keys(['src_imgs', 'src_domain', 'src_labels', 'src_sample_idx'])\n",
            "\n",
            "TargetDataset TargetDataset(\n",
            "  domain=1\n",
            "  (dataset): Dataset FakeData\n",
            "      Number of datapoints: 320\n",
            "      StandardTransform\n",
            "  Transform: ToTensor()\n",
            ")\n",
            "dict_keys(['target_imgs', 'target_domain', 'target_sample_idx'])\n",
            "\n",
            "CombinedSourceAndTarget CombinedSourceAndTargetDataset(\n",
            "  (source_dataset): SourceDataset(\n",
            "    domain=0\n",
            "    (dataset): Dataset FakeData\n",
            "        Number of datapoints: 320\n",
            "        StandardTransform\n",
            "    Transform: ToTensor()\n",
            "  )\n",
            "  (target_dataset): TargetDataset(\n",
            "    domain=1\n",
            "    (dataset): Dataset FakeData\n",
            "        Number of datapoints: 320\n",
            "        StandardTransform\n",
            "    Transform: ToTensor()\n",
            "  )\n",
            ")\n",
            "src index 20\n",
            "target_index 0\n",
            "src index 252\n",
            "target_index 0\n"
          ]
        }
      ],
      "source": [
        "from pytorch_adapt.datasets import (\n",
        "    CombinedSourceAndTargetDataset,\n",
        "    SourceDataset,\n",
        "    TargetDataset,\n",
        ")\n",
        "\n",
        "src = SourceDataset(x)\n",
        "target = TargetDataset(y)\n",
        "print(\"SourceDataset\", src)\n",
        "print(src[0].keys())\n",
        "\n",
        "print(\"\\nTargetDataset\", target)\n",
        "print(target[0].keys())\n",
        "\n",
        "src_target = CombinedSourceAndTargetDataset(src, target)\n",
        "print(\"\\nCombinedSourceAndTarget\", src_target)\n",
        "for _ in range(2):\n",
        "    retrieved = src_target[0]\n",
        "    print(\"src index\", retrieved[\"src_sample_idx\"])\n",
        "    print(\"target_index\", retrieved[\"target_sample_idx\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb35fe25",
      "metadata": {
        "id": "eb35fe25"
      },
      "source": [
        "### Using CombinedSourceAndTargetDataset as input to hooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8329359f",
      "metadata": {
        "id": "8329359f",
        "outputId": "e08d0d8a-88e3-4d95-8f14-ab57d43889a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['src_imgs_features', 'target_imgs_features'])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "from pytorch_adapt.hooks import FeaturesHook\n",
        "\n",
        "models = {\"G\": torch.nn.Conv2d(3, 32, 3)}\n",
        "dataloader = torch.utils.data.DataLoader(src_target, batch_size=32)\n",
        "hook = FeaturesHook()\n",
        "\n",
        "for data in dataloader:\n",
        "    outputs, losses = hook({**models, **data})\n",
        "    print(outputs.keys())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5546c80",
      "metadata": {
        "id": "e5546c80"
      },
      "source": [
        "### DataloaderCreator\n",
        "\n",
        "```DataloaderCreator``` is a factory class. It allows you to specify how dataloaders should be made for multiple datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9625dd2a",
      "metadata": {
        "id": "9625dd2a",
        "outputId": "e30e1117-1df0-455b-e4cc-d874c46d3de1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': (64, 2), 'src_train': (64, 2), 'target_train': (64, 2)}\n",
            "{'train': (64, 2), 'src_val': (256, 4), 'target_val': (256, 4)}\n",
            "{'train': (32, 0), 'val1': (256, 4), 'val2': (256, 4)}\n",
            "{'train': (256, 4), 'val': (256, 4), 'woof': (256, 4)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "from pytorch_adapt.datasets import DataloaderCreator\n",
        "\n",
        "\n",
        "def print_dataloaders(dataloaders):\n",
        "    print({k: (v.batch_size, v.num_workers) for k, v in dataloaders.items()})\n",
        "\n",
        "\n",
        "# set the batch_size and num_workers for all datasets\n",
        "dc = DataloaderCreator(batch_size=64, num_workers=2)\n",
        "dataloaders = dc(train=src_target, src_train=src, target_train=target)\n",
        "print_dataloaders(dataloaders)\n",
        "\n",
        "# set different params for train vs val datasets\n",
        "dc = DataloaderCreator(\n",
        "    train_kwargs={\"batch_size\": 64, \"num_workers\": 2},\n",
        "    val_kwargs={\"batch_size\": 256, \"num_workers\": 4},\n",
        ")\n",
        "dataloaders = dc(train=src_target, src_val=src, target_val=target)\n",
        "print_dataloaders(dataloaders)\n",
        "\n",
        "# specify the name of the validation datasets\n",
        "dc = DataloaderCreator(\n",
        "    val_kwargs={\"batch_size\": 256, \"num_workers\": 4}, val_names=[\"val1\", \"val2\"]\n",
        ")\n",
        "dataloaders = dc(train=src_target, val1=src, val2=target)\n",
        "print_dataloaders(dataloaders)\n",
        "\n",
        "# consider all inputs to be validation datasets\n",
        "dc = DataloaderCreator(val_kwargs={\"batch_size\": 256, \"num_workers\": 4}, all_val=True)\n",
        "dataloaders = dc(train=src_target, val=src, woof=target)\n",
        "print_dataloaders(dataloaders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "afdcc167",
      "metadata": {
        "id": "afdcc167"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Datasets.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}