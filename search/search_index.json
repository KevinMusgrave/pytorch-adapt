{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTorch Adapt \u00b6 Google Colab Examples \u00b6 Overview \u00b6 Installation \u00b6 Pip \u00b6 pip install pytorch-adapt To get the latest dev version : pip install pytorch-adapt --pre","title":"Overview"},{"location":"#pytorch-adapt","text":"","title":"PyTorch Adapt"},{"location":"#google-colab-examples","text":"","title":"Google Colab Examples"},{"location":"#overview","text":"","title":"Overview"},{"location":"#installation","text":"","title":"Installation"},{"location":"#pip","text":"pip install pytorch-adapt To get the latest dev version : pip install pytorch-adapt --pre","title":"Pip"},{"location":"SUMMARY/","text":"Overview Datasets Wrappers Hooks Base Utils Weighters","title":"SUMMARY"},{"location":"adapters/","text":"Adapters \u00b6","title":"Adapters"},{"location":"adapters/#adapters","text":"","title":"Adapters"},{"location":"containers/","text":"Containers \u00b6","title":"Containers"},{"location":"containers/#containers","text":"","title":"Containers"},{"location":"frameworks/","text":"Frameworks \u00b6","title":"Frameworks"},{"location":"frameworks/#frameworks","text":"","title":"Frameworks"},{"location":"layers/","text":"Layers \u00b6","title":"Layers"},{"location":"layers/#layers","text":"","title":"Layers"},{"location":"meta_validators/","text":"Meta Validators \u00b6","title":"Meta Validators"},{"location":"meta_validators/#meta-validators","text":"","title":"Meta Validators"},{"location":"models/","text":"Models \u00b6","title":"Models"},{"location":"models/#models","text":"","title":"Models"},{"location":"utils/","text":"Utils \u00b6","title":"Utils"},{"location":"utils/#utils","text":"","title":"Utils"},{"location":"validators/","text":"Validators \u00b6","title":"Validators"},{"location":"validators/#validators","text":"","title":"Validators"},{"location":"datasets/","text":"Datasets \u00b6","title":"Datasets"},{"location":"datasets/#datasets","text":"","title":"Datasets"},{"location":"datasets/wrappers/","text":"pytorch_adapt.datasets.combined_source_and_target \u00b6 CombinedSourceAndTargetDataset \u00b6 Wraps a source dataset and a target dataset. __getitem__ ( self , idx ) special \u00b6 Parameters: Name Type Description Default idx The index of the target dataset. The source index is picked randomly. required Returns: Type Description Dict[str, Any] A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 33 34 35 36 37 38 39 40 41 42 43 44 def __getitem__ ( self , idx ) -> Dict [ str , Any ]: \"\"\" Arguments: idx: The index of the target dataset. The source index is picked randomly. Returns: A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". \"\"\" target_data = self . target_dataset [ idx ] src_data = self . source_dataset [ self . get_random_src_idx ()] c_f . assert_dicts_are_disjoint ( src_data , target_data ) return { ** src_data , ** target_data } __init__ ( self , source_dataset , target_dataset ) special \u00b6 Parameters: Name Type Description Default source_dataset SourceDataset required target_dataset TargetDataset required Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 16 17 18 19 20 21 22 23 24 def __init__ ( self , source_dataset : SourceDataset , target_dataset : TargetDataset ): \"\"\" Arguments: source_dataset: target_dataset: \"\"\" self . source_dataset = source_dataset self . target_dataset = target_dataset __len__ ( self ) special \u00b6 Returns: Type Description The length of the target dataset. Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 26 27 28 29 30 31 def __len__ ( self ): \"\"\" Returns: The length of the target dataset. \"\"\" return len ( self . target_dataset ) pytorch_adapt.datasets.concat_dataset \u00b6 ConcatDataset \u00b6 Exactly the same as torch.utils.data.ConcatDataset except with a nice __repr__ function. pytorch_adapt.datasets.source_dataset \u00b6 SourceDataset \u00b6 This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label). __getitem__ ( self , idx ) special \u00b6 Returns: Type Description Dict[str, Any] A dictionary with \"src_imgs\" (the data), \"src_domain\" (the integer representing the domain), \"src_labels\" (the class label), \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\source_dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with \"src_imgs\" (the data), \"src_domain\" (the integer representing the domain), \"src_labels\" (the class label), \"src_sample_idx\" (idx) \"\"\" img , src_labels = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : src_labels , \"src_sample_idx\" : idx , } __init__ ( self , dataset , domain = 0 ) special \u00b6 Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\source_dataset.py 15 16 17 18 19 20 21 def __init__ ( self , dataset : Dataset , domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain ) pytorch_adapt.datasets.target_dataset \u00b6 TargetDataset \u00b6 This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label). __getitem__ ( self , idx ) special \u00b6 Returns: Type Description Dict[str, Any] A dictionary with \"target_imgs\" (the data), \"target_domain\" (the integer representing the domain), \"target_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\target_dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with \"target_imgs\" (the data), \"target_domain\" (the integer representing the domain), \"target_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"target_imgs\" : img , \"target_domain\" : self . domain , \"target_sample_idx\" : idx , } __init__ ( self , dataset , domain = 1 ) special \u00b6 Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 1 Source code in pytorch_adapt\\datasets\\target_dataset.py 15 16 17 18 19 20 21 def __init__ ( self , dataset : Dataset , domain : int = 1 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain )","title":"Wrappers"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.combined_source_and_target","text":"","title":"combined_source_and_target"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset","text":"Wraps a source dataset and a target dataset.","title":"CombinedSourceAndTargetDataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset.__getitem__","text":"Parameters: Name Type Description Default idx The index of the target dataset. The source index is picked randomly. required Returns: Type Description Dict[str, Any] A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 33 34 35 36 37 38 39 40 41 42 43 44 def __getitem__ ( self , idx ) -> Dict [ str , Any ]: \"\"\" Arguments: idx: The index of the target dataset. The source index is picked randomly. Returns: A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". \"\"\" target_data = self . target_dataset [ idx ] src_data = self . source_dataset [ self . get_random_src_idx ()] c_f . assert_dicts_are_disjoint ( src_data , target_data ) return { ** src_data , ** target_data }","title":"__getitem__()"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset.__init__","text":"Parameters: Name Type Description Default source_dataset SourceDataset required target_dataset TargetDataset required Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 16 17 18 19 20 21 22 23 24 def __init__ ( self , source_dataset : SourceDataset , target_dataset : TargetDataset ): \"\"\" Arguments: source_dataset: target_dataset: \"\"\" self . source_dataset = source_dataset self . target_dataset = target_dataset","title":"__init__()"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset.__len__","text":"Returns: Type Description The length of the target dataset. Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 26 27 28 29 30 31 def __len__ ( self ): \"\"\" Returns: The length of the target dataset. \"\"\" return len ( self . target_dataset )","title":"__len__()"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.concat_dataset","text":"","title":"concat_dataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.concat_dataset.ConcatDataset","text":"Exactly the same as torch.utils.data.ConcatDataset except with a nice __repr__ function.","title":"ConcatDataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.source_dataset","text":"","title":"source_dataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.source_dataset.SourceDataset","text":"This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label).","title":"SourceDataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.source_dataset.SourceDataset.__getitem__","text":"Returns: Type Description Dict[str, Any] A dictionary with \"src_imgs\" (the data), \"src_domain\" (the integer representing the domain), \"src_labels\" (the class label), \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\source_dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with \"src_imgs\" (the data), \"src_domain\" (the integer representing the domain), \"src_labels\" (the class label), \"src_sample_idx\" (idx) \"\"\" img , src_labels = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : src_labels , \"src_sample_idx\" : idx , }","title":"__getitem__()"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.source_dataset.SourceDataset.__init__","text":"Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\source_dataset.py 15 16 17 18 19 20 21 def __init__ ( self , dataset : Dataset , domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain )","title":"__init__()"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.target_dataset","text":"","title":"target_dataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.target_dataset.TargetDataset","text":"This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label).","title":"TargetDataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.target_dataset.TargetDataset.__getitem__","text":"Returns: Type Description Dict[str, Any] A dictionary with \"target_imgs\" (the data), \"target_domain\" (the integer representing the domain), \"target_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\target_dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with \"target_imgs\" (the data), \"target_domain\" (the integer representing the domain), \"target_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"target_imgs\" : img , \"target_domain\" : self . domain , \"target_sample_idx\" : idx , }","title":"__getitem__()"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.target_dataset.TargetDataset.__init__","text":"Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 1 Source code in pytorch_adapt\\datasets\\target_dataset.py 15 16 17 18 19 20 21 def __init__ ( self , dataset : Dataset , domain : int = 1 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain )","title":"__init__()"},{"location":"hooks/","text":"Hooks \u00b6 Hooks are the main building block of this library. Every hook is a callable that takes in 2 arguments that represent the current context: A dictionary of previously computed losses. A dictionary of everything else that has been previously computed or passed in. The purpose of the context is to compute data only when necessary. For example, to compute a classification loss, a hook will need logits. If these logits are not available in the context, then they are computed, added to the context, and then used to compute the loss. If they are already in the context, then only the loss is computed.","title":"Hooks"},{"location":"hooks/#hooks","text":"Hooks are the main building block of this library. Every hook is a callable that takes in 2 arguments that represent the current context: A dictionary of previously computed losses. A dictionary of everything else that has been previously computed or passed in. The purpose of the context is to compute data only when necessary. For example, to compute a classification loss, a hook will need logits. If these logits are not available in the context, then they are computed, added to the context, and then used to compute the loss. If they are already in the context, then only the loss is computed.","title":"Hooks"},{"location":"hooks/base/","text":"pytorch_adapt.hooks.base \u00b6 BaseConditionHook \u00b6 The base class for hooks that return a boolean BaseHook \u00b6 All hooks extend BaseHook __init__ ( self , loss_prefix = '' , loss_suffix = '' , out_prefix = '' , out_suffix = '' , key_map = None ) special \u00b6 Parameters: Name Type Description Default loss_prefix str prepended to all new loss keys '' loss_suffix str appended to all new loss keys '' out_prefix str prepended to all new output keys '' out_suffix str appended to all new output keys '' key_map Dict[str, str] a mapping from input_key to new_key . For example, if key_map = {\"A\": \"B\"}, and the input dict to __call__ is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting __call__ , the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. None Source code in pytorch_adapt\\hooks\\base.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , loss_prefix : str = \"\" , loss_suffix : str = \"\" , out_prefix : str = \"\" , out_suffix : str = \"\" , key_map : Dict [ str , str ] = None , ): \"\"\" Arguments: loss_prefix: prepended to all new loss keys loss_suffix: appended to all new loss keys out_prefix: prepended to all new output keys out_suffix: appended to all new output keys key_map: a mapping from ```input_key``` to ```new_key```. For example, if key_map = {\"A\": \"B\"}, and the input dict to ```__call__``` is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting ```__call__```, the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. \"\"\" if any ( not isinstance ( x , str ) for x in [ loss_prefix , loss_suffix , out_prefix , out_suffix ] ): raise TypeError ( \"loss prefix/suffix and out prefix/suffix must be strings\" ) self . loss_prefix = loss_prefix self . loss_suffix = loss_suffix self . out_prefix = out_prefix self . out_suffix = out_suffix self . key_map = c_f . default ( key_map , {}) self . in_keys = [] _loss_keys ( self ) private \u00b6 This must be implemented by the child class Returns: Type Description List[str] The names of the losses that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py 83 84 85 86 87 88 89 90 @abstractmethod def _loss_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the losses that will be added to the context. \"\"\" pass _out_keys ( self ) private \u00b6 This must be implemented by the child class Returns: Type Description List[str] The names of the outputs that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py 98 99 100 101 102 103 104 105 @abstractmethod def _out_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the outputs that will be added to the context. \"\"\" pass call ( self , losses , inputs ) \u00b6 This must be implemented by the child class Parameters: Name Type Description Default losses Dict[str, Any] previously computed losses required inputs Dict[str, Any] holds everything else: tensors, models etc. required Returns: Type Description Union[Tuple[Dict[str, Any], Dict[str, Any]], bool] Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean Source code in pytorch_adapt\\hooks\\base.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @abstractmethod def call ( self , losses : Dict [ str , Any ], inputs : Dict [ str , Any ] ) -> Union [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]], bool ]: \"\"\" This must be implemented by the child class Arguments: losses: previously computed losses inputs: holds everything else: tensors, models etc. Returns: Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean \"\"\" pass BaseWrapperHook \u00b6 A simple wrapper for calling self.hook , which should be defined in the child's __init__ function.","title":"Base"},{"location":"hooks/base/#pytorch_adapt.hooks.base","text":"","title":"base"},{"location":"hooks/base/#pytorch_adapt.hooks.base.BaseConditionHook","text":"The base class for hooks that return a boolean","title":"BaseConditionHook"},{"location":"hooks/base/#pytorch_adapt.hooks.base.BaseHook","text":"All hooks extend BaseHook","title":"BaseHook"},{"location":"hooks/base/#pytorch_adapt.hooks.base.BaseHook.__init__","text":"Parameters: Name Type Description Default loss_prefix str prepended to all new loss keys '' loss_suffix str appended to all new loss keys '' out_prefix str prepended to all new output keys '' out_suffix str appended to all new output keys '' key_map Dict[str, str] a mapping from input_key to new_key . For example, if key_map = {\"A\": \"B\"}, and the input dict to __call__ is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting __call__ , the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. None Source code in pytorch_adapt\\hooks\\base.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , loss_prefix : str = \"\" , loss_suffix : str = \"\" , out_prefix : str = \"\" , out_suffix : str = \"\" , key_map : Dict [ str , str ] = None , ): \"\"\" Arguments: loss_prefix: prepended to all new loss keys loss_suffix: appended to all new loss keys out_prefix: prepended to all new output keys out_suffix: appended to all new output keys key_map: a mapping from ```input_key``` to ```new_key```. For example, if key_map = {\"A\": \"B\"}, and the input dict to ```__call__``` is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting ```__call__```, the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. \"\"\" if any ( not isinstance ( x , str ) for x in [ loss_prefix , loss_suffix , out_prefix , out_suffix ] ): raise TypeError ( \"loss prefix/suffix and out prefix/suffix must be strings\" ) self . loss_prefix = loss_prefix self . loss_suffix = loss_suffix self . out_prefix = out_prefix self . out_suffix = out_suffix self . key_map = c_f . default ( key_map , {}) self . in_keys = []","title":"__init__()"},{"location":"hooks/base/#pytorch_adapt.hooks.base.BaseHook._loss_keys","text":"This must be implemented by the child class Returns: Type Description List[str] The names of the losses that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py 83 84 85 86 87 88 89 90 @abstractmethod def _loss_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the losses that will be added to the context. \"\"\" pass","title":"_loss_keys()"},{"location":"hooks/base/#pytorch_adapt.hooks.base.BaseHook._out_keys","text":"This must be implemented by the child class Returns: Type Description List[str] The names of the outputs that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py 98 99 100 101 102 103 104 105 @abstractmethod def _out_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the outputs that will be added to the context. \"\"\" pass","title":"_out_keys()"},{"location":"hooks/base/#pytorch_adapt.hooks.base.BaseHook.call","text":"This must be implemented by the child class Parameters: Name Type Description Default losses Dict[str, Any] previously computed losses required inputs Dict[str, Any] holds everything else: tensors, models etc. required Returns: Type Description Union[Tuple[Dict[str, Any], Dict[str, Any]], bool] Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean Source code in pytorch_adapt\\hooks\\base.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @abstractmethod def call ( self , losses : Dict [ str , Any ], inputs : Dict [ str , Any ] ) -> Union [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]], bool ]: \"\"\" This must be implemented by the child class Arguments: losses: previously computed losses inputs: holds everything else: tensors, models etc. Returns: Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean \"\"\" pass","title":"call()"},{"location":"hooks/base/#pytorch_adapt.hooks.base.BaseWrapperHook","text":"A simple wrapper for calling self.hook , which should be defined in the child's __init__ function.","title":"BaseWrapperHook"},{"location":"hooks/utils/","text":"pytorch_adapt.hooks.utils \u00b6 ApplyFnHook \u00b6 Applies a function to specific values of the context. __init__ ( self , fn , apply_to , is_loss = False , ** kwargs ) special \u00b6 Parameters: Name Type Description Default fn Callable The function that will be applied to the inputs. required apply_to List[str] fn will be applied to inputs[k] for k in apply_to required is_loss bool If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. False Source code in pytorch_adapt\\hooks\\utils.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def __init__ ( self , fn : Callable , apply_to : List [ str ], is_loss : bool = False , ** kwargs ): \"\"\" Arguments: fn: The function that will be applied to the inputs. apply_to: fn will be applied to ```inputs[k]``` for k in apply_to is_loss: If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. \"\"\" super () . __init__ ( ** kwargs ) self . fn = fn self . apply_to = apply_to self . is_loss = is_loss AssertHook \u00b6 Asserts that the output keys of a hook match a specified regex string __init__ ( self , hook , allowed , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The wrapped hook required allowed str The output dictionary of hook must have keys that match the allowed regex. required Source code in pytorch_adapt\\hooks\\utils.py 306 307 308 309 310 311 312 313 314 315 316 317 def __init__ ( self , hook : BaseHook , allowed : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook allowed: The output dictionary of ```hook``` must have keys that match the ```allowed``` regex. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook if not isinstance ( allowed , str ): raise TypeError ( \"allowed must be a str\" ) self . allowed = allowed ChainHook \u00b6 Calls multiple hooks sequentially. The Nth hook receives the context accumulated through hooks 0 to N-1. __init__ ( self , * hooks , * , conditions = None , alts = None , overwrite = False , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially. () conditions List[pytorch_adapt.hooks.base.BaseConditionHook] an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. None alts List[pytorch_adapt.hooks.base.BaseHook] an optional list of hooks that will be executed when the corresponding condition hook returns False None overwrite Union[bool, List[int]] If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. False Source code in pytorch_adapt\\hooks\\utils.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , * hooks : BaseHook , conditions : List [ BaseConditionHook ] = None , alts : List [ BaseHook ] = None , overwrite : Union [ bool , List [ int ]] = False , ** kwargs , ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially. conditions: an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. alts: an optional list of hooks that will be executed when the corresponding condition hook returns False overwrite: If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . conditions = c_f . default ( conditions , [ TrueHook () for _ in range ( len ( hooks ))] ) self . alts = c_f . default ( alts , [ ZeroLossHook ( h . loss_keys , h . out_keys ) for h in self . hooks ] ) self . check_alt_keys_match_hook_keys () if not isinstance ( overwrite , ( list , bool )): raise TypeError ( \"overwrite must be a list or bool\" ) self . overwrite = overwrite self . in_keys = self . hooks [ 0 ] . in_keys EmptyHook \u00b6 Returns two empty dictionaries. FalseHook \u00b6 Returns False MultiplierHook \u00b6 Multiplies every loss by a scalar __init__ ( self , hook , m , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The losses of this hook will be multiplied by m required m float The scalar required Source code in pytorch_adapt\\hooks\\utils.py 342 343 344 345 346 347 348 349 350 def __init__ ( self , hook : BaseHook , m : float , ** kwargs ): \"\"\" Arguments: hook: The losses of this hook will be multiplied by ```m``` m: The scalar \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . m = m NotHook \u00b6 Returns the boolean negation of the wrapped hook. __init__ ( self , hook , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseConditionHook The condition hook that will be negated. required Source code in pytorch_adapt\\hooks\\utils.py 288 289 290 291 292 293 294 def __init__ ( self , hook : BaseConditionHook , ** kwargs ): \"\"\" Arguments: hook: The condition hook that will be negated. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook OnlyNewOutputsHook \u00b6 Returns only outputs that are not present in the input context. You should use this if you want to change the value of a key passed to self.hook, but not propagate that change to the outside. __init__ ( self , hook , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The hook inside which changes to the context will be allowed. required Source code in pytorch_adapt\\hooks\\utils.py 213 214 215 216 217 218 219 def __init__ ( self , hook : BaseHook , ** kwargs ): \"\"\" Arguments: hook: The hook inside which changes to the context will be allowed. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook ParallelHook \u00b6 Calls multiple hooks while keeping contexts separate. The Nth hook receives the same context as hooks 0 to N-1. All the output contexts are merged at the end. __init__ ( self , * hooks , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. () Source code in pytorch_adapt\\hooks\\utils.py 171 172 173 174 175 176 177 178 179 def __init__ ( self , * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . in_keys = c_f . join_lists ([ h . in_keys for h in self . hooks ]) RepeatHook \u00b6 Executes the wrapped hook n times. __init__ ( self , hook , n , keep_only_last = False , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The hook that will be executed n times required n int The number of times the hook will be executed. required keep_only_last bool If False , the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If True , then only the (losses, outputs) of the final execution will be kept. False Source code in pytorch_adapt\\hooks\\utils.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def __init__ ( self , hook : BaseHook , n : int , keep_only_last : bool = False , ** kwargs ): \"\"\" Arguments: hook: The hook that will be executed ```n``` times n: The number of times the hook will be executed. keep_only_last: If ```False```, the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If ```True```, then only the (losses, outputs) of the final execution will be kept. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . n = n self . keep_only_last = keep_only_last TrueHook \u00b6 Returns True ZeroLossHook \u00b6 Returns only 0 losses and None outputs. __init__ ( self , loss_names , out_names , ** kwargs ) special \u00b6 Parameters: Name Type Description Default loss_names List[str] The keys of the loss dictionary which will have tensor(0.) as its values. required out_names List[str] The keys of the output dictionary which will have None as its values. required Source code in pytorch_adapt\\hooks\\utils.py 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , loss_names : List [ str ], out_names : List [ str ], ** kwargs ): \"\"\" Arguments: loss_names: The keys of the loss dictionary which will have ```tensor(0.)``` as its values. out_names: The keys of the output dictionary which will have ```None``` as its values. \"\"\" super () . __init__ ( ** kwargs ) self . loss_names = loss_names self . out_names = out_names","title":"Utils"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ApplyFnHook","text":"Applies a function to specific values of the context.","title":"ApplyFnHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ApplyFnHook.__init__","text":"Parameters: Name Type Description Default fn Callable The function that will be applied to the inputs. required apply_to List[str] fn will be applied to inputs[k] for k in apply_to required is_loss bool If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. False Source code in pytorch_adapt\\hooks\\utils.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def __init__ ( self , fn : Callable , apply_to : List [ str ], is_loss : bool = False , ** kwargs ): \"\"\" Arguments: fn: The function that will be applied to the inputs. apply_to: fn will be applied to ```inputs[k]``` for k in apply_to is_loss: If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. \"\"\" super () . __init__ ( ** kwargs ) self . fn = fn self . apply_to = apply_to self . is_loss = is_loss","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.AssertHook","text":"Asserts that the output keys of a hook match a specified regex string","title":"AssertHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.AssertHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The wrapped hook required allowed str The output dictionary of hook must have keys that match the allowed regex. required Source code in pytorch_adapt\\hooks\\utils.py 306 307 308 309 310 311 312 313 314 315 316 317 def __init__ ( self , hook : BaseHook , allowed : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook allowed: The output dictionary of ```hook``` must have keys that match the ```allowed``` regex. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook if not isinstance ( allowed , str ): raise TypeError ( \"allowed must be a str\" ) self . allowed = allowed","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ChainHook","text":"Calls multiple hooks sequentially. The Nth hook receives the context accumulated through hooks 0 to N-1.","title":"ChainHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ChainHook.__init__","text":"Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially. () conditions List[pytorch_adapt.hooks.base.BaseConditionHook] an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. None alts List[pytorch_adapt.hooks.base.BaseHook] an optional list of hooks that will be executed when the corresponding condition hook returns False None overwrite Union[bool, List[int]] If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. False Source code in pytorch_adapt\\hooks\\utils.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , * hooks : BaseHook , conditions : List [ BaseConditionHook ] = None , alts : List [ BaseHook ] = None , overwrite : Union [ bool , List [ int ]] = False , ** kwargs , ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially. conditions: an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. alts: an optional list of hooks that will be executed when the corresponding condition hook returns False overwrite: If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . conditions = c_f . default ( conditions , [ TrueHook () for _ in range ( len ( hooks ))] ) self . alts = c_f . default ( alts , [ ZeroLossHook ( h . loss_keys , h . out_keys ) for h in self . hooks ] ) self . check_alt_keys_match_hook_keys () if not isinstance ( overwrite , ( list , bool )): raise TypeError ( \"overwrite must be a list or bool\" ) self . overwrite = overwrite self . in_keys = self . hooks [ 0 ] . in_keys","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.EmptyHook","text":"Returns two empty dictionaries.","title":"EmptyHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.FalseHook","text":"Returns False","title":"FalseHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.MultiplierHook","text":"Multiplies every loss by a scalar","title":"MultiplierHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.MultiplierHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The losses of this hook will be multiplied by m required m float The scalar required Source code in pytorch_adapt\\hooks\\utils.py 342 343 344 345 346 347 348 349 350 def __init__ ( self , hook : BaseHook , m : float , ** kwargs ): \"\"\" Arguments: hook: The losses of this hook will be multiplied by ```m``` m: The scalar \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . m = m","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.NotHook","text":"Returns the boolean negation of the wrapped hook.","title":"NotHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.NotHook.__init__","text":"Parameters: Name Type Description Default hook BaseConditionHook The condition hook that will be negated. required Source code in pytorch_adapt\\hooks\\utils.py 288 289 290 291 292 293 294 def __init__ ( self , hook : BaseConditionHook , ** kwargs ): \"\"\" Arguments: hook: The condition hook that will be negated. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.OnlyNewOutputsHook","text":"Returns only outputs that are not present in the input context. You should use this if you want to change the value of a key passed to self.hook, but not propagate that change to the outside.","title":"OnlyNewOutputsHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.OnlyNewOutputsHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The hook inside which changes to the context will be allowed. required Source code in pytorch_adapt\\hooks\\utils.py 213 214 215 216 217 218 219 def __init__ ( self , hook : BaseHook , ** kwargs ): \"\"\" Arguments: hook: The hook inside which changes to the context will be allowed. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ParallelHook","text":"Calls multiple hooks while keeping contexts separate. The Nth hook receives the same context as hooks 0 to N-1. All the output contexts are merged at the end.","title":"ParallelHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ParallelHook.__init__","text":"Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. () Source code in pytorch_adapt\\hooks\\utils.py 171 172 173 174 175 176 177 178 179 def __init__ ( self , * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . in_keys = c_f . join_lists ([ h . in_keys for h in self . hooks ])","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.RepeatHook","text":"Executes the wrapped hook n times.","title":"RepeatHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.RepeatHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The hook that will be executed n times required n int The number of times the hook will be executed. required keep_only_last bool If False , the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If True , then only the (losses, outputs) of the final execution will be kept. False Source code in pytorch_adapt\\hooks\\utils.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def __init__ ( self , hook : BaseHook , n : int , keep_only_last : bool = False , ** kwargs ): \"\"\" Arguments: hook: The hook that will be executed ```n``` times n: The number of times the hook will be executed. keep_only_last: If ```False```, the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If ```True```, then only the (losses, outputs) of the final execution will be kept. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . n = n self . keep_only_last = keep_only_last","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.TrueHook","text":"Returns True","title":"TrueHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ZeroLossHook","text":"Returns only 0 losses and None outputs.","title":"ZeroLossHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ZeroLossHook.__init__","text":"Parameters: Name Type Description Default loss_names List[str] The keys of the loss dictionary which will have tensor(0.) as its values. required out_names List[str] The keys of the output dictionary which will have None as its values. required Source code in pytorch_adapt\\hooks\\utils.py 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , loss_names : List [ str ], out_names : List [ str ], ** kwargs ): \"\"\" Arguments: loss_names: The keys of the loss dictionary which will have ```tensor(0.)``` as its values. out_names: The keys of the output dictionary which will have ```None``` as its values. \"\"\" super () . __init__ ( ** kwargs ) self . loss_names = loss_names self . out_names = out_names","title":"__init__()"},{"location":"weighters/","text":"Weighters \u00b6 Weighters multiply losses by scalar values, and then reduce the losses to a single value on which you call .backward() . For example: import torch from pytorch_adapt.weighters import MeanWeighter weighter = MeanWeighter ( weights = { \"y\" : 2.3 }) logits = torch . randn ( 32 , 512 ) labels = torch . randint ( 0 , 10 , size = ( 32 ,)) x = torch . nn . functional . cross_entropy ( logits , labels ) y = torch . norm ( logits ) # y will by multiplied by 2.3 # x wasn't given a weight, # so it gets multiplied by the default value of 1. loss , components = weighter ({ \"x\" : x , \"y\" : y }) loss . backward () pytorch_adapt.weighters.base_weighter \u00b6 BaseWeighter \u00b6 Multiplies losses by scalar values, and then reduces them to a single value. __call__ ( self , loss_dict ) special \u00b6 Parameters: Name Type Description Default loss_dict Dict[str, torch.Tensor] A mapping from loss names to loss values. required Returns: Type Description Tuple[torch.Tensor, Dict[str, float]] A tuple where tuple[0] is the loss that .backward() can be called on, and tuple[1] is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. Source code in pytorch_adapt\\weighters\\base_weighter.py 52 53 54 55 56 57 58 59 60 61 62 63 def __call__ ( self , loss_dict : Dict [ str , torch . Tensor ] ) -> Tuple [ torch . Tensor , Dict [ str , float ]]: \"\"\" Arguments: loss_dict: A mapping from loss names to loss values. Returns: A tuple where ```tuple[0]``` is the loss that ```.backward()``` can be called on, and ```tuple[1]``` is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. \"\"\" return weight_losses ( self . reduction , self . weights , self . scale , loss_dict ) __init__ ( self , reduction , weights = None , scale = 1 ) special \u00b6 Parameters: Name Type Description Default reduction Callable[[List[torch.Tensor]], torch.Tensor] A function that takes in a list of losses and returns a single loss value. required weights Dict[str, float] A mapping from loss names to weight values. If None , weights are assumed to be 1. None scale float A scalar that every loss gets multiplied by. 1 Source code in pytorch_adapt\\weighters\\base_weighter.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , reduction : Callable [[ List [ torch . Tensor ]], torch . Tensor ], weights : Dict [ str , float ] = None , scale : float = 1 , ): \"\"\" Arguments: reduction: A function that takes in a list of losses and returns a single loss value. weights: A mapping from loss names to weight values. If ```None```, weights are assumed to be 1. scale: A scalar that every loss gets multiplied by. \"\"\" self . reduction = reduction self . weights = c_f . default ( weights , {}) self . scale = scale pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"weights\" , \"scale\" ]) pytorch_adapt.weighters.mean_weighter \u00b6 MeanWeighter \u00b6 Weights the losses and then returns the mean of the weighted losses. pytorch_adapt.weighters.sum_weighter \u00b6 SumWeighter \u00b6 Weights the losses and then returns the sum of the weighted losses.","title":"Weighters"},{"location":"weighters/#weighters","text":"Weighters multiply losses by scalar values, and then reduce the losses to a single value on which you call .backward() . For example: import torch from pytorch_adapt.weighters import MeanWeighter weighter = MeanWeighter ( weights = { \"y\" : 2.3 }) logits = torch . randn ( 32 , 512 ) labels = torch . randint ( 0 , 10 , size = ( 32 ,)) x = torch . nn . functional . cross_entropy ( logits , labels ) y = torch . norm ( logits ) # y will by multiplied by 2.3 # x wasn't given a weight, # so it gets multiplied by the default value of 1. loss , components = weighter ({ \"x\" : x , \"y\" : y }) loss . backward ()","title":"Weighters"},{"location":"weighters/#pytorch_adapt.weighters.base_weighter","text":"","title":"base_weighter"},{"location":"weighters/#pytorch_adapt.weighters.base_weighter.BaseWeighter","text":"Multiplies losses by scalar values, and then reduces them to a single value.","title":"BaseWeighter"},{"location":"weighters/#pytorch_adapt.weighters.base_weighter.BaseWeighter.__call__","text":"Parameters: Name Type Description Default loss_dict Dict[str, torch.Tensor] A mapping from loss names to loss values. required Returns: Type Description Tuple[torch.Tensor, Dict[str, float]] A tuple where tuple[0] is the loss that .backward() can be called on, and tuple[1] is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. Source code in pytorch_adapt\\weighters\\base_weighter.py 52 53 54 55 56 57 58 59 60 61 62 63 def __call__ ( self , loss_dict : Dict [ str , torch . Tensor ] ) -> Tuple [ torch . Tensor , Dict [ str , float ]]: \"\"\" Arguments: loss_dict: A mapping from loss names to loss values. Returns: A tuple where ```tuple[0]``` is the loss that ```.backward()``` can be called on, and ```tuple[1]``` is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. \"\"\" return weight_losses ( self . reduction , self . weights , self . scale , loss_dict )","title":"__call__()"},{"location":"weighters/#pytorch_adapt.weighters.base_weighter.BaseWeighter.__init__","text":"Parameters: Name Type Description Default reduction Callable[[List[torch.Tensor]], torch.Tensor] A function that takes in a list of losses and returns a single loss value. required weights Dict[str, float] A mapping from loss names to weight values. If None , weights are assumed to be 1. None scale float A scalar that every loss gets multiplied by. 1 Source code in pytorch_adapt\\weighters\\base_weighter.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , reduction : Callable [[ List [ torch . Tensor ]], torch . Tensor ], weights : Dict [ str , float ] = None , scale : float = 1 , ): \"\"\" Arguments: reduction: A function that takes in a list of losses and returns a single loss value. weights: A mapping from loss names to weight values. If ```None```, weights are assumed to be 1. scale: A scalar that every loss gets multiplied by. \"\"\" self . reduction = reduction self . weights = c_f . default ( weights , {}) self . scale = scale pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"weights\" , \"scale\" ])","title":"__init__()"},{"location":"weighters/#pytorch_adapt.weighters.mean_weighter","text":"","title":"mean_weighter"},{"location":"weighters/#pytorch_adapt.weighters.mean_weighter.MeanWeighter","text":"Weights the losses and then returns the mean of the weighted losses.","title":"MeanWeighter"},{"location":"weighters/#pytorch_adapt.weighters.sum_weighter","text":"","title":"sum_weighter"},{"location":"weighters/#pytorch_adapt.weighters.sum_weighter.SumWeighter","text":"Weights the losses and then returns the sum of the weighted losses.","title":"SumWeighter"}]}