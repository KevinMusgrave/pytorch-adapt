{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTorch Adapt \u00b6 Google Colab Examples \u00b6 Overview \u00b6 Installation \u00b6 Pip \u00b6 pip install pytorch-adapt To get the latest dev version : pip install pytorch-adapt --pre","title":"Overview"},{"location":"#pytorch-adapt","text":"","title":"PyTorch Adapt"},{"location":"#google-colab-examples","text":"","title":"Google Colab Examples"},{"location":"#overview","text":"","title":"Overview"},{"location":"#installation","text":"","title":"Installation"},{"location":"#pip","text":"pip install pytorch-adapt To get the latest dev version : pip install pytorch-adapt --pre","title":"Pip"},{"location":"SUMMARY/","text":"Overview Datasets CombinedSourceAndTarget ConcatDataset DataloaderCreator DomainNet MNISTM Office31 PseudoLabeledDataset SourceDataset TargetDataset Hooks Base BaseConditionHook BaseHook BaseWrapperHook Utils ApplyFnHook AssertHook ChainHook EmptyHook FalseHook MultiplierHook NotHook OnlyNewOutputsHook ParallelHook RepeatHook TrueHook ZeroLossHook Layers AbsLoss AdaptiveFeatureNorm BatchSpectralLoss BNMLoss ConcatSoftmax ConfidenceWeights CORALLoss DiversityLoss DoNothingOptimizer EntropyLoss EntropyWeights GradientReversal MCCLoss MCDLoss MMDLoss ModelWithBridge MultipleModels NeighborhoodAggregation PlusResidual RandomizedDotProduct SilhouetteScore SlicedWasserstein StochasticLinear SufficientAccuracy UniformDistributionLoss VATLoss Meta Validators ForwardOnlyValidator ReverseValidator Weighters BaseWeighter MeanWeighter SumWeighter","title":"SUMMARY"},{"location":"adapters/","text":"Adapters \u00b6","title":"Adapters"},{"location":"adapters/#adapters","text":"","title":"Adapters"},{"location":"containers/","text":"Containers \u00b6","title":"Containers"},{"location":"containers/#containers","text":"","title":"Containers"},{"location":"frameworks/","text":"Frameworks \u00b6","title":"Frameworks"},{"location":"frameworks/#frameworks","text":"","title":"Frameworks"},{"location":"models/","text":"Models \u00b6","title":"Models"},{"location":"models/#models","text":"","title":"Models"},{"location":"utils/","text":"Utils \u00b6","title":"Utils"},{"location":"utils/#utils","text":"","title":"Utils"},{"location":"validators/","text":"Validators \u00b6","title":"Validators"},{"location":"validators/#validators","text":"","title":"Validators"},{"location":"datasets/","text":"Datasets \u00b6","title":"Datasets"},{"location":"datasets/#datasets","text":"","title":"Datasets"},{"location":"datasets/combined_source_and_target/","text":"pytorch_adapt.datasets.combined_source_and_target \u00b6 CombinedSourceAndTargetDataset \u00b6 Wraps a source dataset and a target dataset. __getitem__ ( self , idx ) special \u00b6 Parameters: Name Type Description Default idx The index of the target dataset. The source index is picked randomly. required Returns: Type Description Dict[str, Any] A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 33 34 35 36 37 38 39 40 41 42 43 44 def __getitem__ ( self , idx ) -> Dict [ str , Any ]: \"\"\" Arguments: idx: The index of the target dataset. The source index is picked randomly. Returns: A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". \"\"\" target_data = self . target_dataset [ idx ] src_data = self . source_dataset [ self . get_random_src_idx ()] c_f . assert_dicts_are_disjoint ( src_data , target_data ) return { ** src_data , ** target_data } __init__ ( self , source_dataset , target_dataset ) special \u00b6 Parameters: Name Type Description Default source_dataset SourceDataset required target_dataset TargetDataset required Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 16 17 18 19 20 21 22 23 24 def __init__ ( self , source_dataset : SourceDataset , target_dataset : TargetDataset ): \"\"\" Arguments: source_dataset: target_dataset: \"\"\" self . source_dataset = source_dataset self . target_dataset = target_dataset __len__ ( self ) special \u00b6 Returns: Type Description int The length of the target dataset. Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 26 27 28 29 30 31 def __len__ ( self ) -> int : \"\"\" Returns: The length of the target dataset. \"\"\" return len ( self . target_dataset )","title":"CombinedSourceAndTarget"},{"location":"datasets/combined_source_and_target/#pytorch_adapt.datasets.combined_source_and_target","text":"","title":"combined_source_and_target"},{"location":"datasets/combined_source_and_target/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset","text":"Wraps a source dataset and a target dataset.","title":"CombinedSourceAndTargetDataset"},{"location":"datasets/combined_source_and_target/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset.__getitem__","text":"Parameters: Name Type Description Default idx The index of the target dataset. The source index is picked randomly. required Returns: Type Description Dict[str, Any] A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 33 34 35 36 37 38 39 40 41 42 43 44 def __getitem__ ( self , idx ) -> Dict [ str , Any ]: \"\"\" Arguments: idx: The index of the target dataset. The source index is picked randomly. Returns: A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". \"\"\" target_data = self . target_dataset [ idx ] src_data = self . source_dataset [ self . get_random_src_idx ()] c_f . assert_dicts_are_disjoint ( src_data , target_data ) return { ** src_data , ** target_data }","title":"__getitem__()"},{"location":"datasets/combined_source_and_target/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset.__init__","text":"Parameters: Name Type Description Default source_dataset SourceDataset required target_dataset TargetDataset required Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 16 17 18 19 20 21 22 23 24 def __init__ ( self , source_dataset : SourceDataset , target_dataset : TargetDataset ): \"\"\" Arguments: source_dataset: target_dataset: \"\"\" self . source_dataset = source_dataset self . target_dataset = target_dataset","title":"__init__()"},{"location":"datasets/combined_source_and_target/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset.__len__","text":"Returns: Type Description int The length of the target dataset. Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 26 27 28 29 30 31 def __len__ ( self ) -> int : \"\"\" Returns: The length of the target dataset. \"\"\" return len ( self . target_dataset )","title":"__len__()"},{"location":"datasets/concat_dataset/","text":"pytorch_adapt.datasets.concat_dataset \u00b6 ConcatDataset \u00b6 Exactly the same as torch.utils.data.ConcatDataset except with a nice __repr__ function.","title":"ConcatDataset"},{"location":"datasets/concat_dataset/#pytorch_adapt.datasets.concat_dataset","text":"","title":"concat_dataset"},{"location":"datasets/concat_dataset/#pytorch_adapt.datasets.concat_dataset.ConcatDataset","text":"Exactly the same as torch.utils.data.ConcatDataset except with a nice __repr__ function.","title":"ConcatDataset"},{"location":"datasets/dataloader_creator/","text":"pytorch_adapt.datasets.dataloader_creator \u00b6 DataloaderCreator \u00b6 This is a factory class for creating dataloaders. The __call__ function takes in keyword arguments which are datasets, and outputs a dictionary of dataloaders (one dataloader for each input dataset). __call__ ( self , ** kwargs ) special \u00b6 Parameters: Name Type Description Default **kwargs keyword arguments mapping from dataset names to datasets. {} Returns: Type Description Dict[str, torch.utils.data.dataloader.DataLoader] a dictionary mapping from dataset names to dataloaders. Source code in pytorch_adapt\\datasets\\dataloader_creator.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def __call__ ( self , ** kwargs ) -> Dict [ str , DataLoader ]: \"\"\" Arguments: **kwargs: keyword arguments mapping from dataset names to datasets. Returns: a dictionary mapping from dataset names to dataloaders. \"\"\" output = {} for k , v in kwargs . items (): if self . all_train : dataloader_kwargs = self . train_kwargs elif self . all_val : dataloader_kwargs = self . val_kwargs elif k in self . train_names : dataloader_kwargs = self . train_kwargs elif k in self . val_names : dataloader_kwargs = self . val_kwargs else : raise ValueError ( f \"Dataset split name must be in {self.train_names} or {self.val_names} , or one of self.all_train or self.all_val must be true\" ) output [ k ] = torch . utils . data . DataLoader ( v , ** dataloader_kwargs ) return output __init__ ( self , train_kwargs = None , val_kwargs = None , train_names = None , val_names = None , all_train = False , all_val = False , batch_size = 32 , num_workers = 0 ) special \u00b6 Parameters: Name Type Description Default train_kwargs Dict[str, Any] The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. None val_kwargs Dict[str, Any] The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. None train_names List[str] A list of the dataset names that are used during training. None val_names List[str] A list of the dataset names that are used during validation. None all_train bool If True, then all input datasets are assumed to be for training, regardless of their names. False all_val bool If True, then all input datasets are assumed to be for validation, regardless of their names. False batch_size int The default batch_size used in train_kwargs (if not provided) and val_kwargs (if not provided) 32 num_workers int The default num_workers used in train_kwargs (if not provided) and val_kwargs (if not provided) 0 Source code in pytorch_adapt\\datasets\\dataloader_creator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , train_kwargs : Dict [ str , Any ] = None , val_kwargs : Dict [ str , Any ] = None , train_names : List [ str ] = None , val_names : List [ str ] = None , all_train : bool = False , all_val : bool = False , batch_size : int = 32 , num_workers : int = 0 , ): \"\"\" Arguments: train_kwargs: The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. val_kwargs: The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. train_names: A list of the dataset names that are used during training. val_names: A list of the dataset names that are used during validation. all_train: If True, then all input datasets are assumed to be for training, regardless of their names. all_val: If True, then all input datasets are assumed to be for validation, regardless of their names. batch_size: The default ```batch_size``` used in train_kwargs (if not provided) and val_kwargs (if not provided) num_workers: The default ```num_workers``` used in train_kwargs (if not provided) and val_kwargs (if not provided) \"\"\" self . train_kwargs = c_f . default ( train_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : True , \"drop_last\" : True , }, ) self . val_kwargs = c_f . default ( val_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : False , \"drop_last\" : False , }, ) self . train_names = c_f . default ( train_names , [ \"train\" ]) self . val_names = c_f . default ( val_names , [ \"src_train\" , \"target_train\" , \"src_val\" , \"target_val\" ] ) if not set ( self . train_names ) . isdisjoint ( self . val_names ): raise ValueError ( f \"train_names {self.train_names} must be disjoint from val_names {self.val_names} \" ) if all_train and all_val : raise ValueError ( \"all_train and all_val cannot both be True\" ) self . all_train = all_train self . all_val = all_val","title":"DataloaderCreator"},{"location":"datasets/dataloader_creator/#pytorch_adapt.datasets.dataloader_creator","text":"","title":"dataloader_creator"},{"location":"datasets/dataloader_creator/#pytorch_adapt.datasets.dataloader_creator.DataloaderCreator","text":"This is a factory class for creating dataloaders. The __call__ function takes in keyword arguments which are datasets, and outputs a dictionary of dataloaders (one dataloader for each input dataset).","title":"DataloaderCreator"},{"location":"datasets/dataloader_creator/#pytorch_adapt.datasets.dataloader_creator.DataloaderCreator.__call__","text":"Parameters: Name Type Description Default **kwargs keyword arguments mapping from dataset names to datasets. {} Returns: Type Description Dict[str, torch.utils.data.dataloader.DataLoader] a dictionary mapping from dataset names to dataloaders. Source code in pytorch_adapt\\datasets\\dataloader_creator.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def __call__ ( self , ** kwargs ) -> Dict [ str , DataLoader ]: \"\"\" Arguments: **kwargs: keyword arguments mapping from dataset names to datasets. Returns: a dictionary mapping from dataset names to dataloaders. \"\"\" output = {} for k , v in kwargs . items (): if self . all_train : dataloader_kwargs = self . train_kwargs elif self . all_val : dataloader_kwargs = self . val_kwargs elif k in self . train_names : dataloader_kwargs = self . train_kwargs elif k in self . val_names : dataloader_kwargs = self . val_kwargs else : raise ValueError ( f \"Dataset split name must be in {self.train_names} or {self.val_names} , or one of self.all_train or self.all_val must be true\" ) output [ k ] = torch . utils . data . DataLoader ( v , ** dataloader_kwargs ) return output","title":"__call__()"},{"location":"datasets/dataloader_creator/#pytorch_adapt.datasets.dataloader_creator.DataloaderCreator.__init__","text":"Parameters: Name Type Description Default train_kwargs Dict[str, Any] The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. None val_kwargs Dict[str, Any] The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. None train_names List[str] A list of the dataset names that are used during training. None val_names List[str] A list of the dataset names that are used during validation. None all_train bool If True, then all input datasets are assumed to be for training, regardless of their names. False all_val bool If True, then all input datasets are assumed to be for validation, regardless of their names. False batch_size int The default batch_size used in train_kwargs (if not provided) and val_kwargs (if not provided) 32 num_workers int The default num_workers used in train_kwargs (if not provided) and val_kwargs (if not provided) 0 Source code in pytorch_adapt\\datasets\\dataloader_creator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , train_kwargs : Dict [ str , Any ] = None , val_kwargs : Dict [ str , Any ] = None , train_names : List [ str ] = None , val_names : List [ str ] = None , all_train : bool = False , all_val : bool = False , batch_size : int = 32 , num_workers : int = 0 , ): \"\"\" Arguments: train_kwargs: The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. val_kwargs: The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. train_names: A list of the dataset names that are used during training. val_names: A list of the dataset names that are used during validation. all_train: If True, then all input datasets are assumed to be for training, regardless of their names. all_val: If True, then all input datasets are assumed to be for validation, regardless of their names. batch_size: The default ```batch_size``` used in train_kwargs (if not provided) and val_kwargs (if not provided) num_workers: The default ```num_workers``` used in train_kwargs (if not provided) and val_kwargs (if not provided) \"\"\" self . train_kwargs = c_f . default ( train_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : True , \"drop_last\" : True , }, ) self . val_kwargs = c_f . default ( val_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : False , \"drop_last\" : False , }, ) self . train_names = c_f . default ( train_names , [ \"train\" ]) self . val_names = c_f . default ( val_names , [ \"src_train\" , \"target_train\" , \"src_val\" , \"target_val\" ] ) if not set ( self . train_names ) . isdisjoint ( self . val_names ): raise ValueError ( f \"train_names {self.train_names} must be disjoint from val_names {self.val_names} \" ) if all_train and all_val : raise ValueError ( \"all_train and all_val cannot both be True\" ) self . all_train = all_train self . all_val = all_val","title":"__init__()"},{"location":"datasets/domainnet/","text":"pytorch_adapt.datasets.domainnet \u00b6 DomainNet \u00b6 A large dataset used in \"Moment Matching for Multi-Source Domain Adaptation\". It consists of 345 classes in 6 domains: clipart, infograph, painting, quickdraw, real, sketch __init__ ( self , root , domain , train , transform , ** kwargs ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 6 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 6 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" {domain} _ {name} .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 33525 , \"test\" : 14604 }[ name ], \"infograph\" : { \"train\" : 36023 , \"test\" : 15582 }[ name ], \"painting\" : { \"train\" : 50416 , \"test\" : 21850 }[ name ], \"quickdraw\" : { \"train\" : 120750 , \"test\" : 51750 }[ name ], \"real\" : { \"train\" : 120906 , \"test\" : 52041 }[ name ], \"sketch\" : { \"train\" : 48212 , \"test\" : 20916 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform DomainNet126 \u00b6 A custom train/test split of DomainNet126Full. __init__ ( self , root , domain , train , transform , ** kwargs ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 4 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" {domain} 126_ {name} .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 14962 , \"test\" : 3741 }[ name ], \"painting\" : { \"train\" : 25201 , \"test\" : 6301 }[ name ], \"real\" : { \"train\" : 56286 , \"test\" : 14072 }[ name ], \"sketch\" : { \"train\" : 19665 , \"test\" : 4917 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform DomainNet126Full \u00b6 A subset of DomainNet consisting of 126 classes and 4 domains: clipart, painting, real, sketch __init__ ( self , root , domain , transform , ** kwargs ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 4 domains required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def __init__ ( self , root : str , domain : str , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) filenames = [ f \"labeled_source_images_ {domain} \" , f \"labeled_target_images_ {domain} _1\" , f \"labeled_target_images_ {domain} _3\" , f \"unlabeled_target_images_ {domain} _1\" , f \"unlabeled_target_images_ {domain} _3\" , f \"validation_target_images_ {domain} _3\" , ] filenames = [ os . path . join ( root , \"domainnet\" , f \" {f} .txt\" ) for f in filenames ] img_dir = os . path . join ( root , \"domainnet\" ) content = OrderedDict () for f in filenames : with open ( f ) as fff : for line in fff : path , label = line . rstrip () . split ( \" \" ) content [ path ] = label self . img_paths = [ os . path . join ( img_dir , x ) for x in content . keys ()] check_img_paths ( img_dir , self . img_paths , domain ) self . labels = [ int ( x ) for x in content . values ()] self . transform = transform","title":"DomainNet"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet","text":"","title":"domainnet"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet","text":"A large dataset used in \"Moment Matching for Multi-Source Domain Adaptation\". It consists of 345 classes in 6 domains: clipart, infograph, painting, quickdraw, real, sketch","title":"DomainNet"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 6 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 6 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" {domain} _ {name} .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 33525 , \"test\" : 14604 }[ name ], \"infograph\" : { \"train\" : 36023 , \"test\" : 15582 }[ name ], \"painting\" : { \"train\" : 50416 , \"test\" : 21850 }[ name ], \"quickdraw\" : { \"train\" : 120750 , \"test\" : 51750 }[ name ], \"real\" : { \"train\" : 120906 , \"test\" : 52041 }[ name ], \"sketch\" : { \"train\" : 48212 , \"test\" : 20916 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"__init__()"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet126","text":"A custom train/test split of DomainNet126Full.","title":"DomainNet126"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet126.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 4 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" {domain} 126_ {name} .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 14962 , \"test\" : 3741 }[ name ], \"painting\" : { \"train\" : 25201 , \"test\" : 6301 }[ name ], \"real\" : { \"train\" : 56286 , \"test\" : 14072 }[ name ], \"sketch\" : { \"train\" : 19665 , \"test\" : 4917 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"__init__()"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet126Full","text":"A subset of DomainNet consisting of 126 classes and 4 domains: clipart, painting, real, sketch","title":"DomainNet126Full"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet126Full.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 4 domains required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def __init__ ( self , root : str , domain : str , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) filenames = [ f \"labeled_source_images_ {domain} \" , f \"labeled_target_images_ {domain} _1\" , f \"labeled_target_images_ {domain} _3\" , f \"unlabeled_target_images_ {domain} _1\" , f \"unlabeled_target_images_ {domain} _3\" , f \"validation_target_images_ {domain} _3\" , ] filenames = [ os . path . join ( root , \"domainnet\" , f \" {f} .txt\" ) for f in filenames ] img_dir = os . path . join ( root , \"domainnet\" ) content = OrderedDict () for f in filenames : with open ( f ) as fff : for line in fff : path , label = line . rstrip () . split ( \" \" ) content [ path ] = label self . img_paths = [ os . path . join ( img_dir , x ) for x in content . keys ()] check_img_paths ( img_dir , self . img_paths , domain ) self . labels = [ int ( x ) for x in content . values ()] self . transform = transform","title":"__init__()"},{"location":"datasets/mnistm/","text":"pytorch_adapt.datasets.mnistm \u00b6 MNISTM \u00b6 The dataset used in \"Domain-Adversarial Training of Neural Networks\". It consists of colored MNIST digits. __init__ ( self , root , train , transform , ** kwargs ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/mnist_m required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\mnistm.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , root : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/mnist_m``` train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = \"MNISTM\" , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ {name} _labels.txt\" ) img_dir = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ {name} \" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_length ( self , { \"train\" : 59001 , \"test\" : 9001 }[ name ]) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"MNISTM"},{"location":"datasets/mnistm/#pytorch_adapt.datasets.mnistm","text":"","title":"mnistm"},{"location":"datasets/mnistm/#pytorch_adapt.datasets.mnistm.MNISTM","text":"The dataset used in \"Domain-Adversarial Training of Neural Networks\". It consists of colored MNIST digits.","title":"MNISTM"},{"location":"datasets/mnistm/#pytorch_adapt.datasets.mnistm.MNISTM.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/mnist_m required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\mnistm.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , root : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/mnist_m``` train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = \"MNISTM\" , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ {name} _labels.txt\" ) img_dir = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ {name} \" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_length ( self , { \"train\" : 59001 , \"test\" : 9001 }[ name ]) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"__init__()"},{"location":"datasets/office31/","text":"pytorch_adapt.datasets.office31 \u00b6 Office31 \u00b6 A custom train/test split of Office31Full. __init__ ( self , root , domain , train , transform , ** kwargs ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/office31 required domain str One of the 3 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\office31.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of the 3 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"office31\" , f \" {domain} _ {name} .txt\" ) img_dir = os . path . join ( root , \"office31\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"amazon\" : { \"train\" : 2253 , \"test\" : 564 }[ name ], \"dslr\" : { \"train\" : 398 , \"test\" : 100 }[ name ], \"webcam\" : { \"train\" : 636 , \"test\" : 159 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform Office31Full \u00b6 A small dataset consisting of 31 classes in 3 domains: amazon, dslr, webcam. __init__ ( self , root , domain , transform ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/office31 required domain str One of the 3 domains required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\office31.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , root : str , domain : str , transform ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of the 3 domains transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain ) self . transform = transform self . dataset = torch_datasets . ImageFolder ( os . path . join ( root , \"office31\" , domain , \"images\" ), transform = self . transform ) check_length ( self , { \"amazon\" : 2817 , \"dslr\" : 498 , \"webcam\" : 795 }[ domain ])","title":"Office31"},{"location":"datasets/office31/#pytorch_adapt.datasets.office31","text":"","title":"office31"},{"location":"datasets/office31/#pytorch_adapt.datasets.office31.Office31","text":"A custom train/test split of Office31Full.","title":"Office31"},{"location":"datasets/office31/#pytorch_adapt.datasets.office31.Office31.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/office31 required domain str One of the 3 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\office31.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of the 3 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"office31\" , f \" {domain} _ {name} .txt\" ) img_dir = os . path . join ( root , \"office31\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"amazon\" : { \"train\" : 2253 , \"test\" : 564 }[ name ], \"dslr\" : { \"train\" : 398 , \"test\" : 100 }[ name ], \"webcam\" : { \"train\" : 636 , \"test\" : 159 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"__init__()"},{"location":"datasets/office31/#pytorch_adapt.datasets.office31.Office31Full","text":"A small dataset consisting of 31 classes in 3 domains: amazon, dslr, webcam.","title":"Office31Full"},{"location":"datasets/office31/#pytorch_adapt.datasets.office31.Office31Full.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/office31 required domain str One of the 3 domains required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\office31.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , root : str , domain : str , transform ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of the 3 domains transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain ) self . transform = transform self . dataset = torch_datasets . ImageFolder ( os . path . join ( root , \"office31\" , domain , \"images\" ), transform = self . transform ) check_length ( self , { \"amazon\" : 2817 , \"dslr\" : 498 , \"webcam\" : 795 }[ domain ])","title":"__init__()"},{"location":"datasets/pseudo_labeled_dataset/","text":"pytorch_adapt.datasets.pseudo_labeled_dataset \u00b6 PseudoLabeledDataset \u00b6 This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) . The label returned by the wrapped dataset is discarded, and the pseudo label is returned instead. __getitem__ ( self , idx ) special \u00b6 Returns: Type Description A dictionary with keys \"src_imgs\" (the data) \"src_domain\" (the integer representing the domain) \"src_labels\" (the pseudo label) \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __getitem__ ( self , idx : int ): \"\"\" Returns: A dictionary with keys: - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the pseudo label) - \"src_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : self . pseudo_labels [ idx ], \"src_sample_idx\" : idx , } __init__ ( self , dataset , pseudo_labels , domain = 0 ) special \u00b6 Parameters: Name Type Description Default dataset Dataset The dataset to wrap required pseudo_labels List[int] The class labels that will be used instead of the labels contained in self.dataset required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py 17 18 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , dataset : Dataset , pseudo_labels : List [ int ], domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap pseudo_labels: The class labels that will be used instead of the labels contained in self.dataset domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain ) if len ( self . dataset ) != len ( pseudo_labels ): raise ValueError ( \"len(dataset) must equal len(pseudo_labels)\" ) self . pseudo_labels = pseudo_labels","title":"PseudoLabeledDataset"},{"location":"datasets/pseudo_labeled_dataset/#pytorch_adapt.datasets.pseudo_labeled_dataset","text":"","title":"pseudo_labeled_dataset"},{"location":"datasets/pseudo_labeled_dataset/#pytorch_adapt.datasets.pseudo_labeled_dataset.PseudoLabeledDataset","text":"This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) . The label returned by the wrapped dataset is discarded, and the pseudo label is returned instead.","title":"PseudoLabeledDataset"},{"location":"datasets/pseudo_labeled_dataset/#pytorch_adapt.datasets.pseudo_labeled_dataset.PseudoLabeledDataset.__getitem__","text":"Returns: Type Description A dictionary with keys \"src_imgs\" (the data) \"src_domain\" (the integer representing the domain) \"src_labels\" (the pseudo label) \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __getitem__ ( self , idx : int ): \"\"\" Returns: A dictionary with keys: - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the pseudo label) - \"src_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : self . pseudo_labels [ idx ], \"src_sample_idx\" : idx , }","title":"__getitem__()"},{"location":"datasets/pseudo_labeled_dataset/#pytorch_adapt.datasets.pseudo_labeled_dataset.PseudoLabeledDataset.__init__","text":"Parameters: Name Type Description Default dataset Dataset The dataset to wrap required pseudo_labels List[int] The class labels that will be used instead of the labels contained in self.dataset required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py 17 18 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , dataset : Dataset , pseudo_labels : List [ int ], domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap pseudo_labels: The class labels that will be used instead of the labels contained in self.dataset domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain ) if len ( self . dataset ) != len ( pseudo_labels ): raise ValueError ( \"len(dataset) must equal len(pseudo_labels)\" ) self . pseudo_labels = pseudo_labels","title":"__init__()"},{"location":"datasets/source_dataset/","text":"pytorch_adapt.datasets.source_dataset \u00b6 SourceDataset \u00b6 This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) . __getitem__ ( self , idx ) special \u00b6 Returns: Type Description Dict[str, Any] A dictionary with keys: \"src_imgs\" (the data) \"src_domain\" (the integer representing the domain) \"src_labels\" (the class label) \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\source_dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys: - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the class label) - \"src_sample_idx\" (idx) \"\"\" img , src_labels = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : src_labels , \"src_sample_idx\" : idx , } __init__ ( self , dataset , domain = 0 ) special \u00b6 Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\source_dataset.py 15 16 17 18 19 20 21 def __init__ ( self , dataset : Dataset , domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain )","title":"SourceDataset"},{"location":"datasets/source_dataset/#pytorch_adapt.datasets.source_dataset","text":"","title":"source_dataset"},{"location":"datasets/source_dataset/#pytorch_adapt.datasets.source_dataset.SourceDataset","text":"This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) .","title":"SourceDataset"},{"location":"datasets/source_dataset/#pytorch_adapt.datasets.source_dataset.SourceDataset.__getitem__","text":"Returns: Type Description Dict[str, Any] A dictionary with keys: \"src_imgs\" (the data) \"src_domain\" (the integer representing the domain) \"src_labels\" (the class label) \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\source_dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys: - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the class label) - \"src_sample_idx\" (idx) \"\"\" img , src_labels = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : src_labels , \"src_sample_idx\" : idx , }","title":"__getitem__()"},{"location":"datasets/source_dataset/#pytorch_adapt.datasets.source_dataset.SourceDataset.__init__","text":"Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\source_dataset.py 15 16 17 18 19 20 21 def __init__ ( self , dataset : Dataset , domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain )","title":"__init__()"},{"location":"datasets/target_dataset/","text":"pytorch_adapt.datasets.target_dataset \u00b6 TargetDataset \u00b6 This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) . __getitem__ ( self , idx ) special \u00b6 Returns: Type Description Dict[str, Any] A dictionary with keys: \"target_imgs\" (the data) \"target_domain\" (the integer representing the domain) \"target_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\target_dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys: - \"target_imgs\" (the data) - \"target_domain\" (the integer representing the domain) - \"target_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"target_imgs\" : img , \"target_domain\" : self . domain , \"target_sample_idx\" : idx , } __init__ ( self , dataset , domain = 1 ) special \u00b6 Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 1 Source code in pytorch_adapt\\datasets\\target_dataset.py 15 16 17 18 19 20 21 def __init__ ( self , dataset : Dataset , domain : int = 1 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain )","title":"TargetDataset"},{"location":"datasets/target_dataset/#pytorch_adapt.datasets.target_dataset","text":"","title":"target_dataset"},{"location":"datasets/target_dataset/#pytorch_adapt.datasets.target_dataset.TargetDataset","text":"This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) .","title":"TargetDataset"},{"location":"datasets/target_dataset/#pytorch_adapt.datasets.target_dataset.TargetDataset.__getitem__","text":"Returns: Type Description Dict[str, Any] A dictionary with keys: \"target_imgs\" (the data) \"target_domain\" (the integer representing the domain) \"target_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\target_dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys: - \"target_imgs\" (the data) - \"target_domain\" (the integer representing the domain) - \"target_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"target_imgs\" : img , \"target_domain\" : self . domain , \"target_sample_idx\" : idx , }","title":"__getitem__()"},{"location":"datasets/target_dataset/#pytorch_adapt.datasets.target_dataset.TargetDataset.__init__","text":"Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 1 Source code in pytorch_adapt\\datasets\\target_dataset.py 15 16 17 18 19 20 21 def __init__ ( self , dataset : Dataset , domain : int = 1 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain )","title":"__init__()"},{"location":"hooks/","text":"Hooks \u00b6 Hooks are the main building block of this library. Every hook is a callable that takes in 2 arguments that represent the current context: A dictionary of previously computed losses. A dictionary of everything else that has been previously computed or passed in. The purpose of the context is to compute data only when necessary. For example, to compute a classification loss, a hook will need logits. If these logits are not available in the context, then they are computed, added to the context, and then used to compute the loss. If they are already in the context, then only the loss is computed.","title":"Hooks"},{"location":"hooks/#hooks","text":"Hooks are the main building block of this library. Every hook is a callable that takes in 2 arguments that represent the current context: A dictionary of previously computed losses. A dictionary of everything else that has been previously computed or passed in. The purpose of the context is to compute data only when necessary. For example, to compute a classification loss, a hook will need logits. If these logits are not available in the context, then they are computed, added to the context, and then used to compute the loss. If they are already in the context, then only the loss is computed.","title":"Hooks"},{"location":"hooks/base/base_condition_hook/","text":"pytorch_adapt.hooks.base \u00b6 BaseConditionHook \u00b6 The base class for hooks that return a boolean","title":"BaseConditionHook"},{"location":"hooks/base/base_condition_hook/#pytorch_adapt.hooks.base","text":"","title":"base"},{"location":"hooks/base/base_condition_hook/#pytorch_adapt.hooks.base.BaseConditionHook","text":"The base class for hooks that return a boolean","title":"BaseConditionHook"},{"location":"hooks/base/base_hook/","text":"pytorch_adapt.hooks.base \u00b6 BaseHook \u00b6 All hooks extend BaseHook __init__ ( self , loss_prefix = '' , loss_suffix = '' , out_prefix = '' , out_suffix = '' , key_map = None ) special \u00b6 Parameters: Name Type Description Default loss_prefix str prepended to all new loss keys '' loss_suffix str appended to all new loss keys '' out_prefix str prepended to all new output keys '' out_suffix str appended to all new output keys '' key_map Dict[str, str] a mapping from input_key to new_key . For example, if key_map = {\"A\": \"B\"}, and the input dict to __call__ is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting __call__ , the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. None Source code in pytorch_adapt\\hooks\\base.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , loss_prefix : str = \"\" , loss_suffix : str = \"\" , out_prefix : str = \"\" , out_suffix : str = \"\" , key_map : Dict [ str , str ] = None , ): \"\"\" Arguments: loss_prefix: prepended to all new loss keys loss_suffix: appended to all new loss keys out_prefix: prepended to all new output keys out_suffix: appended to all new output keys key_map: a mapping from ```input_key``` to ```new_key```. For example, if key_map = {\"A\": \"B\"}, and the input dict to ```__call__``` is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting ```__call__```, the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. \"\"\" if any ( not isinstance ( x , str ) for x in [ loss_prefix , loss_suffix , out_prefix , out_suffix ] ): raise TypeError ( \"loss prefix/suffix and out prefix/suffix must be strings\" ) self . loss_prefix = loss_prefix self . loss_suffix = loss_suffix self . out_prefix = out_prefix self . out_suffix = out_suffix self . key_map = c_f . default ( key_map , {}) self . in_keys = [] _loss_keys ( self ) private \u00b6 This must be implemented by the child class Returns: Type Description List[str] The names of the losses that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py 83 84 85 86 87 88 89 90 @abstractmethod def _loss_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the losses that will be added to the context. \"\"\" pass _out_keys ( self ) private \u00b6 This must be implemented by the child class Returns: Type Description List[str] The names of the outputs that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py 98 99 100 101 102 103 104 105 @abstractmethod def _out_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the outputs that will be added to the context. \"\"\" pass call ( self , losses , inputs ) \u00b6 This must be implemented by the child class Parameters: Name Type Description Default losses Dict[str, Any] previously computed losses required inputs Dict[str, Any] holds everything else: tensors, models etc. required Returns: Type Description Union[Tuple[Dict[str, Any], Dict[str, Any]], bool] Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean Source code in pytorch_adapt\\hooks\\base.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @abstractmethod def call ( self , losses : Dict [ str , Any ], inputs : Dict [ str , Any ] ) -> Union [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]], bool ]: \"\"\" This must be implemented by the child class Arguments: losses: previously computed losses inputs: holds everything else: tensors, models etc. Returns: Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean \"\"\" pass","title":"BaseHook"},{"location":"hooks/base/base_hook/#pytorch_adapt.hooks.base","text":"","title":"base"},{"location":"hooks/base/base_hook/#pytorch_adapt.hooks.base.BaseHook","text":"All hooks extend BaseHook","title":"BaseHook"},{"location":"hooks/base/base_hook/#pytorch_adapt.hooks.base.BaseHook.__init__","text":"Parameters: Name Type Description Default loss_prefix str prepended to all new loss keys '' loss_suffix str appended to all new loss keys '' out_prefix str prepended to all new output keys '' out_suffix str appended to all new output keys '' key_map Dict[str, str] a mapping from input_key to new_key . For example, if key_map = {\"A\": \"B\"}, and the input dict to __call__ is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting __call__ , the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. None Source code in pytorch_adapt\\hooks\\base.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , loss_prefix : str = \"\" , loss_suffix : str = \"\" , out_prefix : str = \"\" , out_suffix : str = \"\" , key_map : Dict [ str , str ] = None , ): \"\"\" Arguments: loss_prefix: prepended to all new loss keys loss_suffix: appended to all new loss keys out_prefix: prepended to all new output keys out_suffix: appended to all new output keys key_map: a mapping from ```input_key``` to ```new_key```. For example, if key_map = {\"A\": \"B\"}, and the input dict to ```__call__``` is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting ```__call__```, the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. \"\"\" if any ( not isinstance ( x , str ) for x in [ loss_prefix , loss_suffix , out_prefix , out_suffix ] ): raise TypeError ( \"loss prefix/suffix and out prefix/suffix must be strings\" ) self . loss_prefix = loss_prefix self . loss_suffix = loss_suffix self . out_prefix = out_prefix self . out_suffix = out_suffix self . key_map = c_f . default ( key_map , {}) self . in_keys = []","title":"__init__()"},{"location":"hooks/base/base_hook/#pytorch_adapt.hooks.base.BaseHook._loss_keys","text":"This must be implemented by the child class Returns: Type Description List[str] The names of the losses that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py 83 84 85 86 87 88 89 90 @abstractmethod def _loss_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the losses that will be added to the context. \"\"\" pass","title":"_loss_keys()"},{"location":"hooks/base/base_hook/#pytorch_adapt.hooks.base.BaseHook._out_keys","text":"This must be implemented by the child class Returns: Type Description List[str] The names of the outputs that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py 98 99 100 101 102 103 104 105 @abstractmethod def _out_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the outputs that will be added to the context. \"\"\" pass","title":"_out_keys()"},{"location":"hooks/base/base_hook/#pytorch_adapt.hooks.base.BaseHook.call","text":"This must be implemented by the child class Parameters: Name Type Description Default losses Dict[str, Any] previously computed losses required inputs Dict[str, Any] holds everything else: tensors, models etc. required Returns: Type Description Union[Tuple[Dict[str, Any], Dict[str, Any]], bool] Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean Source code in pytorch_adapt\\hooks\\base.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @abstractmethod def call ( self , losses : Dict [ str , Any ], inputs : Dict [ str , Any ] ) -> Union [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]], bool ]: \"\"\" This must be implemented by the child class Arguments: losses: previously computed losses inputs: holds everything else: tensors, models etc. Returns: Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean \"\"\" pass","title":"call()"},{"location":"hooks/base/base_wrapper_hook/","text":"pytorch_adapt.hooks.base \u00b6 BaseWrapperHook \u00b6 A simple wrapper for calling self.hook , which should be defined in the child's __init__ function.","title":"BaseWrapperHook"},{"location":"hooks/base/base_wrapper_hook/#pytorch_adapt.hooks.base","text":"","title":"base"},{"location":"hooks/base/base_wrapper_hook/#pytorch_adapt.hooks.base.BaseWrapperHook","text":"A simple wrapper for calling self.hook , which should be defined in the child's __init__ function.","title":"BaseWrapperHook"},{"location":"hooks/utils/apply_fn_hook/","text":"pytorch_adapt.hooks.utils \u00b6 ApplyFnHook \u00b6 Applies a function to specific values of the context. __init__ ( self , fn , apply_to , is_loss = False , ** kwargs ) special \u00b6 Parameters: Name Type Description Default fn Callable The function that will be applied to the inputs. required apply_to List[str] fn will be applied to inputs[k] for k in apply_to required is_loss bool If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. False Source code in pytorch_adapt\\hooks\\utils.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def __init__ ( self , fn : Callable , apply_to : List [ str ], is_loss : bool = False , ** kwargs ): \"\"\" Arguments: fn: The function that will be applied to the inputs. apply_to: fn will be applied to ```inputs[k]``` for k in apply_to is_loss: If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. \"\"\" super () . __init__ ( ** kwargs ) self . fn = fn self . apply_to = apply_to self . is_loss = is_loss","title":"ApplyFnHook"},{"location":"hooks/utils/apply_fn_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/apply_fn_hook/#pytorch_adapt.hooks.utils.ApplyFnHook","text":"Applies a function to specific values of the context.","title":"ApplyFnHook"},{"location":"hooks/utils/apply_fn_hook/#pytorch_adapt.hooks.utils.ApplyFnHook.__init__","text":"Parameters: Name Type Description Default fn Callable The function that will be applied to the inputs. required apply_to List[str] fn will be applied to inputs[k] for k in apply_to required is_loss bool If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. False Source code in pytorch_adapt\\hooks\\utils.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def __init__ ( self , fn : Callable , apply_to : List [ str ], is_loss : bool = False , ** kwargs ): \"\"\" Arguments: fn: The function that will be applied to the inputs. apply_to: fn will be applied to ```inputs[k]``` for k in apply_to is_loss: If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. \"\"\" super () . __init__ ( ** kwargs ) self . fn = fn self . apply_to = apply_to self . is_loss = is_loss","title":"__init__()"},{"location":"hooks/utils/assert_hook/","text":"pytorch_adapt.hooks.utils \u00b6 AssertHook \u00b6 Asserts that the output keys of a hook match a specified regex string __init__ ( self , hook , allowed , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The wrapped hook required allowed str The output dictionary of hook must have keys that match the allowed regex. required Source code in pytorch_adapt\\hooks\\utils.py 306 307 308 309 310 311 312 313 314 315 316 317 def __init__ ( self , hook : BaseHook , allowed : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook allowed: The output dictionary of ```hook``` must have keys that match the ```allowed``` regex. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook if not isinstance ( allowed , str ): raise TypeError ( \"allowed must be a str\" ) self . allowed = allowed","title":"AssertHook"},{"location":"hooks/utils/assert_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/assert_hook/#pytorch_adapt.hooks.utils.AssertHook","text":"Asserts that the output keys of a hook match a specified regex string","title":"AssertHook"},{"location":"hooks/utils/assert_hook/#pytorch_adapt.hooks.utils.AssertHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The wrapped hook required allowed str The output dictionary of hook must have keys that match the allowed regex. required Source code in pytorch_adapt\\hooks\\utils.py 306 307 308 309 310 311 312 313 314 315 316 317 def __init__ ( self , hook : BaseHook , allowed : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook allowed: The output dictionary of ```hook``` must have keys that match the ```allowed``` regex. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook if not isinstance ( allowed , str ): raise TypeError ( \"allowed must be a str\" ) self . allowed = allowed","title":"__init__()"},{"location":"hooks/utils/chain_hook/","text":"pytorch_adapt.hooks.utils \u00b6 ChainHook \u00b6 Calls multiple hooks sequentially. The Nth hook receives the context accumulated through hooks 0 to N-1. __init__ ( self , * hooks , * , conditions = None , alts = None , overwrite = False , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially. () conditions List[pytorch_adapt.hooks.base.BaseConditionHook] an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. None alts List[pytorch_adapt.hooks.base.BaseHook] an optional list of hooks that will be executed when the corresponding condition hook returns False None overwrite Union[bool, List[int]] If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. False Source code in pytorch_adapt\\hooks\\utils.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , * hooks : BaseHook , conditions : List [ BaseConditionHook ] = None , alts : List [ BaseHook ] = None , overwrite : Union [ bool , List [ int ]] = False , ** kwargs , ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially. conditions: an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. alts: an optional list of hooks that will be executed when the corresponding condition hook returns False overwrite: If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . conditions = c_f . default ( conditions , [ TrueHook () for _ in range ( len ( hooks ))] ) self . alts = c_f . default ( alts , [ ZeroLossHook ( h . loss_keys , h . out_keys ) for h in self . hooks ] ) self . check_alt_keys_match_hook_keys () if not isinstance ( overwrite , ( list , bool )): raise TypeError ( \"overwrite must be a list or bool\" ) self . overwrite = overwrite self . in_keys = self . hooks [ 0 ] . in_keys","title":"ChainHook"},{"location":"hooks/utils/chain_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/chain_hook/#pytorch_adapt.hooks.utils.ChainHook","text":"Calls multiple hooks sequentially. The Nth hook receives the context accumulated through hooks 0 to N-1.","title":"ChainHook"},{"location":"hooks/utils/chain_hook/#pytorch_adapt.hooks.utils.ChainHook.__init__","text":"Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially. () conditions List[pytorch_adapt.hooks.base.BaseConditionHook] an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. None alts List[pytorch_adapt.hooks.base.BaseHook] an optional list of hooks that will be executed when the corresponding condition hook returns False None overwrite Union[bool, List[int]] If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. False Source code in pytorch_adapt\\hooks\\utils.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , * hooks : BaseHook , conditions : List [ BaseConditionHook ] = None , alts : List [ BaseHook ] = None , overwrite : Union [ bool , List [ int ]] = False , ** kwargs , ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially. conditions: an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. alts: an optional list of hooks that will be executed when the corresponding condition hook returns False overwrite: If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . conditions = c_f . default ( conditions , [ TrueHook () for _ in range ( len ( hooks ))] ) self . alts = c_f . default ( alts , [ ZeroLossHook ( h . loss_keys , h . out_keys ) for h in self . hooks ] ) self . check_alt_keys_match_hook_keys () if not isinstance ( overwrite , ( list , bool )): raise TypeError ( \"overwrite must be a list or bool\" ) self . overwrite = overwrite self . in_keys = self . hooks [ 0 ] . in_keys","title":"__init__()"},{"location":"hooks/utils/empty_hook/","text":"pytorch_adapt.hooks.utils \u00b6 EmptyHook \u00b6 Returns two empty dictionaries.","title":"EmptyHook"},{"location":"hooks/utils/empty_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/empty_hook/#pytorch_adapt.hooks.utils.EmptyHook","text":"Returns two empty dictionaries.","title":"EmptyHook"},{"location":"hooks/utils/false_hook/","text":"pytorch_adapt.hooks.utils \u00b6 FalseHook \u00b6 Returns False","title":"FalseHook"},{"location":"hooks/utils/false_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/false_hook/#pytorch_adapt.hooks.utils.FalseHook","text":"Returns False","title":"FalseHook"},{"location":"hooks/utils/multiplier_hook/","text":"pytorch_adapt.hooks.utils \u00b6 MultiplierHook \u00b6 Multiplies every loss by a scalar __init__ ( self , hook , m , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The losses of this hook will be multiplied by m required m float The scalar required Source code in pytorch_adapt\\hooks\\utils.py 342 343 344 345 346 347 348 349 350 def __init__ ( self , hook : BaseHook , m : float , ** kwargs ): \"\"\" Arguments: hook: The losses of this hook will be multiplied by ```m``` m: The scalar \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . m = m","title":"MultiplierHook"},{"location":"hooks/utils/multiplier_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/multiplier_hook/#pytorch_adapt.hooks.utils.MultiplierHook","text":"Multiplies every loss by a scalar","title":"MultiplierHook"},{"location":"hooks/utils/multiplier_hook/#pytorch_adapt.hooks.utils.MultiplierHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The losses of this hook will be multiplied by m required m float The scalar required Source code in pytorch_adapt\\hooks\\utils.py 342 343 344 345 346 347 348 349 350 def __init__ ( self , hook : BaseHook , m : float , ** kwargs ): \"\"\" Arguments: hook: The losses of this hook will be multiplied by ```m``` m: The scalar \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . m = m","title":"__init__()"},{"location":"hooks/utils/not_hook/","text":"pytorch_adapt.hooks.utils \u00b6 NotHook \u00b6 Returns the boolean negation of the wrapped hook. __init__ ( self , hook , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseConditionHook The condition hook that will be negated. required Source code in pytorch_adapt\\hooks\\utils.py 288 289 290 291 292 293 294 def __init__ ( self , hook : BaseConditionHook , ** kwargs ): \"\"\" Arguments: hook: The condition hook that will be negated. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook","title":"NotHook"},{"location":"hooks/utils/not_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/not_hook/#pytorch_adapt.hooks.utils.NotHook","text":"Returns the boolean negation of the wrapped hook.","title":"NotHook"},{"location":"hooks/utils/not_hook/#pytorch_adapt.hooks.utils.NotHook.__init__","text":"Parameters: Name Type Description Default hook BaseConditionHook The condition hook that will be negated. required Source code in pytorch_adapt\\hooks\\utils.py 288 289 290 291 292 293 294 def __init__ ( self , hook : BaseConditionHook , ** kwargs ): \"\"\" Arguments: hook: The condition hook that will be negated. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook","title":"__init__()"},{"location":"hooks/utils/only_new_outputs_hook/","text":"pytorch_adapt.hooks.utils \u00b6 OnlyNewOutputsHook \u00b6 Returns only outputs that are not present in the input context. You should use this if you want to change the value of a key passed to self.hook, but not propagate that change to the outside. __init__ ( self , hook , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The hook inside which changes to the context will be allowed. required Source code in pytorch_adapt\\hooks\\utils.py 213 214 215 216 217 218 219 def __init__ ( self , hook : BaseHook , ** kwargs ): \"\"\" Arguments: hook: The hook inside which changes to the context will be allowed. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook","title":"OnlyNewOutputsHook"},{"location":"hooks/utils/only_new_outputs_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/only_new_outputs_hook/#pytorch_adapt.hooks.utils.OnlyNewOutputsHook","text":"Returns only outputs that are not present in the input context. You should use this if you want to change the value of a key passed to self.hook, but not propagate that change to the outside.","title":"OnlyNewOutputsHook"},{"location":"hooks/utils/only_new_outputs_hook/#pytorch_adapt.hooks.utils.OnlyNewOutputsHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The hook inside which changes to the context will be allowed. required Source code in pytorch_adapt\\hooks\\utils.py 213 214 215 216 217 218 219 def __init__ ( self , hook : BaseHook , ** kwargs ): \"\"\" Arguments: hook: The hook inside which changes to the context will be allowed. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook","title":"__init__()"},{"location":"hooks/utils/parallel_hook/","text":"pytorch_adapt.hooks.utils \u00b6 ParallelHook \u00b6 Calls multiple hooks while keeping contexts separate. The Nth hook receives the same context as hooks 0 to N-1. All the output contexts are merged at the end. __init__ ( self , * hooks , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. () Source code in pytorch_adapt\\hooks\\utils.py 171 172 173 174 175 176 177 178 179 def __init__ ( self , * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . in_keys = c_f . join_lists ([ h . in_keys for h in self . hooks ])","title":"ParallelHook"},{"location":"hooks/utils/parallel_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/parallel_hook/#pytorch_adapt.hooks.utils.ParallelHook","text":"Calls multiple hooks while keeping contexts separate. The Nth hook receives the same context as hooks 0 to N-1. All the output contexts are merged at the end.","title":"ParallelHook"},{"location":"hooks/utils/parallel_hook/#pytorch_adapt.hooks.utils.ParallelHook.__init__","text":"Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. () Source code in pytorch_adapt\\hooks\\utils.py 171 172 173 174 175 176 177 178 179 def __init__ ( self , * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . in_keys = c_f . join_lists ([ h . in_keys for h in self . hooks ])","title":"__init__()"},{"location":"hooks/utils/repeat_hook/","text":"pytorch_adapt.hooks.utils \u00b6 RepeatHook \u00b6 Executes the wrapped hook n times. __init__ ( self , hook , n , keep_only_last = False , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The hook that will be executed n times required n int The number of times the hook will be executed. required keep_only_last bool If False , the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If True , then only the (losses, outputs) of the final execution will be kept. False Source code in pytorch_adapt\\hooks\\utils.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def __init__ ( self , hook : BaseHook , n : int , keep_only_last : bool = False , ** kwargs ): \"\"\" Arguments: hook: The hook that will be executed ```n``` times n: The number of times the hook will be executed. keep_only_last: If ```False```, the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If ```True```, then only the (losses, outputs) of the final execution will be kept. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . n = n self . keep_only_last = keep_only_last","title":"RepeatHook"},{"location":"hooks/utils/repeat_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/repeat_hook/#pytorch_adapt.hooks.utils.RepeatHook","text":"Executes the wrapped hook n times.","title":"RepeatHook"},{"location":"hooks/utils/repeat_hook/#pytorch_adapt.hooks.utils.RepeatHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The hook that will be executed n times required n int The number of times the hook will be executed. required keep_only_last bool If False , the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If True , then only the (losses, outputs) of the final execution will be kept. False Source code in pytorch_adapt\\hooks\\utils.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def __init__ ( self , hook : BaseHook , n : int , keep_only_last : bool = False , ** kwargs ): \"\"\" Arguments: hook: The hook that will be executed ```n``` times n: The number of times the hook will be executed. keep_only_last: If ```False```, the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If ```True```, then only the (losses, outputs) of the final execution will be kept. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . n = n self . keep_only_last = keep_only_last","title":"__init__()"},{"location":"hooks/utils/true_hook/","text":"pytorch_adapt.hooks.utils \u00b6 TrueHook \u00b6 Returns True","title":"TrueHook"},{"location":"hooks/utils/true_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/true_hook/#pytorch_adapt.hooks.utils.TrueHook","text":"Returns True","title":"TrueHook"},{"location":"hooks/utils/zero_loss_hook/","text":"pytorch_adapt.hooks.utils \u00b6 ZeroLossHook \u00b6 Returns only 0 losses and None outputs. __init__ ( self , loss_names , out_names , ** kwargs ) special \u00b6 Parameters: Name Type Description Default loss_names List[str] The keys of the loss dictionary which will have tensor(0.) as its values. required out_names List[str] The keys of the output dictionary which will have None as its values. required Source code in pytorch_adapt\\hooks\\utils.py 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , loss_names : List [ str ], out_names : List [ str ], ** kwargs ): \"\"\" Arguments: loss_names: The keys of the loss dictionary which will have ```tensor(0.)``` as its values. out_names: The keys of the output dictionary which will have ```None``` as its values. \"\"\" super () . __init__ ( ** kwargs ) self . loss_names = loss_names self . out_names = out_names","title":"ZeroLossHook"},{"location":"hooks/utils/zero_loss_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/zero_loss_hook/#pytorch_adapt.hooks.utils.ZeroLossHook","text":"Returns only 0 losses and None outputs.","title":"ZeroLossHook"},{"location":"hooks/utils/zero_loss_hook/#pytorch_adapt.hooks.utils.ZeroLossHook.__init__","text":"Parameters: Name Type Description Default loss_names List[str] The keys of the loss dictionary which will have tensor(0.) as its values. required out_names List[str] The keys of the output dictionary which will have None as its values. required Source code in pytorch_adapt\\hooks\\utils.py 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , loss_names : List [ str ], out_names : List [ str ], ** kwargs ): \"\"\" Arguments: loss_names: The keys of the loss dictionary which will have ```tensor(0.)``` as its values. out_names: The keys of the output dictionary which will have ```None``` as its values. \"\"\" super () . __init__ ( ** kwargs ) self . loss_names = loss_names self . out_names = out_names","title":"__init__()"},{"location":"layers/","text":"","title":"Layers"},{"location":"layers/abs_loss/","text":"pytorch_adapt.layers.abs_loss \u00b6 AbsLoss \u00b6 The mean absolute value.","title":"AbsLoss"},{"location":"layers/abs_loss/#pytorch_adapt.layers.abs_loss","text":"","title":"abs_loss"},{"location":"layers/abs_loss/#pytorch_adapt.layers.abs_loss.AbsLoss","text":"The mean absolute value.","title":"AbsLoss"},{"location":"layers/adaptive_feature_norm/","text":"pytorch_adapt.layers.adaptive_feature_norm \u00b6 AdaptiveFeatureNorm \u00b6 Implementation of the loss in Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation . Encourages features to gradually have larger and larger L2 norms. __init__ ( self , step_size = 1 ) special \u00b6 Parameters: Name Type Description Default step_size float The desired increase in L2 norm at each iteration. Note that the loss will always be equal to step_size because the goal is always to make the L2 norm step_size larger than whatever the current L2 norm is. 1 Source code in pytorch_adapt\\layers\\adaptive_feature_norm.py 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , step_size : float = 1 ): \"\"\" Arguments: step_size: The desired increase in L2 norm at each iteration. Note that the loss will always be equal to ```step_size``` because the goal is always to make the L2 norm ```step_size``` larger than whatever the current L2 norm is. \"\"\" super () . __init__ () self . step_size = step_size L2PreservedDropout \u00b6 Implementation of the dropout layer described in Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation . Regular dropout preserves the L1 norm of features, whereas this layer preserves the L2 norm. __init__ ( self , p = 0.5 , inplace = False ) special \u00b6 Parameters: Name Type Description Default p float probability of an element to be zeroed 0.5 inplace bool if set to True, will do this operation in-place False Source code in pytorch_adapt\\layers\\adaptive_feature_norm.py 50 51 52 53 54 55 56 57 58 def __init__ ( self , p : float = 0.5 , inplace : bool = False ): \"\"\" Arguments: p: probability of an element to be zeroed inplace: if set to True, will do this operation in-place \"\"\" super () . __init__ () self . dropout = torch . nn . Dropout ( p = p , inplace = inplace ) self . scale = math . sqrt ( 1 - p )","title":"AdaptiveFeatureNorm"},{"location":"layers/adaptive_feature_norm/#pytorch_adapt.layers.adaptive_feature_norm","text":"","title":"adaptive_feature_norm"},{"location":"layers/adaptive_feature_norm/#pytorch_adapt.layers.adaptive_feature_norm.AdaptiveFeatureNorm","text":"Implementation of the loss in Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation . Encourages features to gradually have larger and larger L2 norms.","title":"AdaptiveFeatureNorm"},{"location":"layers/adaptive_feature_norm/#pytorch_adapt.layers.adaptive_feature_norm.AdaptiveFeatureNorm.__init__","text":"Parameters: Name Type Description Default step_size float The desired increase in L2 norm at each iteration. Note that the loss will always be equal to step_size because the goal is always to make the L2 norm step_size larger than whatever the current L2 norm is. 1 Source code in pytorch_adapt\\layers\\adaptive_feature_norm.py 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , step_size : float = 1 ): \"\"\" Arguments: step_size: The desired increase in L2 norm at each iteration. Note that the loss will always be equal to ```step_size``` because the goal is always to make the L2 norm ```step_size``` larger than whatever the current L2 norm is. \"\"\" super () . __init__ () self . step_size = step_size","title":"__init__()"},{"location":"layers/adaptive_feature_norm/#pytorch_adapt.layers.adaptive_feature_norm.L2PreservedDropout","text":"Implementation of the dropout layer described in Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation . Regular dropout preserves the L1 norm of features, whereas this layer preserves the L2 norm.","title":"L2PreservedDropout"},{"location":"layers/adaptive_feature_norm/#pytorch_adapt.layers.adaptive_feature_norm.L2PreservedDropout.__init__","text":"Parameters: Name Type Description Default p float probability of an element to be zeroed 0.5 inplace bool if set to True, will do this operation in-place False Source code in pytorch_adapt\\layers\\adaptive_feature_norm.py 50 51 52 53 54 55 56 57 58 def __init__ ( self , p : float = 0.5 , inplace : bool = False ): \"\"\" Arguments: p: probability of an element to be zeroed inplace: if set to True, will do this operation in-place \"\"\" super () . __init__ () self . dropout = torch . nn . Dropout ( p = p , inplace = inplace ) self . scale = math . sqrt ( 1 - p )","title":"__init__()"},{"location":"layers/batch_spectral_loss/","text":"pytorch_adapt.layers.batch_spectral_loss \u00b6 BatchSpectralLoss \u00b6 Implementation of the loss in Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation . The loss is the sum of the squares of the first k singular values. __init__ ( self , k = 1 ) special \u00b6 Parameters: Name Type Description Default k int the number of singular values to include in the loss 1 Source code in pytorch_adapt\\layers\\batch_spectral_loss.py 19 20 21 22 23 24 25 def __init__ ( self , k : int = 1 ): \"\"\" Arguments: k: the number of singular values to include in the loss \"\"\" super () . __init__ () self . k = k","title":"BatchSpectralLoss"},{"location":"layers/batch_spectral_loss/#pytorch_adapt.layers.batch_spectral_loss","text":"","title":"batch_spectral_loss"},{"location":"layers/batch_spectral_loss/#pytorch_adapt.layers.batch_spectral_loss.BatchSpectralLoss","text":"Implementation of the loss in Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation . The loss is the sum of the squares of the first k singular values.","title":"BatchSpectralLoss"},{"location":"layers/batch_spectral_loss/#pytorch_adapt.layers.batch_spectral_loss.BatchSpectralLoss.__init__","text":"Parameters: Name Type Description Default k int the number of singular values to include in the loss 1 Source code in pytorch_adapt\\layers\\batch_spectral_loss.py 19 20 21 22 23 24 25 def __init__ ( self , k : int = 1 ): \"\"\" Arguments: k: the number of singular values to include in the loss \"\"\" super () . __init__ () self . k = k","title":"__init__()"},{"location":"layers/bnm_loss/","text":"pytorch_adapt.layers.bnm_loss \u00b6 BNMLoss \u00b6 Implementation of the loss in Towards Discriminability and Diversity: Batch Nuclear-norm Maximization under Label Insufficient Situations .","title":"BNMLoss"},{"location":"layers/bnm_loss/#pytorch_adapt.layers.bnm_loss","text":"","title":"bnm_loss"},{"location":"layers/bnm_loss/#pytorch_adapt.layers.bnm_loss.BNMLoss","text":"Implementation of the loss in Towards Discriminability and Diversity: Batch Nuclear-norm Maximization under Label Insufficient Situations .","title":"BNMLoss"},{"location":"layers/concat_softmax/","text":"pytorch_adapt.layers.concat_softmax \u00b6 ConcatSoftmax \u00b6 Applies softmax to the concatenation of a list of tensors. __init__ ( self , dim = 1 ) special \u00b6 Parameters: Name Type Description Default dim int a dimension along which softmax will be computed 1 Source code in pytorch_adapt\\layers\\concat_softmax.py 11 12 13 14 15 16 17 def __init__ ( self , dim : int = 1 ): \"\"\" Arguments: dim: a dimension along which softmax will be computed \"\"\" super () . __init__ () self . dim = dim forward ( self , * x ) \u00b6 Parameters: Name Type Description Default *x Tensor A sequence of tensors to be concatenated () Source code in pytorch_adapt\\layers\\concat_softmax.py 19 20 21 22 23 24 25 def forward ( self , * x : torch . Tensor ): \"\"\" Arguments: *x: A sequence of tensors to be concatenated \"\"\" all_logits = torch . cat ( x , dim = self . dim ) return torch . nn . functional . softmax ( all_logits , dim = self . dim )","title":"ConcatSoftmax"},{"location":"layers/concat_softmax/#pytorch_adapt.layers.concat_softmax","text":"","title":"concat_softmax"},{"location":"layers/concat_softmax/#pytorch_adapt.layers.concat_softmax.ConcatSoftmax","text":"Applies softmax to the concatenation of a list of tensors.","title":"ConcatSoftmax"},{"location":"layers/concat_softmax/#pytorch_adapt.layers.concat_softmax.ConcatSoftmax.__init__","text":"Parameters: Name Type Description Default dim int a dimension along which softmax will be computed 1 Source code in pytorch_adapt\\layers\\concat_softmax.py 11 12 13 14 15 16 17 def __init__ ( self , dim : int = 1 ): \"\"\" Arguments: dim: a dimension along which softmax will be computed \"\"\" super () . __init__ () self . dim = dim","title":"__init__()"},{"location":"layers/concat_softmax/#pytorch_adapt.layers.concat_softmax.ConcatSoftmax.forward","text":"Parameters: Name Type Description Default *x Tensor A sequence of tensors to be concatenated () Source code in pytorch_adapt\\layers\\concat_softmax.py 19 20 21 22 23 24 25 def forward ( self , * x : torch . Tensor ): \"\"\" Arguments: *x: A sequence of tensors to be concatenated \"\"\" all_logits = torch . cat ( x , dim = self . dim ) return torch . nn . functional . softmax ( all_logits , dim = self . dim )","title":"forward()"},{"location":"layers/confidence_weights/","text":"pytorch_adapt.layers.confidence_weights \u00b6 ConfidenceWeights \u00b6 Returns the max value along each row of the input, followed by an optional normalization function. The output of this can be used to weight classification losses by the \"confidence\" of the predictions. __init__ ( self , normalizer = None ) special \u00b6 Parameters: Name Type Description Default normalizer Callable[[torch.Tensor], torch.Tensor] A callable for normalizing (e.g. min-max normalization) the weights. If None , then no normalization is used. None Source code in pytorch_adapt\\layers\\confidence_weights.py 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , normalizer : Callable [[ torch . Tensor ], torch . Tensor ] = None ): \"\"\" Arguments: normalizer: A callable for normalizing (e.g. min-max normalization) the weights. If ```None```, then no normalization is used. \"\"\" super () . __init__ () self . normalizer = c_f . default ( normalizer , NoNormalizer ())","title":"ConfidenceWeights"},{"location":"layers/confidence_weights/#pytorch_adapt.layers.confidence_weights","text":"","title":"confidence_weights"},{"location":"layers/confidence_weights/#pytorch_adapt.layers.confidence_weights.ConfidenceWeights","text":"Returns the max value along each row of the input, followed by an optional normalization function. The output of this can be used to weight classification losses by the \"confidence\" of the predictions.","title":"ConfidenceWeights"},{"location":"layers/confidence_weights/#pytorch_adapt.layers.confidence_weights.ConfidenceWeights.__init__","text":"Parameters: Name Type Description Default normalizer Callable[[torch.Tensor], torch.Tensor] A callable for normalizing (e.g. min-max normalization) the weights. If None , then no normalization is used. None Source code in pytorch_adapt\\layers\\confidence_weights.py 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , normalizer : Callable [[ torch . Tensor ], torch . Tensor ] = None ): \"\"\" Arguments: normalizer: A callable for normalizing (e.g. min-max normalization) the weights. If ```None```, then no normalization is used. \"\"\" super () . __init__ () self . normalizer = c_f . default ( normalizer , NoNormalizer ())","title":"__init__()"},{"location":"layers/coral_loss/","text":"pytorch_adapt.layers.coral_loss \u00b6 CORALLoss \u00b6 Implementation of Deep CORAL: Correlation Alignment for Deep Domain Adaptation forward ( self , x , y ) \u00b6 Parameters: Name Type Description Default x Tensor features from one domain required y Tensor features from the other domain required Source code in pytorch_adapt\\layers\\coral_loss.py 19 20 21 22 23 24 25 26 27 28 29 def forward ( self , x : torch . Tensor , y : torch . Tensor ): \"\"\" Arguments: x: features from one domain y: features from the other domain \"\"\" embedding_size = x . shape [ 1 ] cx = covariance ( x ) cy = covariance ( y ) squared_fro_norm = torch . linalg . norm ( cx - cy , ord = \"fro\" ) ** 2 return squared_fro_norm / ( 4 * ( embedding_size ** 2 ))","title":"CORALLoss"},{"location":"layers/coral_loss/#pytorch_adapt.layers.coral_loss","text":"","title":"coral_loss"},{"location":"layers/coral_loss/#pytorch_adapt.layers.coral_loss.CORALLoss","text":"Implementation of Deep CORAL: Correlation Alignment for Deep Domain Adaptation","title":"CORALLoss"},{"location":"layers/coral_loss/#pytorch_adapt.layers.coral_loss.CORALLoss.forward","text":"Parameters: Name Type Description Default x Tensor features from one domain required y Tensor features from the other domain required Source code in pytorch_adapt\\layers\\coral_loss.py 19 20 21 22 23 24 25 26 27 28 29 def forward ( self , x : torch . Tensor , y : torch . Tensor ): \"\"\" Arguments: x: features from one domain y: features from the other domain \"\"\" embedding_size = x . shape [ 1 ] cx = covariance ( x ) cy = covariance ( y ) squared_fro_norm = torch . linalg . norm ( cx - cy , ord = \"fro\" ) ** 2 return squared_fro_norm / ( 4 * ( embedding_size ** 2 ))","title":"forward()"},{"location":"layers/diversity_loss/","text":"pytorch_adapt.layers.diversity_loss \u00b6 DiversityLoss \u00b6 Encourages predictions to be uniform, batch wise. Takes logits (before softmax) as input. For example: A tensor with a large loss: torch.tensor([[1e4, 0, 0], [1e4, 0, 0], [1e4, 0, 0]]) A tensor with a small loss: torch.tensor([[1e4, 0, 0], [0, 1e4, 0], [0, 0, 1e4]])","title":"DiversityLoss"},{"location":"layers/diversity_loss/#pytorch_adapt.layers.diversity_loss","text":"","title":"diversity_loss"},{"location":"layers/diversity_loss/#pytorch_adapt.layers.diversity_loss.DiversityLoss","text":"Encourages predictions to be uniform, batch wise. Takes logits (before softmax) as input. For example: A tensor with a large loss: torch.tensor([[1e4, 0, 0], [1e4, 0, 0], [1e4, 0, 0]]) A tensor with a small loss: torch.tensor([[1e4, 0, 0], [0, 1e4, 0], [0, 0, 1e4]])","title":"DiversityLoss"},{"location":"layers/do_nothing_optimizer/","text":"pytorch_adapt.layers.do_nothing_optimizer \u00b6 DoNothingOptimizer \u00b6 An optimizer that doesn't do anything, i.e. step and zero_grad are empty functions.","title":"DoNothingOptimizer"},{"location":"layers/do_nothing_optimizer/#pytorch_adapt.layers.do_nothing_optimizer","text":"","title":"do_nothing_optimizer"},{"location":"layers/do_nothing_optimizer/#pytorch_adapt.layers.do_nothing_optimizer.DoNothingOptimizer","text":"An optimizer that doesn't do anything, i.e. step and zero_grad are empty functions.","title":"DoNothingOptimizer"},{"location":"layers/entropy_loss/","text":"pytorch_adapt.layers.entropy_loss \u00b6 EntropyLoss \u00b6 Encourages low entropy predictions, or in other words, \"confident\" predictions. __init__ ( self , after_softmax = False , return_mean = True ) special \u00b6 Parameters: Name Type Description Default after_softmax bool If True , then the rows of the input are assumed to already have softmax applied to them. False return_mean bool If True , the mean entropy will be returned. If False , the entropy per row of the input will be returned. True Source code in pytorch_adapt\\layers\\entropy_loss.py 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , after_softmax : bool = False , return_mean : bool = True ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. return_mean: If ```True```, the mean entropy will be returned. If ```False```, the entropy per row of the input will be returned. \"\"\" super () . __init__ () self . after_softmax = after_softmax self . return_mean = return_mean forward ( self , logits ) \u00b6 Parameters: Name Type Description Default logits Tensor Raw logits if self.after_softmax is False. Otherwise each row should be predictions that sum up to 1. required Source code in pytorch_adapt\\layers\\entropy_loss.py 43 44 45 46 47 48 49 50 51 52 def forward ( self , logits : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: logits: Raw logits if ```self.after_softmax``` is False. Otherwise each row should be predictions that sum up to 1. \"\"\" entropies = get_entropy ( logits , self . after_softmax ) if self . return_mean : return torch . mean ( entropies ) return entropies","title":"EntropyLoss"},{"location":"layers/entropy_loss/#pytorch_adapt.layers.entropy_loss","text":"","title":"entropy_loss"},{"location":"layers/entropy_loss/#pytorch_adapt.layers.entropy_loss.EntropyLoss","text":"Encourages low entropy predictions, or in other words, \"confident\" predictions.","title":"EntropyLoss"},{"location":"layers/entropy_loss/#pytorch_adapt.layers.entropy_loss.EntropyLoss.__init__","text":"Parameters: Name Type Description Default after_softmax bool If True , then the rows of the input are assumed to already have softmax applied to them. False return_mean bool If True , the mean entropy will be returned. If False , the entropy per row of the input will be returned. True Source code in pytorch_adapt\\layers\\entropy_loss.py 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , after_softmax : bool = False , return_mean : bool = True ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. return_mean: If ```True```, the mean entropy will be returned. If ```False```, the entropy per row of the input will be returned. \"\"\" super () . __init__ () self . after_softmax = after_softmax self . return_mean = return_mean","title":"__init__()"},{"location":"layers/entropy_loss/#pytorch_adapt.layers.entropy_loss.EntropyLoss.forward","text":"Parameters: Name Type Description Default logits Tensor Raw logits if self.after_softmax is False. Otherwise each row should be predictions that sum up to 1. required Source code in pytorch_adapt\\layers\\entropy_loss.py 43 44 45 46 47 48 49 50 51 52 def forward ( self , logits : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: logits: Raw logits if ```self.after_softmax``` is False. Otherwise each row should be predictions that sum up to 1. \"\"\" entropies = get_entropy ( logits , self . after_softmax ) if self . return_mean : return torch . mean ( entropies ) return entropies","title":"forward()"},{"location":"layers/entropy_weights/","text":"pytorch_adapt.layers.entropy_weights \u00b6 EntropyWeights \u00b6 Implementation of entropy weighting described in Conditional Adversarial Domain Adaptation . Computes the entropy ( x ) per row of the input, and returns 1+exp(-x) . This can be used to weight losses, such that the most confidently scored samples have a higher weighting. __init__ ( self , after_softmax = False , normalizer = None ) special \u00b6 Parameters: Name Type Description Default after_softmax bool If True , then the rows of the input are assumed to already have softmax applied to them. False normalizer Callable[[torch.Tensor], torch.Tensor] A callable for normalizing (e.g. min-max normalization) the weights. If None , then sum normalization is used. None Source code in pytorch_adapt\\layers\\entropy_weights.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , after_softmax : bool = False , normalizer : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. normalizer: A callable for normalizing (e.g. min-max normalization) the weights. If ```None```, then sum normalization is used. \"\"\" super () . __init__ () self . after_softmax = after_softmax self . normalizer = c_f . default ( normalizer , sum_normalizer ) forward ( self , logits ) \u00b6 Parameters: Name Type Description Default logits Tensor Raw logits if self.after_softmax is False. Otherwise each row should be predictions that sum up to 1. required Source code in pytorch_adapt\\layers\\entropy_weights.py 43 44 45 46 47 48 49 def forward ( self , logits : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: logits: Raw logits if ```self.after_softmax``` is False. Otherwise each row should be predictions that sum up to 1. \"\"\" return entropy_weights ( logits , self . after_softmax , self . normalizer )","title":"EntropyWeights"},{"location":"layers/entropy_weights/#pytorch_adapt.layers.entropy_weights","text":"","title":"entropy_weights"},{"location":"layers/entropy_weights/#pytorch_adapt.layers.entropy_weights.EntropyWeights","text":"Implementation of entropy weighting described in Conditional Adversarial Domain Adaptation . Computes the entropy ( x ) per row of the input, and returns 1+exp(-x) . This can be used to weight losses, such that the most confidently scored samples have a higher weighting.","title":"EntropyWeights"},{"location":"layers/entropy_weights/#pytorch_adapt.layers.entropy_weights.EntropyWeights.__init__","text":"Parameters: Name Type Description Default after_softmax bool If True , then the rows of the input are assumed to already have softmax applied to them. False normalizer Callable[[torch.Tensor], torch.Tensor] A callable for normalizing (e.g. min-max normalization) the weights. If None , then sum normalization is used. None Source code in pytorch_adapt\\layers\\entropy_weights.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , after_softmax : bool = False , normalizer : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. normalizer: A callable for normalizing (e.g. min-max normalization) the weights. If ```None```, then sum normalization is used. \"\"\" super () . __init__ () self . after_softmax = after_softmax self . normalizer = c_f . default ( normalizer , sum_normalizer )","title":"__init__()"},{"location":"layers/entropy_weights/#pytorch_adapt.layers.entropy_weights.EntropyWeights.forward","text":"Parameters: Name Type Description Default logits Tensor Raw logits if self.after_softmax is False. Otherwise each row should be predictions that sum up to 1. required Source code in pytorch_adapt\\layers\\entropy_weights.py 43 44 45 46 47 48 49 def forward ( self , logits : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: logits: Raw logits if ```self.after_softmax``` is False. Otherwise each row should be predictions that sum up to 1. \"\"\" return entropy_weights ( logits , self . after_softmax , self . normalizer )","title":"forward()"},{"location":"layers/gradient_reversal/","text":"pytorch_adapt.layers.gradient_reversal \u00b6 GradientReversal \u00b6 Implementation of the gradient reversal layer described in Domain-Adversarial Training of Neural Networks , which 'leaves the input unchanged during forward propagation and reverses the gradient by multiplying it by a negative scalar during backpropagation.' __init__ ( self , weight = 1.0 ) special \u00b6 Parameters: Name Type Description Default weight float The gradients will be multiplied by -weight during the backward pass. 1.0 Source code in pytorch_adapt\\layers\\gradient_reversal.py 16 17 18 19 20 21 22 23 24 def __init__ ( self , weight : float = 1.0 ): \"\"\" Arguments: weight: The gradients will be multiplied by ```-weight``` during the backward pass. \"\"\" super () . __init__ () self . register_buffer ( \"weight\" , torch . tensor ([ weight ])) pml_cf . add_to_recordable_attributes ( self , \"weight\" )","title":"GradientReversal"},{"location":"layers/gradient_reversal/#pytorch_adapt.layers.gradient_reversal","text":"","title":"gradient_reversal"},{"location":"layers/gradient_reversal/#pytorch_adapt.layers.gradient_reversal.GradientReversal","text":"Implementation of the gradient reversal layer described in Domain-Adversarial Training of Neural Networks , which 'leaves the input unchanged during forward propagation and reverses the gradient by multiplying it by a negative scalar during backpropagation.'","title":"GradientReversal"},{"location":"layers/gradient_reversal/#pytorch_adapt.layers.gradient_reversal.GradientReversal.__init__","text":"Parameters: Name Type Description Default weight float The gradients will be multiplied by -weight during the backward pass. 1.0 Source code in pytorch_adapt\\layers\\gradient_reversal.py 16 17 18 19 20 21 22 23 24 def __init__ ( self , weight : float = 1.0 ): \"\"\" Arguments: weight: The gradients will be multiplied by ```-weight``` during the backward pass. \"\"\" super () . __init__ () self . register_buffer ( \"weight\" , torch . tensor ([ weight ])) pml_cf . add_to_recordable_attributes ( self , \"weight\" )","title":"__init__()"},{"location":"layers/mcc_loss/","text":"pytorch_adapt.layers.mcc_loss \u00b6 MCCLoss \u00b6 Implementation of Minimum Class Confusion for Versatile Domain Adaptation . __init__ ( self , T = 1 , entropy_weighter = None ) special \u00b6 Parameters: Name Type Description Default T float softmax temperature applied to the input target logits 1 entropy_weighter Callable[[torch.Tensor], torch.Tensor] a function that returns a weight for each sample. The weights are used in the process of computing the class confusion tensor as described in the paper. If None , then layers.EntropyWeights is used. None Source code in pytorch_adapt\\layers\\mcc_loss.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , T : float = 1 , entropy_weighter : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: T: softmax temperature applied to the input target logits entropy_weighter: a function that returns a weight for each sample. The weights are used in the process of computing the class confusion tensor as described in the paper. If ```None```, then ```layers.EntropyWeights``` is used. \"\"\" super () . __init__ () self . T = T self . entropy_weighter = c_f . default ( entropy_weighter , EntropyWeights ( after_softmax = True , normalizer = SumNormalizer ( scale_by_batch_size = True ) ), ) forward ( self , x ) \u00b6 Parameters: Name Type Description Default x Tensor target logits required Source code in pytorch_adapt\\layers\\mcc_loss.py 40 41 42 43 44 45 46 47 48 49 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: target logits \"\"\" Y = torch . nn . functional . softmax ( x / self . T , dim = 1 ) H_weights = self . entropy_weighter ( Y . detach ()) C = torch . linalg . multi_dot ([ Y . t (), torch . diag ( H_weights ), Y ]) C = C / torch . sum ( C , dim = 1 ) return ( torch . sum ( C ) - torch . trace ( C )) / C . shape [ 0 ]","title":"MCCLoss"},{"location":"layers/mcc_loss/#pytorch_adapt.layers.mcc_loss","text":"","title":"mcc_loss"},{"location":"layers/mcc_loss/#pytorch_adapt.layers.mcc_loss.MCCLoss","text":"Implementation of Minimum Class Confusion for Versatile Domain Adaptation .","title":"MCCLoss"},{"location":"layers/mcc_loss/#pytorch_adapt.layers.mcc_loss.MCCLoss.__init__","text":"Parameters: Name Type Description Default T float softmax temperature applied to the input target logits 1 entropy_weighter Callable[[torch.Tensor], torch.Tensor] a function that returns a weight for each sample. The weights are used in the process of computing the class confusion tensor as described in the paper. If None , then layers.EntropyWeights is used. None Source code in pytorch_adapt\\layers\\mcc_loss.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , T : float = 1 , entropy_weighter : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: T: softmax temperature applied to the input target logits entropy_weighter: a function that returns a weight for each sample. The weights are used in the process of computing the class confusion tensor as described in the paper. If ```None```, then ```layers.EntropyWeights``` is used. \"\"\" super () . __init__ () self . T = T self . entropy_weighter = c_f . default ( entropy_weighter , EntropyWeights ( after_softmax = True , normalizer = SumNormalizer ( scale_by_batch_size = True ) ), )","title":"__init__()"},{"location":"layers/mcc_loss/#pytorch_adapt.layers.mcc_loss.MCCLoss.forward","text":"Parameters: Name Type Description Default x Tensor target logits required Source code in pytorch_adapt\\layers\\mcc_loss.py 40 41 42 43 44 45 46 47 48 49 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: target logits \"\"\" Y = torch . nn . functional . softmax ( x / self . T , dim = 1 ) H_weights = self . entropy_weighter ( Y . detach ()) C = torch . linalg . multi_dot ([ Y . t (), torch . diag ( H_weights ), Y ]) C = C / torch . sum ( C , dim = 1 ) return ( torch . sum ( C ) - torch . trace ( C )) / C . shape [ 0 ]","title":"forward()"},{"location":"layers/mcd_loss/","text":"pytorch_adapt.layers.mcd_loss \u00b6 MCDLoss \u00b6 Implementation of the loss function used in Maximum Classifier Discrepancy for Unsupervised Domain Adaptation . __init__ ( self , dist_fn = None ) special \u00b6 Parameters: Name Type Description Default dist_fn Callable[[torch.Tensor], torch.Tensor] Computes the mean distance between two softmaxed tensors. If None , then torch.nn.L1Loss is used. None Source code in pytorch_adapt\\layers\\mcd_loss.py 19 20 21 22 23 24 25 26 def __init__ ( self , dist_fn : Callable [[ torch . Tensor ], torch . Tensor ] = None ): \"\"\" Arguments: dist_fn: Computes the mean distance between two softmaxed tensors. If ```None```, then ```torch.nn.L1Loss``` is used. \"\"\" super () . __init__ () self . dist_fn = c_f . default ( dist_fn , torch . nn . L1Loss , {}) forward ( self , x , y ) \u00b6 Parameters: Name Type Description Default x Tensor a batch of class logits required y Tensor the other batch of class logits required Returns: Type Description Tensor The discrepancy between the two batches of class logits. Source code in pytorch_adapt\\layers\\mcd_loss.py 28 29 30 31 32 33 34 35 36 def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: a batch of class logits y: the other batch of class logits Returns: The discrepancy between the two batches of class logits. \"\"\" return mcd_loss ( x , y , self . dist_fn )","title":"MCDLoss"},{"location":"layers/mcd_loss/#pytorch_adapt.layers.mcd_loss","text":"","title":"mcd_loss"},{"location":"layers/mcd_loss/#pytorch_adapt.layers.mcd_loss.MCDLoss","text":"Implementation of the loss function used in Maximum Classifier Discrepancy for Unsupervised Domain Adaptation .","title":"MCDLoss"},{"location":"layers/mcd_loss/#pytorch_adapt.layers.mcd_loss.MCDLoss.__init__","text":"Parameters: Name Type Description Default dist_fn Callable[[torch.Tensor], torch.Tensor] Computes the mean distance between two softmaxed tensors. If None , then torch.nn.L1Loss is used. None Source code in pytorch_adapt\\layers\\mcd_loss.py 19 20 21 22 23 24 25 26 def __init__ ( self , dist_fn : Callable [[ torch . Tensor ], torch . Tensor ] = None ): \"\"\" Arguments: dist_fn: Computes the mean distance between two softmaxed tensors. If ```None```, then ```torch.nn.L1Loss``` is used. \"\"\" super () . __init__ () self . dist_fn = c_f . default ( dist_fn , torch . nn . L1Loss , {})","title":"__init__()"},{"location":"layers/mcd_loss/#pytorch_adapt.layers.mcd_loss.MCDLoss.forward","text":"Parameters: Name Type Description Default x Tensor a batch of class logits required y Tensor the other batch of class logits required Returns: Type Description Tensor The discrepancy between the two batches of class logits. Source code in pytorch_adapt\\layers\\mcd_loss.py 28 29 30 31 32 33 34 35 36 def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: a batch of class logits y: the other batch of class logits Returns: The discrepancy between the two batches of class logits. \"\"\" return mcd_loss ( x , y , self . dist_fn )","title":"forward()"},{"location":"layers/mmd_loss/","text":"pytorch_adapt.layers.mmd_loss \u00b6 MMDLoss \u00b6 Implementation of Learning Transferable Features with Deep Adaptation Networks Deep Transfer Learning with Joint Adaptation Networks . __init__ ( self , kernel_scales = 1 , mmd_type = 'linear' ) special \u00b6 Parameters: Name Type Description Default kernel_scales Union[float, torch.Tensor] The kernel bandwidth is scaled by this amount. If a tensor, then multiple kernel bandwidths are used. 1 mmd_type str 'linear' or 'quadratic'. 'linear' uses the linear estimate of MK-MMD. 'linear' Source code in pytorch_adapt\\layers\\mmd_loss.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , kernel_scales : Union [ float , torch . Tensor ] = 1 , mmd_type : str = \"linear\" ): \"\"\" Arguments: kernel_scales: The kernel bandwidth is scaled by this amount. If a tensor, then multiple kernel bandwidths are used. mmd_type: 'linear' or 'quadratic'. 'linear' uses the linear estimate of MK-MMD. \"\"\" super () . __init__ () self . kernel_scales = kernel_scales self . dist_func = LpDistance ( normalize_embeddings = False , p = 2 , power = 2 ) self . mmd_type = mmd_type if mmd_type == \"linear\" : self . mmd_func = l_u . get_mmd_linear elif mmd_type == \"quadratic\" : self . mmd_func = l_u . get_mmd_quadratic else : raise ValueError ( \"mmd_type must be either linear or quadratic\" ) forward ( self , x , y ) \u00b6 Parameters: Name Type Description Default x Union[torch.Tensor, List[torch.Tensor]] features or a list of features from one domain. required y Union[torch.Tensor, List[torch.Tensor]] features or a list of features from the other domain. required Returns: Type Description Tensor MMD if the inputs are tensors, and Joint MMD (JMMD) if the inputs are lists of tensors. Source code in pytorch_adapt\\layers\\mmd_loss.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def forward ( self , x : Union [ torch . Tensor , List [ torch . Tensor ]], y : Union [ torch . Tensor , List [ torch . Tensor ]], ) -> torch . Tensor : \"\"\" Arguments: x: features or a list of features from one domain. y: features or a list of features from the other domain. Returns: MMD if the inputs are tensors, and Joint MMD (JMMD) if the inputs are lists of tensors. \"\"\" xx , yy , zz , scale = l_u . get_mmd_dist_mats ( x , y , self . dist_func ) if torch . is_tensor ( self . kernel_scales ): s = scale [ 0 ] if c_f . is_list_or_tuple ( scale ) else scale self . kernel_scales = pml_cf . to_device ( self . kernel_scales , s , dtype = s . dtype ) if c_f . is_list_or_tuple ( scale ): for i in range ( len ( scale )): scale [ i ] = scale [ i ] * self . kernel_scales else : scale = scale * self . kernel_scales return self . mmd_func ( xx , yy , zz , scale )","title":"MMDLoss"},{"location":"layers/mmd_loss/#pytorch_adapt.layers.mmd_loss","text":"","title":"mmd_loss"},{"location":"layers/mmd_loss/#pytorch_adapt.layers.mmd_loss.MMDLoss","text":"Implementation of Learning Transferable Features with Deep Adaptation Networks Deep Transfer Learning with Joint Adaptation Networks .","title":"MMDLoss"},{"location":"layers/mmd_loss/#pytorch_adapt.layers.mmd_loss.MMDLoss.__init__","text":"Parameters: Name Type Description Default kernel_scales Union[float, torch.Tensor] The kernel bandwidth is scaled by this amount. If a tensor, then multiple kernel bandwidths are used. 1 mmd_type str 'linear' or 'quadratic'. 'linear' uses the linear estimate of MK-MMD. 'linear' Source code in pytorch_adapt\\layers\\mmd_loss.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , kernel_scales : Union [ float , torch . Tensor ] = 1 , mmd_type : str = \"linear\" ): \"\"\" Arguments: kernel_scales: The kernel bandwidth is scaled by this amount. If a tensor, then multiple kernel bandwidths are used. mmd_type: 'linear' or 'quadratic'. 'linear' uses the linear estimate of MK-MMD. \"\"\" super () . __init__ () self . kernel_scales = kernel_scales self . dist_func = LpDistance ( normalize_embeddings = False , p = 2 , power = 2 ) self . mmd_type = mmd_type if mmd_type == \"linear\" : self . mmd_func = l_u . get_mmd_linear elif mmd_type == \"quadratic\" : self . mmd_func = l_u . get_mmd_quadratic else : raise ValueError ( \"mmd_type must be either linear or quadratic\" )","title":"__init__()"},{"location":"layers/mmd_loss/#pytorch_adapt.layers.mmd_loss.MMDLoss.forward","text":"Parameters: Name Type Description Default x Union[torch.Tensor, List[torch.Tensor]] features or a list of features from one domain. required y Union[torch.Tensor, List[torch.Tensor]] features or a list of features from the other domain. required Returns: Type Description Tensor MMD if the inputs are tensors, and Joint MMD (JMMD) if the inputs are lists of tensors. Source code in pytorch_adapt\\layers\\mmd_loss.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def forward ( self , x : Union [ torch . Tensor , List [ torch . Tensor ]], y : Union [ torch . Tensor , List [ torch . Tensor ]], ) -> torch . Tensor : \"\"\" Arguments: x: features or a list of features from one domain. y: features or a list of features from the other domain. Returns: MMD if the inputs are tensors, and Joint MMD (JMMD) if the inputs are lists of tensors. \"\"\" xx , yy , zz , scale = l_u . get_mmd_dist_mats ( x , y , self . dist_func ) if torch . is_tensor ( self . kernel_scales ): s = scale [ 0 ] if c_f . is_list_or_tuple ( scale ) else scale self . kernel_scales = pml_cf . to_device ( self . kernel_scales , s , dtype = s . dtype ) if c_f . is_list_or_tuple ( scale ): for i in range ( len ( scale )): scale [ i ] = scale [ i ] * self . kernel_scales else : scale = scale * self . kernel_scales return self . mmd_func ( xx , yy , zz , scale )","title":"forward()"},{"location":"layers/model_with_bridge/","text":"pytorch_adapt.layers.model_with_bridge \u00b6 ModelWithBridge \u00b6 Implementation of the bridge architecture described in Gradually Vanishing Bridge for Adversarial Domain Adaptation . __init__ ( self , model , bridge = None ) special \u00b6 Parameters: Name Type Description Default model Module Any pytorch model. required bridge Module A model which has the same input/output sizes as model . If None , then the bridge is formed by copying model , and randomly reinitialization all its parameters. None Source code in pytorch_adapt\\layers\\model_with_bridge.py 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , model : torch . nn . Module , bridge : torch . nn . Module = None ): \"\"\" Arguments: model: Any pytorch model. bridge: A model which has the same input/output sizes as ```model```. If ```None```, then the bridge is formed by copying ```model```, and randomly reinitialization all its parameters. \"\"\" super () . __init__ () self . model = model if bridge is None : bridge = c_f . reinit ( copy . deepcopy ( model )) self . bridge = bridge forward ( self , x , return_bridge = False ) \u00b6 Parameters: Name Type Description Default x Tensor The input to both self.model and self.bridge . required return_bridge bool Whether or not to return the bridge output in addition to the model - bridge output False Returns: Type Description Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] If return_bridge = False , then return just model - bridge . If return_bridge = True , then return a tuple of (model - bridge), bridge Source code in pytorch_adapt\\layers\\model_with_bridge.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def forward ( self , x : torch . Tensor , return_bridge : bool = False ) -> Union [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]]: \"\"\" Arguments: x: The input to both ```self.model``` and ```self.bridge```. return_bridge: Whether or not to return the bridge output in addition to the ```model - bridge``` output Returns: If ```return_bridge = False```, then return just ```model - bridge```. If ```return_bridge = True```, then return a tuple of ```(model - bridge), bridge``` \"\"\" y = self . model ( x ) z = self . bridge ( x ) output = y - z if return_bridge : return output , z return output","title":"ModelWithBridge"},{"location":"layers/model_with_bridge/#pytorch_adapt.layers.model_with_bridge","text":"","title":"model_with_bridge"},{"location":"layers/model_with_bridge/#pytorch_adapt.layers.model_with_bridge.ModelWithBridge","text":"Implementation of the bridge architecture described in Gradually Vanishing Bridge for Adversarial Domain Adaptation .","title":"ModelWithBridge"},{"location":"layers/model_with_bridge/#pytorch_adapt.layers.model_with_bridge.ModelWithBridge.__init__","text":"Parameters: Name Type Description Default model Module Any pytorch model. required bridge Module A model which has the same input/output sizes as model . If None , then the bridge is formed by copying model , and randomly reinitialization all its parameters. None Source code in pytorch_adapt\\layers\\model_with_bridge.py 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , model : torch . nn . Module , bridge : torch . nn . Module = None ): \"\"\" Arguments: model: Any pytorch model. bridge: A model which has the same input/output sizes as ```model```. If ```None```, then the bridge is formed by copying ```model```, and randomly reinitialization all its parameters. \"\"\" super () . __init__ () self . model = model if bridge is None : bridge = c_f . reinit ( copy . deepcopy ( model )) self . bridge = bridge","title":"__init__()"},{"location":"layers/model_with_bridge/#pytorch_adapt.layers.model_with_bridge.ModelWithBridge.forward","text":"Parameters: Name Type Description Default x Tensor The input to both self.model and self.bridge . required return_bridge bool Whether or not to return the bridge output in addition to the model - bridge output False Returns: Type Description Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] If return_bridge = False , then return just model - bridge . If return_bridge = True , then return a tuple of (model - bridge), bridge Source code in pytorch_adapt\\layers\\model_with_bridge.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def forward ( self , x : torch . Tensor , return_bridge : bool = False ) -> Union [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]]: \"\"\" Arguments: x: The input to both ```self.model``` and ```self.bridge```. return_bridge: Whether or not to return the bridge output in addition to the ```model - bridge``` output Returns: If ```return_bridge = False```, then return just ```model - bridge```. If ```return_bridge = True```, then return a tuple of ```(model - bridge), bridge``` \"\"\" y = self . model ( x ) z = self . bridge ( x ) output = y - z if return_bridge : return output , z return output","title":"forward()"},{"location":"layers/multiple_models/","text":"pytorch_adapt.layers.multiple_models \u00b6 MultipleModels \u00b6 Wraps a list of models, and returns their outputs as a list of tensors __init__ ( self , * models ) special \u00b6 Parameters: Name Type Description Default models Module The models to be wrapped. () Source code in pytorch_adapt\\layers\\multiple_models.py 11 12 13 14 15 16 17 def __init__ ( self , * models : torch . nn . Module ): \"\"\" Arguments: models: The models to be wrapped. \"\"\" super () . __init__ () self . models = torch . nn . ModuleList ( models ) forward ( self , x ) \u00b6 Parameters: Name Type Description Default x Tensor the input to each model required Returns: Type Description List[Any] A list containing the output of each model. Source code in pytorch_adapt\\layers\\multiple_models.py 19 20 21 22 23 24 25 26 27 28 29 def forward ( self , x : torch . Tensor ) -> List [ Any ]: \"\"\" Arguments: x: the input to each model Returns: A list containing the output of each model. \"\"\" outputs = [] for m in self . models : outputs . append ( m ( x )) return outputs","title":"MultipleModels"},{"location":"layers/multiple_models/#pytorch_adapt.layers.multiple_models","text":"","title":"multiple_models"},{"location":"layers/multiple_models/#pytorch_adapt.layers.multiple_models.MultipleModels","text":"Wraps a list of models, and returns their outputs as a list of tensors","title":"MultipleModels"},{"location":"layers/multiple_models/#pytorch_adapt.layers.multiple_models.MultipleModels.__init__","text":"Parameters: Name Type Description Default models Module The models to be wrapped. () Source code in pytorch_adapt\\layers\\multiple_models.py 11 12 13 14 15 16 17 def __init__ ( self , * models : torch . nn . Module ): \"\"\" Arguments: models: The models to be wrapped. \"\"\" super () . __init__ () self . models = torch . nn . ModuleList ( models )","title":"__init__()"},{"location":"layers/multiple_models/#pytorch_adapt.layers.multiple_models.MultipleModels.forward","text":"Parameters: Name Type Description Default x Tensor the input to each model required Returns: Type Description List[Any] A list containing the output of each model. Source code in pytorch_adapt\\layers\\multiple_models.py 19 20 21 22 23 24 25 26 27 28 29 def forward ( self , x : torch . Tensor ) -> List [ Any ]: \"\"\" Arguments: x: the input to each model Returns: A list containing the output of each model. \"\"\" outputs = [] for m in self . models : outputs . append ( m ( x )) return outputs","title":"forward()"},{"location":"layers/neighborhood_aggregation/","text":"pytorch_adapt.layers.neighborhood_aggregation \u00b6 NeighborhoodAggregation \u00b6 Implementation of the pseudo labeling step in Domain Adaptation with Auxiliary Target Domain-Oriented Classifier . __init__ ( self , dataset_size , feature_dim , num_classes , k = 5 , T = 0.5 ) special \u00b6 Parameters: Name Type Description Default dataset_size int The number of samples in the target dataset. required feature_dim int The feature dimensionality, i.e at each iteration the features should be size (N, D) where N is batch size and D is feature_dim . required num_classes int The number of class labels in the target dataset. required k int The number of nearest neighbors used to determine each sample's pseudolabel 5 T float The softmax temperature used when storing predictions in memory. 0.5 Source code in pytorch_adapt\\layers\\neighborhood_aggregation.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , dataset_size : int , feature_dim : int , num_classes : int , k : int = 5 , T : float = 0.5 , ): \"\"\" Arguments: dataset_size: The number of samples in the target dataset. feature_dim: The feature dimensionality, i.e at each iteration the features should be size ```(N, D)``` where N is batch size and D is ```feature_dim```. num_classes: The number of class labels in the target dataset. k: The number of nearest neighbors used to determine each sample's pseudolabel T: The softmax temperature used when storing predictions in memory. \"\"\" super () . __init__ () self . register_buffer ( \"feat_memory\" , F . normalize ( torch . rand ( dataset_size , feature_dim )) ) self . register_buffer ( \"pred_memory\" , torch . ones ( dataset_size , num_classes ) / num_classes ) self . k = k self . T = T forward ( self , features , logits = None , update = False , idx = None ) \u00b6 Parameters: Name Type Description Default features Tensor The features to compute pseudolabels for. required logits Tensor The logits from which predictions will be computed and stored in memory. Required if update = True None update bool If True, the current batch of predictions is added to the memory bank. False idx Tensor A tensor containing the dataset indices that produced each row of features . None Source code in pytorch_adapt\\layers\\neighborhood_aggregation.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def forward ( self , features : torch . Tensor , logits : torch . Tensor = None , update : bool = False , idx : torch . Tensor = None , ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Arguments: features: The features to compute pseudolabels for. logits: The logits from which predictions will be computed and stored in memory. Required if ```update = True``` update: If True, the current batch of predictions is added to the memory bank. idx: A tensor containing the dataset indices that produced each row of ```features```. \"\"\" # move to device if necessary self . feat_memory = pml_cf . to_device ( self . feat_memory , features ) self . pred_memory = pml_cf . to_device ( self . pred_memory , features ) with torch . no_grad (): features = F . normalize ( features ) pseudo_labels , mean_logits = self . get_pseudo_labels ( features , idx ) if update : self . update_memory ( features , logits , idx ) return pseudo_labels , mean_logits","title":"NeighborhoodAggregation"},{"location":"layers/neighborhood_aggregation/#pytorch_adapt.layers.neighborhood_aggregation","text":"","title":"neighborhood_aggregation"},{"location":"layers/neighborhood_aggregation/#pytorch_adapt.layers.neighborhood_aggregation.NeighborhoodAggregation","text":"Implementation of the pseudo labeling step in Domain Adaptation with Auxiliary Target Domain-Oriented Classifier .","title":"NeighborhoodAggregation"},{"location":"layers/neighborhood_aggregation/#pytorch_adapt.layers.neighborhood_aggregation.NeighborhoodAggregation.__init__","text":"Parameters: Name Type Description Default dataset_size int The number of samples in the target dataset. required feature_dim int The feature dimensionality, i.e at each iteration the features should be size (N, D) where N is batch size and D is feature_dim . required num_classes int The number of class labels in the target dataset. required k int The number of nearest neighbors used to determine each sample's pseudolabel 5 T float The softmax temperature used when storing predictions in memory. 0.5 Source code in pytorch_adapt\\layers\\neighborhood_aggregation.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , dataset_size : int , feature_dim : int , num_classes : int , k : int = 5 , T : float = 0.5 , ): \"\"\" Arguments: dataset_size: The number of samples in the target dataset. feature_dim: The feature dimensionality, i.e at each iteration the features should be size ```(N, D)``` where N is batch size and D is ```feature_dim```. num_classes: The number of class labels in the target dataset. k: The number of nearest neighbors used to determine each sample's pseudolabel T: The softmax temperature used when storing predictions in memory. \"\"\" super () . __init__ () self . register_buffer ( \"feat_memory\" , F . normalize ( torch . rand ( dataset_size , feature_dim )) ) self . register_buffer ( \"pred_memory\" , torch . ones ( dataset_size , num_classes ) / num_classes ) self . k = k self . T = T","title":"__init__()"},{"location":"layers/neighborhood_aggregation/#pytorch_adapt.layers.neighborhood_aggregation.NeighborhoodAggregation.forward","text":"Parameters: Name Type Description Default features Tensor The features to compute pseudolabels for. required logits Tensor The logits from which predictions will be computed and stored in memory. Required if update = True None update bool If True, the current batch of predictions is added to the memory bank. False idx Tensor A tensor containing the dataset indices that produced each row of features . None Source code in pytorch_adapt\\layers\\neighborhood_aggregation.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def forward ( self , features : torch . Tensor , logits : torch . Tensor = None , update : bool = False , idx : torch . Tensor = None , ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Arguments: features: The features to compute pseudolabels for. logits: The logits from which predictions will be computed and stored in memory. Required if ```update = True``` update: If True, the current batch of predictions is added to the memory bank. idx: A tensor containing the dataset indices that produced each row of ```features```. \"\"\" # move to device if necessary self . feat_memory = pml_cf . to_device ( self . feat_memory , features ) self . pred_memory = pml_cf . to_device ( self . pred_memory , features ) with torch . no_grad (): features = F . normalize ( features ) pseudo_labels , mean_logits = self . get_pseudo_labels ( features , idx ) if update : self . update_memory ( features , logits , idx ) return pseudo_labels , mean_logits","title":"forward()"},{"location":"layers/plus_residual/","text":"pytorch_adapt.layers.plus_residual \u00b6 PlusResidual \u00b6 Wraps a layer such that the forward pass returns x + self.layer(x) __init__ ( self , layer ) special \u00b6 Parameters: Name Type Description Default layer Module The layer to be wrapped. required Source code in pytorch_adapt\\layers\\plus_residual.py 10 11 12 13 14 15 16 def __init__ ( self , layer : torch . nn . Module ): \"\"\" Arguments: layer: The layer to be wrapped. \"\"\" super () . __init__ () self . layer = layer","title":"PlusResidual"},{"location":"layers/plus_residual/#pytorch_adapt.layers.plus_residual","text":"","title":"plus_residual"},{"location":"layers/plus_residual/#pytorch_adapt.layers.plus_residual.PlusResidual","text":"Wraps a layer such that the forward pass returns x + self.layer(x)","title":"PlusResidual"},{"location":"layers/plus_residual/#pytorch_adapt.layers.plus_residual.PlusResidual.__init__","text":"Parameters: Name Type Description Default layer Module The layer to be wrapped. required Source code in pytorch_adapt\\layers\\plus_residual.py 10 11 12 13 14 15 16 def __init__ ( self , layer : torch . nn . Module ): \"\"\" Arguments: layer: The layer to be wrapped. \"\"\" super () . __init__ () self . layer = layer","title":"__init__()"},{"location":"layers/randomized_dot_product/","text":"pytorch_adapt.layers.randomized_dot_product \u00b6 RandomizedDotProduct \u00b6 Implementation of randomized multilinear conditioning from Conditional Adversarial Domain Adaptation . __init__ ( self , in_dims , out_dim = 1024 ) special \u00b6 Parameters: Name Type Description Default in_dims List[int] A list of the feature dims. For example, if the input features have shapes (32, 512) and (32, 64) , then in_dims = [512, 64] . required out_dim int The output feature dim. 1024 Source code in pytorch_adapt\\layers\\randomized_dot_product.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , in_dims : List [ int ], out_dim : int = 1024 ): \"\"\" Arguments: in_dims: A list of the feature dims. For example, if the input features have shapes ```(32, 512)``` and ```(32, 64)```, then ```in_dims = [512, 64]```. out_dim: The output feature dim. \"\"\" super () . __init__ () self . in_dims = in_dims for i , d in enumerate ( in_dims ): self . register_buffer ( self . rand_mat_name ( i ), torch . randn ( d , out_dim )) self . out_dim = out_dim self . num_mats = len ( in_dims ) self . divisor = math . pow ( float ( self . out_dim ), 1.0 / self . num_mats ) forward ( self , * inputs ) \u00b6 Parameters: Name Type Description Default inputs Tensor The number of inputs must be equal to the length of self.in_dims . () Source code in pytorch_adapt\\layers\\randomized_dot_product.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def forward ( self , * inputs : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: inputs: The number of inputs must be equal to the length of ```self.in_dims```. \"\"\" for i in range ( self . num_mats ): # move to device if necessary curr = inputs [ i ] self . set_rand_mat ( i , pml_cf . to_device ( self . get_rand_mat ( i ), curr , dtype = curr . dtype ) ) return_list = [ torch . mm ( inputs [ i ], self . get_rand_mat ( i )) for i in range ( self . num_mats ) ] return_tensor = return_list [ 0 ] / self . divisor for single in return_list [ 1 :]: return_tensor = return_tensor * single return return_tensor","title":"RandomizedDotProduct"},{"location":"layers/randomized_dot_product/#pytorch_adapt.layers.randomized_dot_product","text":"","title":"randomized_dot_product"},{"location":"layers/randomized_dot_product/#pytorch_adapt.layers.randomized_dot_product.RandomizedDotProduct","text":"Implementation of randomized multilinear conditioning from Conditional Adversarial Domain Adaptation .","title":"RandomizedDotProduct"},{"location":"layers/randomized_dot_product/#pytorch_adapt.layers.randomized_dot_product.RandomizedDotProduct.__init__","text":"Parameters: Name Type Description Default in_dims List[int] A list of the feature dims. For example, if the input features have shapes (32, 512) and (32, 64) , then in_dims = [512, 64] . required out_dim int The output feature dim. 1024 Source code in pytorch_adapt\\layers\\randomized_dot_product.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , in_dims : List [ int ], out_dim : int = 1024 ): \"\"\" Arguments: in_dims: A list of the feature dims. For example, if the input features have shapes ```(32, 512)``` and ```(32, 64)```, then ```in_dims = [512, 64]```. out_dim: The output feature dim. \"\"\" super () . __init__ () self . in_dims = in_dims for i , d in enumerate ( in_dims ): self . register_buffer ( self . rand_mat_name ( i ), torch . randn ( d , out_dim )) self . out_dim = out_dim self . num_mats = len ( in_dims ) self . divisor = math . pow ( float ( self . out_dim ), 1.0 / self . num_mats )","title":"__init__()"},{"location":"layers/randomized_dot_product/#pytorch_adapt.layers.randomized_dot_product.RandomizedDotProduct.forward","text":"Parameters: Name Type Description Default inputs Tensor The number of inputs must be equal to the length of self.in_dims . () Source code in pytorch_adapt\\layers\\randomized_dot_product.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def forward ( self , * inputs : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: inputs: The number of inputs must be equal to the length of ```self.in_dims```. \"\"\" for i in range ( self . num_mats ): # move to device if necessary curr = inputs [ i ] self . set_rand_mat ( i , pml_cf . to_device ( self . get_rand_mat ( i ), curr , dtype = curr . dtype ) ) return_list = [ torch . mm ( inputs [ i ], self . get_rand_mat ( i )) for i in range ( self . num_mats ) ] return_tensor = return_list [ 0 ] / self . divisor for single in return_list [ 1 :]: return_tensor = return_tensor * single return return_tensor","title":"forward()"},{"location":"layers/silhouette_score/","text":"pytorch_adapt.layers.silhouette_score \u00b6 SilhouetteScore \u00b6 A PyTorch implementation of the silhouette score","title":"SilhouetteScore"},{"location":"layers/silhouette_score/#pytorch_adapt.layers.silhouette_score","text":"","title":"silhouette_score"},{"location":"layers/silhouette_score/#pytorch_adapt.layers.silhouette_score.SilhouetteScore","text":"A PyTorch implementation of the silhouette score","title":"SilhouetteScore"},{"location":"layers/sliced_wasserstein/","text":"pytorch_adapt.layers.sliced_wasserstein \u00b6 SlicedWasserstein \u00b6 Implementation of the loss used in Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation __init__ ( self , m = 128 ) special \u00b6 Parameters: Name Type Description Default m int The dimensionality to project to. 128 Source code in pytorch_adapt\\layers\\sliced_wasserstein.py 12 13 14 15 16 17 18 def __init__ ( self , m : int = 128 ): \"\"\" Arguments: m: The dimensionality to project to. \"\"\" super () . __init__ () self . m = 128 forward ( self , x , y ) \u00b6 Parameters: Name Type Description Default x Tensor a batch of class predictions required y Tensor the other batch of class predictions required Returns: Type Description Tensor The discrepancy between the two batches of class predictions. Source code in pytorch_adapt\\layers\\sliced_wasserstein.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: a batch of class predictions y: the other batch of class predictions Returns: The discrepancy between the two batches of class predictions. \"\"\" d = x . shape [ 1 ] proj = torch . randn ( d , self . m , device = x . device ) proj = torch . nn . functional . normalize ( proj , dim = 0 ) x = torch . matmul ( x , proj ) y = torch . matmul ( y , proj ) x , _ = torch . sort ( x , dim = 0 ) y , _ = torch . sort ( y , dim = 0 ) return torch . mean (( x - y ) ** 2 )","title":"SlicedWasserstein"},{"location":"layers/sliced_wasserstein/#pytorch_adapt.layers.sliced_wasserstein","text":"","title":"sliced_wasserstein"},{"location":"layers/sliced_wasserstein/#pytorch_adapt.layers.sliced_wasserstein.SlicedWasserstein","text":"Implementation of the loss used in Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation","title":"SlicedWasserstein"},{"location":"layers/sliced_wasserstein/#pytorch_adapt.layers.sliced_wasserstein.SlicedWasserstein.__init__","text":"Parameters: Name Type Description Default m int The dimensionality to project to. 128 Source code in pytorch_adapt\\layers\\sliced_wasserstein.py 12 13 14 15 16 17 18 def __init__ ( self , m : int = 128 ): \"\"\" Arguments: m: The dimensionality to project to. \"\"\" super () . __init__ () self . m = 128","title":"__init__()"},{"location":"layers/sliced_wasserstein/#pytorch_adapt.layers.sliced_wasserstein.SlicedWasserstein.forward","text":"Parameters: Name Type Description Default x Tensor a batch of class predictions required y Tensor the other batch of class predictions required Returns: Type Description Tensor The discrepancy between the two batches of class predictions. Source code in pytorch_adapt\\layers\\sliced_wasserstein.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: a batch of class predictions y: the other batch of class predictions Returns: The discrepancy between the two batches of class predictions. \"\"\" d = x . shape [ 1 ] proj = torch . randn ( d , self . m , device = x . device ) proj = torch . nn . functional . normalize ( proj , dim = 0 ) x = torch . matmul ( x , proj ) y = torch . matmul ( y , proj ) x , _ = torch . sort ( x , dim = 0 ) y , _ = torch . sort ( y , dim = 0 ) return torch . mean (( x - y ) ** 2 )","title":"forward()"},{"location":"layers/stochastic_linear/","text":"pytorch_adapt.layers.stochastic_linear \u00b6 StochasticLinear \u00b6 Implementation of the stochastic layer from Stochastic Classifiers for Unsupervised Domain Adaptation . In train() mode, it uses random weights and biases that are sampled from a learned normal distribution. In eval() mode, the learned mean is used. __init__ ( self , in_features , out_features , device = None , dtype = None ) special \u00b6 Parameters: Name Type Description Default in_features int size of each input sample required out_features int size of each output sample required Source code in pytorch_adapt\\layers\\stochastic_linear.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , in_features : int , out_features : int , device = None , dtype = None ): \"\"\" Arguments: in_features: size of each input sample out_features: size of each output sample \"\"\" factory_kwargs = { \"device\" : device , \"dtype\" : dtype } super () . __init__ () self . in_features = in_features self . out_features = out_features self . weight_mean = torch . nn . Parameter ( torch . rand ( in_features , out_features , ** factory_kwargs ) ) self . weight_sigma = torch . nn . Parameter ( torch . rand ( in_features , out_features , ** factory_kwargs ) ) self . bias_mean = torch . nn . Parameter ( torch . rand ( out_features , ** factory_kwargs )) self . bias_sigma = torch . nn . Parameter ( torch . rand ( out_features , ** factory_kwargs ))","title":"StochasticLinear"},{"location":"layers/stochastic_linear/#pytorch_adapt.layers.stochastic_linear","text":"","title":"stochastic_linear"},{"location":"layers/stochastic_linear/#pytorch_adapt.layers.stochastic_linear.StochasticLinear","text":"Implementation of the stochastic layer from Stochastic Classifiers for Unsupervised Domain Adaptation . In train() mode, it uses random weights and biases that are sampled from a learned normal distribution. In eval() mode, the learned mean is used.","title":"StochasticLinear"},{"location":"layers/stochastic_linear/#pytorch_adapt.layers.stochastic_linear.StochasticLinear.__init__","text":"Parameters: Name Type Description Default in_features int size of each input sample required out_features int size of each output sample required Source code in pytorch_adapt\\layers\\stochastic_linear.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , in_features : int , out_features : int , device = None , dtype = None ): \"\"\" Arguments: in_features: size of each input sample out_features: size of each output sample \"\"\" factory_kwargs = { \"device\" : device , \"dtype\" : dtype } super () . __init__ () self . in_features = in_features self . out_features = out_features self . weight_mean = torch . nn . Parameter ( torch . rand ( in_features , out_features , ** factory_kwargs ) ) self . weight_sigma = torch . nn . Parameter ( torch . rand ( in_features , out_features , ** factory_kwargs ) ) self . bias_mean = torch . nn . Parameter ( torch . rand ( out_features , ** factory_kwargs )) self . bias_sigma = torch . nn . Parameter ( torch . rand ( out_features , ** factory_kwargs ))","title":"__init__()"},{"location":"layers/sufficient_accuracy/","text":"pytorch_adapt.layers.sufficient_accuracy \u00b6 SufficientAccuracy \u00b6 Determines if a batch of logits has accuracy greater than some threshold. This can be used to control program flow. Examples: condition_fn = SufficientAccuracy ( threshold = 0.7 ) if condition_fn ( logits , labels ): ... __init__ ( self , threshold , accuracy_func = None , to_probs_func = None ) special \u00b6 Parameters: Name Type Description Default threshold float The accuracy must be greater than this for the forward pass to return True. required accuracy_func Callable[[torch.Tensor, torch.Tensor], torch.Tensor] function that takes in (to_probs_func(logits), labels) and returns accuracy. If None , then classification accuracy is used. None to_probs_func Callable[[torch.Tensor], torch.Tensor] function that processes the logits before they get passed to accuracy_func . If None , then torch.nn.Sigmoid is used None Source code in pytorch_adapt\\layers\\sufficient_accuracy.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , threshold : float , accuracy_func : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , to_probs_func : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: threshold: The accuracy must be greater than this for the forward pass to return True. accuracy_func: function that takes in ```(to_probs_func(logits), labels)``` and returns accuracy. If ```None```, then classification accuracy is used. to_probs_func: function that processes the logits before they get passed to ```accuracy_func```. If ```None```, then ```torch.nn.Sigmoid``` is used \"\"\" super () . __init__ () self . threshold = threshold self . accuracy_func = c_f . default ( accuracy_func , accuracy ) self . to_probs_func = c_f . default ( to_probs_func , torch . nn . Sigmoid ()) pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"accuracy\" , \"threshold\" ] ) forward ( self , x , labels ) \u00b6 Parameters: Name Type Description Default x Tensor logits to compute accuracy for required labels Tensor the corresponding labels required Returns: Type Description bool True if the accuracy is greater than self.threshold Source code in pytorch_adapt\\layers\\sufficient_accuracy.py 48 49 50 51 52 53 54 55 56 57 58 59 60 def forward ( self , x : torch . Tensor , labels : torch . Tensor ) -> bool : \"\"\" Arguments: x: logits to compute accuracy for labels: the corresponding labels Returns: ```True``` if the accuracy is greater than ```self.threshold``` \"\"\" with torch . no_grad (): x = self . to_probs_func ( x ) labels = labels . type ( torch . int ) self . accuracy = self . accuracy_func ( x , labels ) . item () return self . accuracy > self . threshold","title":"SufficientAccuracy"},{"location":"layers/sufficient_accuracy/#pytorch_adapt.layers.sufficient_accuracy","text":"","title":"sufficient_accuracy"},{"location":"layers/sufficient_accuracy/#pytorch_adapt.layers.sufficient_accuracy.SufficientAccuracy","text":"Determines if a batch of logits has accuracy greater than some threshold. This can be used to control program flow. Examples: condition_fn = SufficientAccuracy ( threshold = 0.7 ) if condition_fn ( logits , labels ): ...","title":"SufficientAccuracy"},{"location":"layers/sufficient_accuracy/#pytorch_adapt.layers.sufficient_accuracy.SufficientAccuracy.__init__","text":"Parameters: Name Type Description Default threshold float The accuracy must be greater than this for the forward pass to return True. required accuracy_func Callable[[torch.Tensor, torch.Tensor], torch.Tensor] function that takes in (to_probs_func(logits), labels) and returns accuracy. If None , then classification accuracy is used. None to_probs_func Callable[[torch.Tensor], torch.Tensor] function that processes the logits before they get passed to accuracy_func . If None , then torch.nn.Sigmoid is used None Source code in pytorch_adapt\\layers\\sufficient_accuracy.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , threshold : float , accuracy_func : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , to_probs_func : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: threshold: The accuracy must be greater than this for the forward pass to return True. accuracy_func: function that takes in ```(to_probs_func(logits), labels)``` and returns accuracy. If ```None```, then classification accuracy is used. to_probs_func: function that processes the logits before they get passed to ```accuracy_func```. If ```None```, then ```torch.nn.Sigmoid``` is used \"\"\" super () . __init__ () self . threshold = threshold self . accuracy_func = c_f . default ( accuracy_func , accuracy ) self . to_probs_func = c_f . default ( to_probs_func , torch . nn . Sigmoid ()) pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"accuracy\" , \"threshold\" ] )","title":"__init__()"},{"location":"layers/sufficient_accuracy/#pytorch_adapt.layers.sufficient_accuracy.SufficientAccuracy.forward","text":"Parameters: Name Type Description Default x Tensor logits to compute accuracy for required labels Tensor the corresponding labels required Returns: Type Description bool True if the accuracy is greater than self.threshold Source code in pytorch_adapt\\layers\\sufficient_accuracy.py 48 49 50 51 52 53 54 55 56 57 58 59 60 def forward ( self , x : torch . Tensor , labels : torch . Tensor ) -> bool : \"\"\" Arguments: x: logits to compute accuracy for labels: the corresponding labels Returns: ```True``` if the accuracy is greater than ```self.threshold``` \"\"\" with torch . no_grad (): x = self . to_probs_func ( x ) labels = labels . type ( torch . int ) self . accuracy = self . accuracy_func ( x , labels ) . item () return self . accuracy > self . threshold","title":"forward()"},{"location":"layers/uniform_distribution_loss/","text":"pytorch_adapt.layers.uniform_distribution_loss \u00b6 UniformDistributionLoss \u00b6 Implementation of the confusion loss from Simultaneous Deep Transfer Across Domains and Tasks .","title":"UniformDistributionLoss"},{"location":"layers/uniform_distribution_loss/#pytorch_adapt.layers.uniform_distribution_loss","text":"","title":"uniform_distribution_loss"},{"location":"layers/uniform_distribution_loss/#pytorch_adapt.layers.uniform_distribution_loss.UniformDistributionLoss","text":"Implementation of the confusion loss from Simultaneous Deep Transfer Across Domains and Tasks .","title":"UniformDistributionLoss"},{"location":"layers/vat_loss/","text":"pytorch_adapt.layers.vat_loss \u00b6 VATLoss \u00b6 Implementation of the loss used in Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning A DIRT-T Approach to Unsupervised Domain Adaptation __init__ ( self , num_power_iterations = 1 , xi = 1e-06 , epsilon = 8.0 ) special \u00b6 Parameters: Name Type Description Default num_power_iterations int The number of iterations for computing the approximation of the adversarial perturbation. 1 xi float The L2 norm of the the generated noise which is used in the process of creating the perturbation. 1e-06 epsilon float The L2 norm of the generated perturbation. 8.0 Source code in pytorch_adapt\\layers\\vat_loss.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , num_power_iterations : int = 1 , xi : float = 1e-6 , epsilon : float = 8.0 ): \"\"\" Arguments: num_power_iterations: The number of iterations for computing the approximation of the adversarial perturbation. xi: The L2 norm of the the generated noise which is used in the process of creating the perturbation. epsilon: The L2 norm of the generated perturbation. \"\"\" super () . __init__ () self . num_power_iterations = num_power_iterations self . xi = xi self . epsilon = epsilon self . kl_div = torch . nn . KLDivLoss ( reduction = \"batchmean\" ) pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"num_power_iterations\" , \"xi\" , \"epsilon\" ] ) forward ( self , imgs , logits , model ) \u00b6 Parameters: Name Type Description Default imgs Tensor The input to the model required logits Tensor The model's logits computed from imgs required model Module The aforementioned model required Source code in pytorch_adapt\\layers\\vat_loss.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def forward ( self , imgs : torch . Tensor , logits : torch . Tensor , model : torch . nn . Module ) -> torch . Tensor : \"\"\" Arguments: imgs: The input to the model logits: The model's logits computed from ```imgs``` model: The aforementioned model \"\"\" logits = logits . detach () model . apply ( c_f . set_layers_mode ( \"eval\" , c_f . batchnorm_types ())) perturbation = self . get_perturbation ( imgs , logits , model ) new_logits = model ( imgs + perturbation ) preds = F . softmax ( logits , dim = 1 ) new_preds = F . log_softmax ( new_logits , dim = 1 ) model . apply ( c_f . set_layers_mode ( \"train\" , c_f . batchnorm_types ())) return self . kl_div ( new_preds , preds )","title":"VATLoss"},{"location":"layers/vat_loss/#pytorch_adapt.layers.vat_loss","text":"","title":"vat_loss"},{"location":"layers/vat_loss/#pytorch_adapt.layers.vat_loss.VATLoss","text":"Implementation of the loss used in Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning A DIRT-T Approach to Unsupervised Domain Adaptation","title":"VATLoss"},{"location":"layers/vat_loss/#pytorch_adapt.layers.vat_loss.VATLoss.__init__","text":"Parameters: Name Type Description Default num_power_iterations int The number of iterations for computing the approximation of the adversarial perturbation. 1 xi float The L2 norm of the the generated noise which is used in the process of creating the perturbation. 1e-06 epsilon float The L2 norm of the generated perturbation. 8.0 Source code in pytorch_adapt\\layers\\vat_loss.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , num_power_iterations : int = 1 , xi : float = 1e-6 , epsilon : float = 8.0 ): \"\"\" Arguments: num_power_iterations: The number of iterations for computing the approximation of the adversarial perturbation. xi: The L2 norm of the the generated noise which is used in the process of creating the perturbation. epsilon: The L2 norm of the generated perturbation. \"\"\" super () . __init__ () self . num_power_iterations = num_power_iterations self . xi = xi self . epsilon = epsilon self . kl_div = torch . nn . KLDivLoss ( reduction = \"batchmean\" ) pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"num_power_iterations\" , \"xi\" , \"epsilon\" ] )","title":"__init__()"},{"location":"layers/vat_loss/#pytorch_adapt.layers.vat_loss.VATLoss.forward","text":"Parameters: Name Type Description Default imgs Tensor The input to the model required logits Tensor The model's logits computed from imgs required model Module The aforementioned model required Source code in pytorch_adapt\\layers\\vat_loss.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def forward ( self , imgs : torch . Tensor , logits : torch . Tensor , model : torch . nn . Module ) -> torch . Tensor : \"\"\" Arguments: imgs: The input to the model logits: The model's logits computed from ```imgs``` model: The aforementioned model \"\"\" logits = logits . detach () model . apply ( c_f . set_layers_mode ( \"eval\" , c_f . batchnorm_types ())) perturbation = self . get_perturbation ( imgs , logits , model ) new_logits = model ( imgs + perturbation ) preds = F . softmax ( logits , dim = 1 ) new_preds = F . log_softmax ( new_logits , dim = 1 ) model . apply ( c_f . set_layers_mode ( \"train\" , c_f . batchnorm_types ())) return self . kl_div ( new_preds , preds )","title":"forward()"},{"location":"meta_validators/","text":"","title":"Meta Validators"},{"location":"meta_validators/forward_only_validator/","text":"pytorch_adapt.meta_validators.forward_only_validator \u00b6 ForwardOnlyValidator \u00b6 This is basically a pass-through function. It returns the best score and best epoch that is returned by the inner adapter. run ( self , adapter , ** kwargs ) \u00b6 Parameters: Name Type Description Default adapter the framework-wrapped adapter. required **kwargs keyword arguments to be passed into adapter.run() {} Returns: Type Description Tuple[float, int] the best score and best epoch Source code in pytorch_adapt\\meta_validators\\forward_only_validator.py 11 12 13 14 15 16 17 18 19 20 21 22 23 def run ( self , adapter , ** kwargs ) -> Tuple [ float , int ]: \"\"\" Arguments: adapter: the framework-wrapped adapter. **kwargs: keyword arguments to be passed into adapter.run() Returns: the best score and best epoch \"\"\" if \"validator\" not in kwargs : raise KeyError ( \"An adapter validator is required when using ForwardOnlyValidator\" ) return adapter . run ( ** kwargs )","title":"ForwardOnlyValidator"},{"location":"meta_validators/forward_only_validator/#pytorch_adapt.meta_validators.forward_only_validator","text":"","title":"forward_only_validator"},{"location":"meta_validators/forward_only_validator/#pytorch_adapt.meta_validators.forward_only_validator.ForwardOnlyValidator","text":"This is basically a pass-through function. It returns the best score and best epoch that is returned by the inner adapter.","title":"ForwardOnlyValidator"},{"location":"meta_validators/forward_only_validator/#pytorch_adapt.meta_validators.forward_only_validator.ForwardOnlyValidator.run","text":"Parameters: Name Type Description Default adapter the framework-wrapped adapter. required **kwargs keyword arguments to be passed into adapter.run() {} Returns: Type Description Tuple[float, int] the best score and best epoch Source code in pytorch_adapt\\meta_validators\\forward_only_validator.py 11 12 13 14 15 16 17 18 19 20 21 22 23 def run ( self , adapter , ** kwargs ) -> Tuple [ float , int ]: \"\"\" Arguments: adapter: the framework-wrapped adapter. **kwargs: keyword arguments to be passed into adapter.run() Returns: the best score and best epoch \"\"\" if \"validator\" not in kwargs : raise KeyError ( \"An adapter validator is required when using ForwardOnlyValidator\" ) return adapter . run ( ** kwargs )","title":"run()"},{"location":"meta_validators/reverse_validator/","text":"pytorch_adapt.meta_validators.reverse_validator \u00b6 ReverseValidator \u00b6 Reverse validation consists of three steps. Train a model on the labeled source and unlabeled target Use the trained model to create pseudolabels for the target dataset. Train a new model on the labeled target and \"unlabeled\" source. The final score is the accuracy of the model from step 3. run ( self , forward_adapter , reverse_adapter , forward_kwargs , reverse_kwargs , pl_dataloader_creator = None ) \u00b6 Parameters: Name Type Description Default forward_adapter the framework-wrapped adapter for step 1. required reverse_adapter the framework-wrapped adapter for step 3. required forward_kwargs a dict of keyword arguments to be passed to forward_adapter.run() required reverse_kwargs a dict of keyword arguments to be passed to reverse_adapter.run() required pl_dataloader_creator An optional DataloaderCreator for obtaining pseudolabels in step 2. None Returns: Type Description Tuple[float, int] the best score and best epoch of the reverse model Source code in pytorch_adapt\\meta_validators\\reverse_validator.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def run ( self , forward_adapter , reverse_adapter , forward_kwargs , reverse_kwargs , pl_dataloader_creator = None , ) -> Tuple [ float , int ]: \"\"\" Arguments: forward_adapter: the framework-wrapped adapter for step 1. reverse_adapter: the framework-wrapped adapter for step 3. forward_kwargs: a dict of keyword arguments to be passed to forward_adapter.run() reverse_kwargs: a dict of keyword arguments to be passed to reverse_adapter.run() pl_dataloader_creator: An optional DataloaderCreator for obtaining pseudolabels in step 2. Returns: the best score and best epoch of the reverse model \"\"\" if \"datasets\" in reverse_kwargs : raise KeyError ( \"'datasets' should not be in reverse_kwargs because the reverse datasets will be pseudo labeled.\" ) if \"validator\" not in reverse_kwargs : raise KeyError ( \"reverse_kwargs must include 'validator'\" ) forward_adapter . run ( ** forward_kwargs ) if all ( x in forward_kwargs for x in [ \"validator\" , \"saver\" ]): forward_kwargs [ \"saver\" ] . load_adapter ( forward_adapter . adapter , \"best\" ) datasets = forward_kwargs [ \"datasets\" ] pl_dataloader_creator = c_f . default ( pl_dataloader_creator , DataloaderCreator , { \"all_val\" : True } ) d = {} d [ \"src_train\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_train\" , pl_dataloader_creator ) d [ \"src_val\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_val\" , pl_dataloader_creator ) d [ \"target_train\" ] = TargetDataset ( datasets [ \"src_train\" ] . dataset ) d [ \"target_val\" ] = TargetDataset ( datasets [ \"src_val\" ] . dataset ) d [ \"train\" ] = CombinedSourceAndTargetDataset ( d [ \"src_train\" ], d [ \"target_train\" ]) self . pseudo_train = d [ \"src_train\" ] self . pseudo_val = d [ \"src_val\" ] reverse_kwargs [ \"datasets\" ] = d return reverse_adapter . run ( ** reverse_kwargs )","title":"ReverseValidator"},{"location":"meta_validators/reverse_validator/#pytorch_adapt.meta_validators.reverse_validator","text":"","title":"reverse_validator"},{"location":"meta_validators/reverse_validator/#pytorch_adapt.meta_validators.reverse_validator.ReverseValidator","text":"Reverse validation consists of three steps. Train a model on the labeled source and unlabeled target Use the trained model to create pseudolabels for the target dataset. Train a new model on the labeled target and \"unlabeled\" source. The final score is the accuracy of the model from step 3.","title":"ReverseValidator"},{"location":"meta_validators/reverse_validator/#pytorch_adapt.meta_validators.reverse_validator.ReverseValidator.run","text":"Parameters: Name Type Description Default forward_adapter the framework-wrapped adapter for step 1. required reverse_adapter the framework-wrapped adapter for step 3. required forward_kwargs a dict of keyword arguments to be passed to forward_adapter.run() required reverse_kwargs a dict of keyword arguments to be passed to reverse_adapter.run() required pl_dataloader_creator An optional DataloaderCreator for obtaining pseudolabels in step 2. None Returns: Type Description Tuple[float, int] the best score and best epoch of the reverse model Source code in pytorch_adapt\\meta_validators\\reverse_validator.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def run ( self , forward_adapter , reverse_adapter , forward_kwargs , reverse_kwargs , pl_dataloader_creator = None , ) -> Tuple [ float , int ]: \"\"\" Arguments: forward_adapter: the framework-wrapped adapter for step 1. reverse_adapter: the framework-wrapped adapter for step 3. forward_kwargs: a dict of keyword arguments to be passed to forward_adapter.run() reverse_kwargs: a dict of keyword arguments to be passed to reverse_adapter.run() pl_dataloader_creator: An optional DataloaderCreator for obtaining pseudolabels in step 2. Returns: the best score and best epoch of the reverse model \"\"\" if \"datasets\" in reverse_kwargs : raise KeyError ( \"'datasets' should not be in reverse_kwargs because the reverse datasets will be pseudo labeled.\" ) if \"validator\" not in reverse_kwargs : raise KeyError ( \"reverse_kwargs must include 'validator'\" ) forward_adapter . run ( ** forward_kwargs ) if all ( x in forward_kwargs for x in [ \"validator\" , \"saver\" ]): forward_kwargs [ \"saver\" ] . load_adapter ( forward_adapter . adapter , \"best\" ) datasets = forward_kwargs [ \"datasets\" ] pl_dataloader_creator = c_f . default ( pl_dataloader_creator , DataloaderCreator , { \"all_val\" : True } ) d = {} d [ \"src_train\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_train\" , pl_dataloader_creator ) d [ \"src_val\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_val\" , pl_dataloader_creator ) d [ \"target_train\" ] = TargetDataset ( datasets [ \"src_train\" ] . dataset ) d [ \"target_val\" ] = TargetDataset ( datasets [ \"src_val\" ] . dataset ) d [ \"train\" ] = CombinedSourceAndTargetDataset ( d [ \"src_train\" ], d [ \"target_train\" ]) self . pseudo_train = d [ \"src_train\" ] self . pseudo_val = d [ \"src_val\" ] reverse_kwargs [ \"datasets\" ] = d return reverse_adapter . run ( ** reverse_kwargs )","title":"run()"},{"location":"weighters/","text":"Weighters \u00b6 Weighters multiply losses by scalar values, and then reduce the losses to a single value on which you call .backward() . For example: import torch from pytorch_adapt.weighters import MeanWeighter weighter = MeanWeighter ( weights = { \"y\" : 2.3 }) logits = torch . randn ( 32 , 512 ) labels = torch . randint ( 0 , 10 , size = ( 32 ,)) x = torch . nn . functional . cross_entropy ( logits , labels ) y = torch . norm ( logits ) # y will by multiplied by 2.3 # x wasn't given a weight, # so it gets multiplied by the default value of 1. loss , components = weighter ({ \"x\" : x , \"y\" : y }) loss . backward ()","title":"Weighters"},{"location":"weighters/#weighters","text":"Weighters multiply losses by scalar values, and then reduce the losses to a single value on which you call .backward() . For example: import torch from pytorch_adapt.weighters import MeanWeighter weighter = MeanWeighter ( weights = { \"y\" : 2.3 }) logits = torch . randn ( 32 , 512 ) labels = torch . randint ( 0 , 10 , size = ( 32 ,)) x = torch . nn . functional . cross_entropy ( logits , labels ) y = torch . norm ( logits ) # y will by multiplied by 2.3 # x wasn't given a weight, # so it gets multiplied by the default value of 1. loss , components = weighter ({ \"x\" : x , \"y\" : y }) loss . backward ()","title":"Weighters"},{"location":"weighters/base_weighter/","text":"pytorch_adapt.weighters.base_weighter \u00b6 BaseWeighter \u00b6 Multiplies losses by scalar values, and then reduces them to a single value. __call__ ( self , loss_dict ) special \u00b6 Parameters: Name Type Description Default loss_dict Dict[str, torch.Tensor] A mapping from loss names to loss values. required Returns: Type Description Tuple[torch.Tensor, Dict[str, float]] A tuple where tuple[0] is the loss that .backward() can be called on, and tuple[1] is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. Source code in pytorch_adapt\\weighters\\base_weighter.py 52 53 54 55 56 57 58 59 60 61 62 63 def __call__ ( self , loss_dict : Dict [ str , torch . Tensor ] ) -> Tuple [ torch . Tensor , Dict [ str , float ]]: \"\"\" Arguments: loss_dict: A mapping from loss names to loss values. Returns: A tuple where ```tuple[0]``` is the loss that ```.backward()``` can be called on, and ```tuple[1]``` is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. \"\"\" return weight_losses ( self . reduction , self . weights , self . scale , loss_dict ) __init__ ( self , reduction , weights = None , scale = 1 ) special \u00b6 Parameters: Name Type Description Default reduction Callable[[List[torch.Tensor]], torch.Tensor] A function that takes in a list of losses and returns a single loss value. required weights Dict[str, float] A mapping from loss names to weight values. If None , weights are assumed to be 1. None scale float A scalar that every loss gets multiplied by. 1 Source code in pytorch_adapt\\weighters\\base_weighter.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , reduction : Callable [[ List [ torch . Tensor ]], torch . Tensor ], weights : Dict [ str , float ] = None , scale : float = 1 , ): \"\"\" Arguments: reduction: A function that takes in a list of losses and returns a single loss value. weights: A mapping from loss names to weight values. If ```None```, weights are assumed to be 1. scale: A scalar that every loss gets multiplied by. \"\"\" self . reduction = reduction self . weights = c_f . default ( weights , {}) self . scale = scale pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"weights\" , \"scale\" ])","title":"BaseWeighter"},{"location":"weighters/base_weighter/#pytorch_adapt.weighters.base_weighter","text":"","title":"base_weighter"},{"location":"weighters/base_weighter/#pytorch_adapt.weighters.base_weighter.BaseWeighter","text":"Multiplies losses by scalar values, and then reduces them to a single value.","title":"BaseWeighter"},{"location":"weighters/base_weighter/#pytorch_adapt.weighters.base_weighter.BaseWeighter.__call__","text":"Parameters: Name Type Description Default loss_dict Dict[str, torch.Tensor] A mapping from loss names to loss values. required Returns: Type Description Tuple[torch.Tensor, Dict[str, float]] A tuple where tuple[0] is the loss that .backward() can be called on, and tuple[1] is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. Source code in pytorch_adapt\\weighters\\base_weighter.py 52 53 54 55 56 57 58 59 60 61 62 63 def __call__ ( self , loss_dict : Dict [ str , torch . Tensor ] ) -> Tuple [ torch . Tensor , Dict [ str , float ]]: \"\"\" Arguments: loss_dict: A mapping from loss names to loss values. Returns: A tuple where ```tuple[0]``` is the loss that ```.backward()``` can be called on, and ```tuple[1]``` is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. \"\"\" return weight_losses ( self . reduction , self . weights , self . scale , loss_dict )","title":"__call__()"},{"location":"weighters/base_weighter/#pytorch_adapt.weighters.base_weighter.BaseWeighter.__init__","text":"Parameters: Name Type Description Default reduction Callable[[List[torch.Tensor]], torch.Tensor] A function that takes in a list of losses and returns a single loss value. required weights Dict[str, float] A mapping from loss names to weight values. If None , weights are assumed to be 1. None scale float A scalar that every loss gets multiplied by. 1 Source code in pytorch_adapt\\weighters\\base_weighter.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , reduction : Callable [[ List [ torch . Tensor ]], torch . Tensor ], weights : Dict [ str , float ] = None , scale : float = 1 , ): \"\"\" Arguments: reduction: A function that takes in a list of losses and returns a single loss value. weights: A mapping from loss names to weight values. If ```None```, weights are assumed to be 1. scale: A scalar that every loss gets multiplied by. \"\"\" self . reduction = reduction self . weights = c_f . default ( weights , {}) self . scale = scale pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"weights\" , \"scale\" ])","title":"__init__()"},{"location":"weighters/mean_weighter/","text":"pytorch_adapt.weighters.mean_weighter \u00b6 MeanWeighter \u00b6 Weights the losses and then returns the mean of the weighted losses.","title":"MeanWeighter"},{"location":"weighters/mean_weighter/#pytorch_adapt.weighters.mean_weighter","text":"","title":"mean_weighter"},{"location":"weighters/mean_weighter/#pytorch_adapt.weighters.mean_weighter.MeanWeighter","text":"Weights the losses and then returns the mean of the weighted losses.","title":"MeanWeighter"},{"location":"weighters/sum_weighter/","text":"pytorch_adapt.weighters.sum_weighter \u00b6 SumWeighter \u00b6 Weights the losses and then returns the sum of the weighted losses.","title":"SumWeighter"},{"location":"weighters/sum_weighter/#pytorch_adapt.weighters.sum_weighter","text":"","title":"sum_weighter"},{"location":"weighters/sum_weighter/#pytorch_adapt.weighters.sum_weighter.SumWeighter","text":"Weights the losses and then returns the sum of the weighted losses.","title":"SumWeighter"}]}