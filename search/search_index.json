{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTorch Adapt \u00b6 Google Colab Examples \u00b6 Overview \u00b6 Installation \u00b6 Pip \u00b6 pip install pytorch-adapt To get the latest dev version : pip install pytorch-adapt --pre","title":"Overview"},{"location":"#pytorch-adapt","text":"","title":"PyTorch Adapt"},{"location":"#google-colab-examples","text":"","title":"Google Colab Examples"},{"location":"#overview","text":"","title":"Overview"},{"location":"#installation","text":"","title":"Installation"},{"location":"#pip","text":"pip install pytorch-adapt To get the latest dev version : pip install pytorch-adapt --pre","title":"Pip"},{"location":"SUMMARY/","text":"Overview Algorithms Overview Unsupervised Domain Adaptation Validators Adapters ADDA Aligner BaseAdapter Classifier DANN GAN MCD SymNets Containers BaseContainer Misc LRSchedulers Models Optimizers Datasets CombinedSourceAndTarget ConcatDataset DataloaderCreator DomainNet MNISTM Office31 PseudoLabeledDataset SourceDataset TargetDataset Frameworks Ignite Ignite Loggers Hooks ADDAHook ATDOCHook Aligners AlignerHook AlignerPlusCHook FeaturesLogitsAlignerHook JointAlignerHook Base BaseConditionHook BaseHook BaseWrapperHook CDANHook Classification ClassifierHook CLossHook FinetunerHook SoftmaxHook SoftmaxLocallyHook Conditions StrongDHook DANNHook Domain DomainLossHook FeaturesForDomainLossHook DomainConfusionHook GANHook GVBHook Features BaseFeaturesHook CombinedFeaturesHook DLogitsHook FeaturesAndLogitsHook FeaturesChainHook FeaturesHook FeaturesWithGradAndDetachedHook FrozenModelHook LogitsHook MCDHook OptimizerHook Reducers BaseReducer EntropyReducer MeanReducer RTNHook SymNetsHook Utils ApplyFnHook AssertHook ChainHook EmptyHook FalseHook MultiplierHook NotHook OnlyNewOutputsHook ParallelHook RepeatHook TrueHook ZeroLossHook VADAHook Validate Layers AbsLoss AdaptiveFeatureNorm BatchSpectralLoss BNMLoss ConcatSoftmax ConfidenceWeights CORALLoss DiversityLoss DoNothingOptimizer EntropyLoss EntropyWeights GradientReversal MCCLoss MCDLoss MMDLoss ModelWithBridge MultipleModels NeighborhoodAggregation PlusResidual RandomizedDotProduct SilhouetteScore SlicedWasserstein StochasticLinear SufficientAccuracy UniformDistributionLoss VATLoss Meta Validators ForwardOnlyValidator ReverseValidator Models Classifier Discriminator MNISTFeatures Utils Validators AccuracyValidator BaseValidator DeepEmbeddedValidator DiversityValidator EntropyValidator ErrorValidator SNDValidator Weighters BaseWeighter MeanWeighter SumWeighter","title":"SUMMARY"},{"location":"adapters/","text":"Adapters \u00b6 Adapters contain an algorithm's training step and inference step. The training step is defined in the wrapped hook . Examples \u00b6 Initialization import torch from pytorch_adapt.adapters import DANN from pytorch_adapt.containers import Models G = torch . nn . Linear ( 1000 , 100 ) C = torch . nn . Linear ( 100 , 10 ) D = torch . nn . Linear ( 100 , 1 ) models = Models ({ \"G\" : G , \"C\" : C , \"D\" : D }) adapter = DANN ( models = models ) Training step device = torch . device ( \"cuda\" ) adapter . models . to ( device ) data = { \"src_imgs\" : torch . randn ( 32 , 1000 ), \"target_imgs\" : torch . randn ( 32 , 1000 ), \"src_labels\" : torch . randint ( 0 , 10 , size = ( 32 ,)), \"src_domain\" : torch . zeros ( 32 ), \"target_domain\" : torch . zeros ( 32 ), } loss = adapter . training_step ( data , device ) Inference data = torch . randn ( 32 , 1000 ) . to ( device ) features , logits = adapter . inference ( data )","title":"Adapters"},{"location":"adapters/#adapters","text":"Adapters contain an algorithm's training step and inference step. The training step is defined in the wrapped hook .","title":"Adapters"},{"location":"adapters/#examples","text":"Initialization import torch from pytorch_adapt.adapters import DANN from pytorch_adapt.containers import Models G = torch . nn . Linear ( 1000 , 100 ) C = torch . nn . Linear ( 100 , 10 ) D = torch . nn . Linear ( 100 , 1 ) models = Models ({ \"G\" : G , \"C\" : C , \"D\" : D }) adapter = DANN ( models = models ) Training step device = torch . device ( \"cuda\" ) adapter . models . to ( device ) data = { \"src_imgs\" : torch . randn ( 32 , 1000 ), \"target_imgs\" : torch . randn ( 32 , 1000 ), \"src_labels\" : torch . randint ( 0 , 10 , size = ( 32 ,)), \"src_domain\" : torch . zeros ( 32 ), \"target_domain\" : torch . zeros ( 32 ), } loss = adapter . training_step ( data , device ) Inference data = torch . randn ( 32 , 1000 ) . to ( device ) features , logits = adapter . inference ( data )","title":"Examples"},{"location":"adapters/adda/","text":"pytorch_adapt.adapters.adda \u00b6 ADDA \u00b6 hook_cls \u00b6 Implementation of Adversarial Discriminative Domain Adaptation . Extends GANHook . __init__ ( self , threshold = 0.6 , pre_g = None , post_g = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default threshold float In each training iteration, the generator is only updated if the discriminator's accuracy is greater than threshold . 0.6 Source code in pytorch_adapt\\adapters\\adda.py def __init__ ( self , threshold : float = 0.6 , pre_g = None , post_g = None , ** kwargs ): \"\"\" Arguments: threshold: In each training iteration, the generator is only updated if the discriminator's accuracy is greater than ```threshold```. \"\"\" [ pre_g , post_g ] = c_f . many_default ([ pre_g , post_g ], [[], []]) sf_frozen = FrozenModelHook ( FeaturesHook ( detach = True , domains = [ \"src\" ]), \"G\" ) tf_all = FeaturesWithGradAndDetachedHook ( model_name = \"T\" , domains = [ \"target\" ]) pre_d = ChainHook ( sf_frozen , tf_all ) num_pre_g = len ( pre_g ) gen_conditions = [ TrueHook () for _ in range ( num_pre_g + len ( post_g ) + 2 )] # generator condition, classifier condition gen_conditions [ num_pre_g : num_pre_g + 2 ] = [ StrongDHook ( threshold ), FalseHook (), ] super () . __init__ ( pre_d = [ pre_d ], pre_g = pre_g , post_g = post_g , gen_conditions = gen_conditions , gen_domains = [ \"target\" ], c_hook = EmptyHook (), ** kwargs )","title":"ADDA"},{"location":"adapters/adda/#pytorch_adapt.adapters.adda","text":"","title":"adda"},{"location":"adapters/adda/#pytorch_adapt.adapters.adda.ADDA","text":"","title":"ADDA"},{"location":"adapters/adda/#pytorch_adapt.adapters.adda.ADDA.hook_cls","text":"Implementation of Adversarial Discriminative Domain Adaptation . Extends GANHook .","title":"hook_cls"},{"location":"adapters/adda/#pytorch_adapt.adapters.adda.ADDA.hook_cls.__init__","text":"Parameters: Name Type Description Default threshold float In each training iteration, the generator is only updated if the discriminator's accuracy is greater than threshold . 0.6 Source code in pytorch_adapt\\adapters\\adda.py def __init__ ( self , threshold : float = 0.6 , pre_g = None , post_g = None , ** kwargs ): \"\"\" Arguments: threshold: In each training iteration, the generator is only updated if the discriminator's accuracy is greater than ```threshold```. \"\"\" [ pre_g , post_g ] = c_f . many_default ([ pre_g , post_g ], [[], []]) sf_frozen = FrozenModelHook ( FeaturesHook ( detach = True , domains = [ \"src\" ]), \"G\" ) tf_all = FeaturesWithGradAndDetachedHook ( model_name = \"T\" , domains = [ \"target\" ]) pre_d = ChainHook ( sf_frozen , tf_all ) num_pre_g = len ( pre_g ) gen_conditions = [ TrueHook () for _ in range ( num_pre_g + len ( post_g ) + 2 )] # generator condition, classifier condition gen_conditions [ num_pre_g : num_pre_g + 2 ] = [ StrongDHook ( threshold ), FalseHook (), ] super () . __init__ ( pre_d = [ pre_d ], pre_g = pre_g , post_g = post_g , gen_conditions = gen_conditions , gen_domains = [ \"target\" ], c_hook = EmptyHook (), ** kwargs )","title":"__init__()"},{"location":"adapters/aligner/","text":"pytorch_adapt.adapters.aligner \u00b6 Aligner \u00b6 hook_cls \u00b6 Computes an alignment loss plus a classification loss, and then optimizes the models. RTN \u00b6 hook_cls \u00b6 Implementation of Unsupervised Domain Adaptation with Residual Transfer Networks .","title":"Aligner"},{"location":"adapters/aligner/#pytorch_adapt.adapters.aligner","text":"","title":"aligner"},{"location":"adapters/aligner/#pytorch_adapt.adapters.aligner.Aligner","text":"","title":"Aligner"},{"location":"adapters/aligner/#pytorch_adapt.adapters.aligner.Aligner.hook_cls","text":"Computes an alignment loss plus a classification loss, and then optimizes the models.","title":"hook_cls"},{"location":"adapters/aligner/#pytorch_adapt.adapters.aligner.RTN","text":"","title":"RTN"},{"location":"adapters/aligner/#pytorch_adapt.adapters.aligner.RTN.hook_cls","text":"Implementation of Unsupervised Domain Adaptation with Residual Transfer Networks .","title":"hook_cls"},{"location":"adapters/base_adapter/","text":"pytorch_adapt.adapters.base_adapter \u00b6","title":"BaseAdapter"},{"location":"adapters/base_adapter/#pytorch_adapt.adapters.base_adapter","text":"","title":"base_adapter"},{"location":"adapters/classifier/","text":"pytorch_adapt.adapters.classifier \u00b6 Classifier \u00b6 hook_cls \u00b6 This computes the classification loss and also optimizes the models. Finetuner \u00b6 hook_cls \u00b6 This is the same as ClassifierHook , but it freezes the generator model (\"G\").","title":"Classifier"},{"location":"adapters/classifier/#pytorch_adapt.adapters.classifier","text":"","title":"classifier"},{"location":"adapters/classifier/#pytorch_adapt.adapters.classifier.Classifier","text":"","title":"Classifier"},{"location":"adapters/classifier/#pytorch_adapt.adapters.classifier.Classifier.hook_cls","text":"This computes the classification loss and also optimizes the models.","title":"hook_cls"},{"location":"adapters/classifier/#pytorch_adapt.adapters.classifier.Finetuner","text":"","title":"Finetuner"},{"location":"adapters/classifier/#pytorch_adapt.adapters.classifier.Finetuner.hook_cls","text":"This is the same as ClassifierHook , but it freezes the generator model (\"G\").","title":"hook_cls"},{"location":"adapters/dann/","text":"pytorch_adapt.adapters.dann \u00b6 DANN \u00b6 hook_cls \u00b6 Implementation of Domain-Adversarial Training of Neural Networks . This includes the model optimization step. __init__ ( self , opts , weighter = None , reducer = None , pre = None , pre_d = None , post_d = None , pre_g = None , post_g = None , gradient_reversal = None , use_logits = False , f_hook = None , d_hook = None , c_hook = None , domain_loss_hook = None , d_hook_allowed = '_dlogits$' , ** kwargs ) special \u00b6 Parameters: Name Type Description Default opts List of optimizers for updating the models. required weighter Weights the losses before backpropagation. If None then it defaults to MeanWeighter None reducer Reduces loss tensors. If None then it defaults to MeanReducer None pre List of hooks that will be executed at the very beginning of each iteration. None pre_d List of hooks that will be executed after gradient reversal, but before the domain loss. None post_d List of hooks that will be executed after gradient reversal, and after the domain loss. None pre_g List of hooks that will be executed outside of the gradient reversal step, and before the generator and classifier loss. None post_g List of hooks that will be executed after the generator and classifier losses. None gradient_reversal Called before all D hooks, including pre_d . None use_logits If True , then D receives the output of C instead of the output of G. False f_hook The hook used for computing features and logits. If None then it defaults to FeaturesForDomainLossHook None d_hook The hook used for computing discriminator logits. If None then it defaults to DLogitsHook None c_hook The hook used for computing the classifiers's loss. If None then it defaults to CLossHook None domain_loss_hook The hook used for computing the domain loss. If None then it defaults to DomainLossHook . None d_hook_allowed A regex string that specifies the allowed output names of the discriminator block. '_dlogits$' Source code in pytorch_adapt\\adapters\\dann.py def __init__ ( self , opts , weighter = None , reducer = None , pre = None , pre_d = None , post_d = None , pre_g = None , post_g = None , gradient_reversal = None , use_logits = False , f_hook = None , d_hook = None , c_hook = None , domain_loss_hook = None , d_hook_allowed = \"_dlogits$\" , ** kwargs ): \"\"\" Arguments: opts: List of optimizers for updating the models. weighter: Weights the losses before backpropagation. If ```None``` then it defaults to [```MeanWeighter```][pytorch_adapt.weighters.mean_weighter.MeanWeighter] reducer: Reduces loss tensors. If ```None``` then it defaults to [```MeanReducer```][pytorch_adapt.hooks.reducers.MeanReducer] pre: List of hooks that will be executed at the very beginning of each iteration. pre_d: List of hooks that will be executed after gradient reversal, but before the domain loss. post_d: List of hooks that will be executed after gradient reversal, and after the domain loss. pre_g: List of hooks that will be executed outside of the gradient reversal step, and before the generator and classifier loss. post_g: List of hooks that will be executed after the generator and classifier losses. gradient_reversal: Called before all D hooks, including ```pre_d```. use_logits: If ```True```, then D receives the output of C instead of the output of G. f_hook: The hook used for computing features and logits. If ```None``` then it defaults to [```FeaturesForDomainLossHook```][pytorch_adapt.hooks.domain.FeaturesForDomainLossHook] d_hook: The hook used for computing discriminator logits. If ```None``` then it defaults to [```DLogitsHook```][pytorch_adapt.hooks.features.DLogitsHook] c_hook: The hook used for computing the classifiers's loss. If ```None``` then it defaults to [```CLossHook```][pytorch_adapt.hooks.classification.CLossHook] domain_loss_hook: The hook used for computing the domain loss. If ```None``` then it defaults to [```DomainLossHook```][pytorch_adapt.hooks.domain.DomainLossHook]. d_hook_allowed: A regex string that specifies the allowed output names of the discriminator block. \"\"\" super () . __init__ ( ** kwargs ) [ pre , pre_d , post_d , pre_g , post_g ] = c_f . many_default ( [ pre , pre_d , post_d , pre_g , post_g ], [[], [], [], [], []] ) f_hook = c_f . default ( f_hook , FeaturesForDomainLossHook , { \"use_logits\" : use_logits } ) gradient_reversal = c_f . default ( gradient_reversal , GradientReversalHook , { \"apply_to\" : f_hook . out_keys } ) c_hook = c_f . default ( c_hook , CLossHook , {}) domain_loss_hook = c_f . default ( domain_loss_hook , DomainLossHook , { \"f_hook\" : f_hook , \"d_hook\" : d_hook } ) disc_hook = AssertHook ( OnlyNewOutputsHook ( ChainHook ( gradient_reversal , * pre_d , domain_loss_hook , * post_d , overwrite = [ 1 ], ) ), d_hook_allowed , ) gen_hook = ChainHook ( * pre_g , c_hook , * post_g ) hook = ChainHook ( * pre , f_hook , disc_hook , gen_hook ) hook = OptimizerHook ( hook , opts , weighter , reducer ) s_hook = SummaryHook ({ \"total_loss\" : hook }) self . hook = ChainHook ( hook , s_hook ) GVB \u00b6 hook_cls \u00b6 Implementation of Gradually Vanishing Bridge for Adversarial Domain Adaptation","title":"DANN"},{"location":"adapters/dann/#pytorch_adapt.adapters.dann","text":"","title":"dann"},{"location":"adapters/dann/#pytorch_adapt.adapters.dann.DANN","text":"","title":"DANN"},{"location":"adapters/dann/#pytorch_adapt.adapters.dann.DANN.hook_cls","text":"Implementation of Domain-Adversarial Training of Neural Networks . This includes the model optimization step.","title":"hook_cls"},{"location":"adapters/dann/#pytorch_adapt.adapters.dann.DANN.hook_cls.__init__","text":"Parameters: Name Type Description Default opts List of optimizers for updating the models. required weighter Weights the losses before backpropagation. If None then it defaults to MeanWeighter None reducer Reduces loss tensors. If None then it defaults to MeanReducer None pre List of hooks that will be executed at the very beginning of each iteration. None pre_d List of hooks that will be executed after gradient reversal, but before the domain loss. None post_d List of hooks that will be executed after gradient reversal, and after the domain loss. None pre_g List of hooks that will be executed outside of the gradient reversal step, and before the generator and classifier loss. None post_g List of hooks that will be executed after the generator and classifier losses. None gradient_reversal Called before all D hooks, including pre_d . None use_logits If True , then D receives the output of C instead of the output of G. False f_hook The hook used for computing features and logits. If None then it defaults to FeaturesForDomainLossHook None d_hook The hook used for computing discriminator logits. If None then it defaults to DLogitsHook None c_hook The hook used for computing the classifiers's loss. If None then it defaults to CLossHook None domain_loss_hook The hook used for computing the domain loss. If None then it defaults to DomainLossHook . None d_hook_allowed A regex string that specifies the allowed output names of the discriminator block. '_dlogits$' Source code in pytorch_adapt\\adapters\\dann.py def __init__ ( self , opts , weighter = None , reducer = None , pre = None , pre_d = None , post_d = None , pre_g = None , post_g = None , gradient_reversal = None , use_logits = False , f_hook = None , d_hook = None , c_hook = None , domain_loss_hook = None , d_hook_allowed = \"_dlogits$\" , ** kwargs ): \"\"\" Arguments: opts: List of optimizers for updating the models. weighter: Weights the losses before backpropagation. If ```None``` then it defaults to [```MeanWeighter```][pytorch_adapt.weighters.mean_weighter.MeanWeighter] reducer: Reduces loss tensors. If ```None``` then it defaults to [```MeanReducer```][pytorch_adapt.hooks.reducers.MeanReducer] pre: List of hooks that will be executed at the very beginning of each iteration. pre_d: List of hooks that will be executed after gradient reversal, but before the domain loss. post_d: List of hooks that will be executed after gradient reversal, and after the domain loss. pre_g: List of hooks that will be executed outside of the gradient reversal step, and before the generator and classifier loss. post_g: List of hooks that will be executed after the generator and classifier losses. gradient_reversal: Called before all D hooks, including ```pre_d```. use_logits: If ```True```, then D receives the output of C instead of the output of G. f_hook: The hook used for computing features and logits. If ```None``` then it defaults to [```FeaturesForDomainLossHook```][pytorch_adapt.hooks.domain.FeaturesForDomainLossHook] d_hook: The hook used for computing discriminator logits. If ```None``` then it defaults to [```DLogitsHook```][pytorch_adapt.hooks.features.DLogitsHook] c_hook: The hook used for computing the classifiers's loss. If ```None``` then it defaults to [```CLossHook```][pytorch_adapt.hooks.classification.CLossHook] domain_loss_hook: The hook used for computing the domain loss. If ```None``` then it defaults to [```DomainLossHook```][pytorch_adapt.hooks.domain.DomainLossHook]. d_hook_allowed: A regex string that specifies the allowed output names of the discriminator block. \"\"\" super () . __init__ ( ** kwargs ) [ pre , pre_d , post_d , pre_g , post_g ] = c_f . many_default ( [ pre , pre_d , post_d , pre_g , post_g ], [[], [], [], [], []] ) f_hook = c_f . default ( f_hook , FeaturesForDomainLossHook , { \"use_logits\" : use_logits } ) gradient_reversal = c_f . default ( gradient_reversal , GradientReversalHook , { \"apply_to\" : f_hook . out_keys } ) c_hook = c_f . default ( c_hook , CLossHook , {}) domain_loss_hook = c_f . default ( domain_loss_hook , DomainLossHook , { \"f_hook\" : f_hook , \"d_hook\" : d_hook } ) disc_hook = AssertHook ( OnlyNewOutputsHook ( ChainHook ( gradient_reversal , * pre_d , domain_loss_hook , * post_d , overwrite = [ 1 ], ) ), d_hook_allowed , ) gen_hook = ChainHook ( * pre_g , c_hook , * post_g ) hook = ChainHook ( * pre , f_hook , disc_hook , gen_hook ) hook = OptimizerHook ( hook , opts , weighter , reducer ) s_hook = SummaryHook ({ \"total_loss\" : hook }) self . hook = ChainHook ( hook , s_hook )","title":"__init__()"},{"location":"adapters/dann/#pytorch_adapt.adapters.dann.GVB","text":"","title":"GVB"},{"location":"adapters/dann/#pytorch_adapt.adapters.dann.GVB.hook_cls","text":"Implementation of Gradually Vanishing Bridge for Adversarial Domain Adaptation","title":"hook_cls"},{"location":"adapters/gan/","text":"pytorch_adapt.adapters.gan \u00b6 CDAN \u00b6 hook_cls \u00b6 Implementation of Conditional Adversarial Domain Adaptation Extends GANHook . DomainConfusion \u00b6 hook_cls \u00b6 Implementation of Simultaneous Deep Transfer Across Domains and Tasks Extends GANHook . GAN \u00b6 hook_cls \u00b6 A generic GAN architecture for domain adaptation. This includes the model optimization steps. __init__ ( self , d_opts , g_opts , d_weighter = None , d_reducer = None , g_weighter = None , g_reducer = None , pre_d = None , post_d = None , pre_g = None , post_g = None , use_logits = False , disc_hook = None , gen_hook = None , disc_f_hook = None , gen_f_hook = None , disc_d_hook = None , gen_d_hook = None , c_hook = None , disc_conditions = None , disc_alts = None , gen_conditions = None , gen_alts = None , disc_domains = None , gen_domains = None , disc_domain_loss_fn = None , gen_domain_loss_fn = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default d_opts List of optimizers for the D phase. required g_opts List of optimizers for the G phase. required d_weighter A loss weighter for the D phase. If None then MeanWeighter is used. None d_reducer A loss reducer for the D phase. If None then MeanReducer is used. None g_weighter A loss weighter for the G phase. If None then MeanWeighter is used. None g_reducer A loss reducer for the G phase. If None then MeanReducer is used. None pre_d List of hooks that will be executed at the very beginning of the D phase. None post_d List of hooks that will be executed at the end of the D phase, but before the optimizers are called. None pre_g List of hooks that will be executed at the very beginning of the G phase. None post_g List of hooks that will be executed at the end of the G phase, but before the optimizers are called. None use_logits If True , then D receives the output of C instead of the output of G. False disc_hook The hook used for computing the discriminator's domain loss. If None then DomainLossHook is used. None gen_hook The hook used for computing the generator's domain loss. If None then DomainLossHook is used. None c_hook The hook used for computing the classifiers's loss. If None then CLossHook is used. None disc_conditions The condition hooks used in the ChainHook for the D phase. None disc_alts The alt hooks used in the ChainHook for the D phase. None gen_conditions The condition hooks used in the ChainHook for the G phase. None gen_alts The alt hooks used in the ChainHook for the G phase. None disc_domains The domains used to compute the discriminator's domain loss. If None , then [\"src\", \"target\"] is used. None gen_domains The domains used to compute the generators's domain loss. If None , then [\"src\", \"target\"] is used. None disc_domain_loss_fn The loss function used to compute the discriminator's domain loss. If None then torch.nn.BCEWithLogitsLoss is used. None gen_domain_loss_fn The loss function used to compute the generator's domain loss. If None then torch.nn.BCEWithLogitsLoss is used. None Source code in pytorch_adapt\\adapters\\gan.py def __init__ ( self , d_opts , g_opts , d_weighter = None , d_reducer = None , g_weighter = None , g_reducer = None , pre_d = None , post_d = None , pre_g = None , post_g = None , use_logits = False , disc_hook = None , gen_hook = None , disc_f_hook = None , gen_f_hook = None , disc_d_hook = None , gen_d_hook = None , c_hook = None , disc_conditions = None , disc_alts = None , gen_conditions = None , gen_alts = None , disc_domains = None , gen_domains = None , disc_domain_loss_fn = None , gen_domain_loss_fn = None , ** kwargs ): \"\"\" Arguments: d_opts: List of optimizers for the D phase. g_opts: List of optimizers for the G phase. d_weighter: A loss weighter for the D phase. If ```None``` then ```MeanWeighter``` is used. d_reducer: A loss reducer for the D phase. If ```None``` then ```MeanReducer``` is used. g_weighter: A loss weighter for the G phase. If ```None``` then ```MeanWeighter``` is used. g_reducer: A loss reducer for the G phase. If ```None``` then ```MeanReducer``` is used. pre_d: List of hooks that will be executed at the very beginning of the D phase. post_d: List of hooks that will be executed at the end of the D phase, but before the optimizers are called. pre_g: List of hooks that will be executed at the very beginning of the G phase. post_g: List of hooks that will be executed at the end of the G phase, but before the optimizers are called. use_logits: If ```True```, then D receives the output of C instead of the output of G. disc_hook: The hook used for computing the discriminator's domain loss. If ```None``` then ```DomainLossHook``` is used. gen_hook: The hook used for computing the generator's domain loss. If ```None``` then ```DomainLossHook``` is used. c_hook: The hook used for computing the classifiers's loss. If ```None``` then ```CLossHook``` is used. disc_conditions: The condition hooks used in the ```ChainHook``` for the D phase. disc_alts: The alt hooks used in the ```ChainHook``` for the D phase. gen_conditions: The condition hooks used in the ```ChainHook``` for the G phase. gen_alts: The alt hooks used in the ```ChainHook``` for the G phase. disc_domains: The domains used to compute the discriminator's domain loss. If ```None```, then ```[\"src\", \"target\"]``` is used. gen_domains: The domains used to compute the generators's domain loss. If ```None```, then ```[\"src\", \"target\"]``` is used. disc_domain_loss_fn: The loss function used to compute the discriminator's domain loss. If ```None``` then ```torch.nn.BCEWithLogitsLoss``` is used. gen_domain_loss_fn: The loss function used to compute the generator's domain loss. If ```None``` then ```torch.nn.BCEWithLogitsLoss``` is used. \"\"\" super () . __init__ ( ** kwargs ) [ pre_d , post_d , pre_g , post_g ] = c_f . many_default ( [ pre_d , post_d , pre_g , post_g ], [[], [], [], []] ) disc_f_hook = c_f . default ( disc_f_hook , FeaturesForDomainLossHook , { \"detach\" : True , \"use_logits\" : use_logits , \"domains\" : disc_domains }, ) gen_f_hook = c_f . default ( gen_f_hook , FeaturesForDomainLossHook , { \"use_logits\" : use_logits , \"domains\" : gen_domains }, ) c_hook = c_f . default ( c_hook , CLossHook , {}) disc_hook = c_f . default ( disc_hook , DomainLossHook , { \"d_loss_fn\" : disc_domain_loss_fn , \"loss_prefix\" : \"d_\" , \"detach_features\" : True , \"f_hook\" : disc_f_hook , \"d_hook\" : disc_d_hook , \"domains\" : disc_domains , }, ) gen_hook = c_f . default ( gen_hook , DomainLossHook , { \"d_loss_fn\" : gen_domain_loss_fn , \"loss_prefix\" : \"g_\" , \"reverse_labels\" : True , \"f_hook\" : gen_f_hook , \"d_hook\" : gen_d_hook , \"domains\" : gen_domains , }, ) # use gen_f_hook to get undetached features first disc_hook = ChainHook ( * pre_d , gen_f_hook , disc_hook , * post_d , conditions = disc_conditions , alts = disc_alts ) gen_hook = ChainHook ( * pre_g , gen_hook , c_hook , * post_g , conditions = gen_conditions , alts = gen_alts ) disc_hook = OptimizerHook ( disc_hook , d_opts , d_weighter , d_reducer ) gen_hook = OptimizerHook ( gen_hook , g_opts , g_weighter , g_reducer ) s_hook = SummaryHook ({ \"d_loss\" : disc_hook , \"g_loss\" : gen_hook }) self . hook = ChainHook ( disc_hook , gen_hook , s_hook ) VADA \u00b6 hook_cls \u00b6 Implementation of VADA from A DIRT-T Approach to Unsupervised Domain Adaptation .","title":"GAN"},{"location":"adapters/gan/#pytorch_adapt.adapters.gan","text":"","title":"gan"},{"location":"adapters/gan/#pytorch_adapt.adapters.gan.CDAN","text":"","title":"CDAN"},{"location":"adapters/gan/#pytorch_adapt.adapters.gan.CDAN.hook_cls","text":"Implementation of Conditional Adversarial Domain Adaptation Extends GANHook .","title":"hook_cls"},{"location":"adapters/gan/#pytorch_adapt.adapters.gan.DomainConfusion","text":"","title":"DomainConfusion"},{"location":"adapters/gan/#pytorch_adapt.adapters.gan.DomainConfusion.hook_cls","text":"Implementation of Simultaneous Deep Transfer Across Domains and Tasks Extends GANHook .","title":"hook_cls"},{"location":"adapters/gan/#pytorch_adapt.adapters.gan.GAN","text":"","title":"GAN"},{"location":"adapters/gan/#pytorch_adapt.adapters.gan.GAN.hook_cls","text":"A generic GAN architecture for domain adaptation. This includes the model optimization steps.","title":"hook_cls"},{"location":"adapters/gan/#pytorch_adapt.adapters.gan.GAN.hook_cls.__init__","text":"Parameters: Name Type Description Default d_opts List of optimizers for the D phase. required g_opts List of optimizers for the G phase. required d_weighter A loss weighter for the D phase. If None then MeanWeighter is used. None d_reducer A loss reducer for the D phase. If None then MeanReducer is used. None g_weighter A loss weighter for the G phase. If None then MeanWeighter is used. None g_reducer A loss reducer for the G phase. If None then MeanReducer is used. None pre_d List of hooks that will be executed at the very beginning of the D phase. None post_d List of hooks that will be executed at the end of the D phase, but before the optimizers are called. None pre_g List of hooks that will be executed at the very beginning of the G phase. None post_g List of hooks that will be executed at the end of the G phase, but before the optimizers are called. None use_logits If True , then D receives the output of C instead of the output of G. False disc_hook The hook used for computing the discriminator's domain loss. If None then DomainLossHook is used. None gen_hook The hook used for computing the generator's domain loss. If None then DomainLossHook is used. None c_hook The hook used for computing the classifiers's loss. If None then CLossHook is used. None disc_conditions The condition hooks used in the ChainHook for the D phase. None disc_alts The alt hooks used in the ChainHook for the D phase. None gen_conditions The condition hooks used in the ChainHook for the G phase. None gen_alts The alt hooks used in the ChainHook for the G phase. None disc_domains The domains used to compute the discriminator's domain loss. If None , then [\"src\", \"target\"] is used. None gen_domains The domains used to compute the generators's domain loss. If None , then [\"src\", \"target\"] is used. None disc_domain_loss_fn The loss function used to compute the discriminator's domain loss. If None then torch.nn.BCEWithLogitsLoss is used. None gen_domain_loss_fn The loss function used to compute the generator's domain loss. If None then torch.nn.BCEWithLogitsLoss is used. None Source code in pytorch_adapt\\adapters\\gan.py def __init__ ( self , d_opts , g_opts , d_weighter = None , d_reducer = None , g_weighter = None , g_reducer = None , pre_d = None , post_d = None , pre_g = None , post_g = None , use_logits = False , disc_hook = None , gen_hook = None , disc_f_hook = None , gen_f_hook = None , disc_d_hook = None , gen_d_hook = None , c_hook = None , disc_conditions = None , disc_alts = None , gen_conditions = None , gen_alts = None , disc_domains = None , gen_domains = None , disc_domain_loss_fn = None , gen_domain_loss_fn = None , ** kwargs ): \"\"\" Arguments: d_opts: List of optimizers for the D phase. g_opts: List of optimizers for the G phase. d_weighter: A loss weighter for the D phase. If ```None``` then ```MeanWeighter``` is used. d_reducer: A loss reducer for the D phase. If ```None``` then ```MeanReducer``` is used. g_weighter: A loss weighter for the G phase. If ```None``` then ```MeanWeighter``` is used. g_reducer: A loss reducer for the G phase. If ```None``` then ```MeanReducer``` is used. pre_d: List of hooks that will be executed at the very beginning of the D phase. post_d: List of hooks that will be executed at the end of the D phase, but before the optimizers are called. pre_g: List of hooks that will be executed at the very beginning of the G phase. post_g: List of hooks that will be executed at the end of the G phase, but before the optimizers are called. use_logits: If ```True```, then D receives the output of C instead of the output of G. disc_hook: The hook used for computing the discriminator's domain loss. If ```None``` then ```DomainLossHook``` is used. gen_hook: The hook used for computing the generator's domain loss. If ```None``` then ```DomainLossHook``` is used. c_hook: The hook used for computing the classifiers's loss. If ```None``` then ```CLossHook``` is used. disc_conditions: The condition hooks used in the ```ChainHook``` for the D phase. disc_alts: The alt hooks used in the ```ChainHook``` for the D phase. gen_conditions: The condition hooks used in the ```ChainHook``` for the G phase. gen_alts: The alt hooks used in the ```ChainHook``` for the G phase. disc_domains: The domains used to compute the discriminator's domain loss. If ```None```, then ```[\"src\", \"target\"]``` is used. gen_domains: The domains used to compute the generators's domain loss. If ```None```, then ```[\"src\", \"target\"]``` is used. disc_domain_loss_fn: The loss function used to compute the discriminator's domain loss. If ```None``` then ```torch.nn.BCEWithLogitsLoss``` is used. gen_domain_loss_fn: The loss function used to compute the generator's domain loss. If ```None``` then ```torch.nn.BCEWithLogitsLoss``` is used. \"\"\" super () . __init__ ( ** kwargs ) [ pre_d , post_d , pre_g , post_g ] = c_f . many_default ( [ pre_d , post_d , pre_g , post_g ], [[], [], [], []] ) disc_f_hook = c_f . default ( disc_f_hook , FeaturesForDomainLossHook , { \"detach\" : True , \"use_logits\" : use_logits , \"domains\" : disc_domains }, ) gen_f_hook = c_f . default ( gen_f_hook , FeaturesForDomainLossHook , { \"use_logits\" : use_logits , \"domains\" : gen_domains }, ) c_hook = c_f . default ( c_hook , CLossHook , {}) disc_hook = c_f . default ( disc_hook , DomainLossHook , { \"d_loss_fn\" : disc_domain_loss_fn , \"loss_prefix\" : \"d_\" , \"detach_features\" : True , \"f_hook\" : disc_f_hook , \"d_hook\" : disc_d_hook , \"domains\" : disc_domains , }, ) gen_hook = c_f . default ( gen_hook , DomainLossHook , { \"d_loss_fn\" : gen_domain_loss_fn , \"loss_prefix\" : \"g_\" , \"reverse_labels\" : True , \"f_hook\" : gen_f_hook , \"d_hook\" : gen_d_hook , \"domains\" : gen_domains , }, ) # use gen_f_hook to get undetached features first disc_hook = ChainHook ( * pre_d , gen_f_hook , disc_hook , * post_d , conditions = disc_conditions , alts = disc_alts ) gen_hook = ChainHook ( * pre_g , gen_hook , c_hook , * post_g , conditions = gen_conditions , alts = gen_alts ) disc_hook = OptimizerHook ( disc_hook , d_opts , d_weighter , d_reducer ) gen_hook = OptimizerHook ( gen_hook , g_opts , g_weighter , g_reducer ) s_hook = SummaryHook ({ \"d_loss\" : disc_hook , \"g_loss\" : gen_hook }) self . hook = ChainHook ( disc_hook , gen_hook , s_hook )","title":"__init__()"},{"location":"adapters/gan/#pytorch_adapt.adapters.gan.VADA","text":"","title":"VADA"},{"location":"adapters/gan/#pytorch_adapt.adapters.gan.VADA.hook_cls","text":"Implementation of VADA from A DIRT-T Approach to Unsupervised Domain Adaptation .","title":"hook_cls"},{"location":"adapters/mcd/","text":"pytorch_adapt.adapters.mcd \u00b6 MCD \u00b6 hook_cls \u00b6 Implementation of Maximum Classifier Discrepancy for Unsupervised Domain Adaptation .","title":"MCD"},{"location":"adapters/mcd/#pytorch_adapt.adapters.mcd","text":"","title":"mcd"},{"location":"adapters/mcd/#pytorch_adapt.adapters.mcd.MCD","text":"","title":"MCD"},{"location":"adapters/mcd/#pytorch_adapt.adapters.mcd.MCD.hook_cls","text":"Implementation of Maximum Classifier Discrepancy for Unsupervised Domain Adaptation .","title":"hook_cls"},{"location":"adapters/symnets/","text":"pytorch_adapt.adapters.symnets \u00b6 SymNets \u00b6 hook_cls \u00b6 Implementation of Domain-Symmetric Networks for Adversarial Domain Adaptation .","title":"SymNets"},{"location":"adapters/symnets/#pytorch_adapt.adapters.symnets","text":"","title":"symnets"},{"location":"adapters/symnets/#pytorch_adapt.adapters.symnets.SymNets","text":"","title":"SymNets"},{"location":"adapters/symnets/#pytorch_adapt.adapters.symnets.SymNets.hook_cls","text":"Implementation of Domain-Symmetric Networks for Adversarial Domain Adaptation .","title":"hook_cls"},{"location":"algorithms/uda/","text":"Unsupervised Domain Adaptation \u00b6 DANN \u00b6 Domain-Adversarial Training of Neural Networks adapters.DANN hooks.DANNHook layers.GradientReversal MMD \u00b6 Learning Transferable Features with Deep Adaptation Networks adapters.Aligner hooks.AlignerPlusCHook layers.MMDLoss Domain Confusion \u00b6 Simultaneous Deep Transfer Across Domains and Tasks adapters.DomainConfusion hooks.DomainConfusionHook layers.UniformDistributionLoss CORAL \u00b6 Deep CORAL: Correlation Alignment for Deep Domain Adaptation adapters.Aligner hooks.AlignerPlusCHook layers.CORALLoss RTN \u00b6 Unsupervised Domain Adaptation with Residual Transfer Networks adapters.RTN hooks.RTNHook layers.PlusResidual JMMD \u00b6 Deep Transfer Learning with Joint Adaptation Networks adapters.Aligner hooks.AlignerPlusCHook hooks.JointAlignerHook layers.MMDLoss ADDA \u00b6 Adversarial Discriminative Domain Adaptation adapters.ADDA hooks.ADDAHook hooks.StrongDHook AdaBN \u00b6 Revisiting Batch Normalization For Practical Domain Adaptation Docs coming soon GAN \u00b6 adapters.GAN hooks.GANHook VADA \u00b6 A DIRT-T Approach to Unsupervised Domain Adaptation adapters.VADA hooks.VADAHook hooks.VATHook layers.VATLoss layers.EntropyLoss MCD \u00b6 Maximum Classifier Discrepancy for Unsupervised Domain Adaptation adapters.MCD hooks.MCDHook layers.MCDLoss CDAN \u00b6 Conditional Adversarial Domain Adaptation adapters.CDAN hooks.CDANHook hooks.EntropyReducer layers.EntropyWeights layers.RandomizedDotProduct BSP \u00b6 Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation layers.BatchSpectralLoss AFN \u00b6 Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation layers.AdaptiveFeatureNorm layers.L2PreservedDropout SWD \u00b6 Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation adapters.MCD hooks.MCDHook layers.SlicedWasserstein SymNets \u00b6 Domain-Symmetric Networks for Adversarial Domain Adaptation adapters.SymNets hooks.SymNets layers.ConcatSoftmax GVB \u00b6 Gradually Vanishing Bridge for Adversarial Domain Adaptation adapters.GVB hooks.GVBHook layers.ModelWithBridge STAR \u00b6 Stochastic Classifiers for Unsupervised Domain Adaptation adapters.MCD hooks.MCDHook layers.StochasticLinear BNM \u00b6 Towards Discriminability and Diversity: Batch Nuclear-norm Maximization under Label Insufficient Situations layers.BNMLoss MCC \u00b6 Minimum Class Confusion for Versatile Domain Adaptation layers.MCCLoss ATDOC \u00b6 Domain Adaptation with Auxiliary Target Domain-Oriented Classifier hooks.ATDOCHook layers.NeighborhoodAggregation layers.ConfidenceWeights","title":"Unsupervised Domain Adaptation"},{"location":"algorithms/uda/#unsupervised-domain-adaptation","text":"","title":"Unsupervised Domain Adaptation"},{"location":"algorithms/uda/#dann","text":"Domain-Adversarial Training of Neural Networks adapters.DANN hooks.DANNHook layers.GradientReversal","title":"DANN"},{"location":"algorithms/uda/#mmd","text":"Learning Transferable Features with Deep Adaptation Networks adapters.Aligner hooks.AlignerPlusCHook layers.MMDLoss","title":"MMD"},{"location":"algorithms/uda/#domain-confusion","text":"Simultaneous Deep Transfer Across Domains and Tasks adapters.DomainConfusion hooks.DomainConfusionHook layers.UniformDistributionLoss","title":"Domain Confusion"},{"location":"algorithms/uda/#coral","text":"Deep CORAL: Correlation Alignment for Deep Domain Adaptation adapters.Aligner hooks.AlignerPlusCHook layers.CORALLoss","title":"CORAL"},{"location":"algorithms/uda/#rtn","text":"Unsupervised Domain Adaptation with Residual Transfer Networks adapters.RTN hooks.RTNHook layers.PlusResidual","title":"RTN"},{"location":"algorithms/uda/#jmmd","text":"Deep Transfer Learning with Joint Adaptation Networks adapters.Aligner hooks.AlignerPlusCHook hooks.JointAlignerHook layers.MMDLoss","title":"JMMD"},{"location":"algorithms/uda/#adda","text":"Adversarial Discriminative Domain Adaptation adapters.ADDA hooks.ADDAHook hooks.StrongDHook","title":"ADDA"},{"location":"algorithms/uda/#adabn","text":"Revisiting Batch Normalization For Practical Domain Adaptation Docs coming soon","title":"AdaBN"},{"location":"algorithms/uda/#gan","text":"adapters.GAN hooks.GANHook","title":"GAN"},{"location":"algorithms/uda/#vada","text":"A DIRT-T Approach to Unsupervised Domain Adaptation adapters.VADA hooks.VADAHook hooks.VATHook layers.VATLoss layers.EntropyLoss","title":"VADA"},{"location":"algorithms/uda/#mcd","text":"Maximum Classifier Discrepancy for Unsupervised Domain Adaptation adapters.MCD hooks.MCDHook layers.MCDLoss","title":"MCD"},{"location":"algorithms/uda/#cdan","text":"Conditional Adversarial Domain Adaptation adapters.CDAN hooks.CDANHook hooks.EntropyReducer layers.EntropyWeights layers.RandomizedDotProduct","title":"CDAN"},{"location":"algorithms/uda/#bsp","text":"Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation layers.BatchSpectralLoss","title":"BSP"},{"location":"algorithms/uda/#afn","text":"Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation layers.AdaptiveFeatureNorm layers.L2PreservedDropout","title":"AFN"},{"location":"algorithms/uda/#swd","text":"Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation adapters.MCD hooks.MCDHook layers.SlicedWasserstein","title":"SWD"},{"location":"algorithms/uda/#symnets","text":"Domain-Symmetric Networks for Adversarial Domain Adaptation adapters.SymNets hooks.SymNets layers.ConcatSoftmax","title":"SymNets"},{"location":"algorithms/uda/#gvb","text":"Gradually Vanishing Bridge for Adversarial Domain Adaptation adapters.GVB hooks.GVBHook layers.ModelWithBridge","title":"GVB"},{"location":"algorithms/uda/#star","text":"Stochastic Classifiers for Unsupervised Domain Adaptation adapters.MCD hooks.MCDHook layers.StochasticLinear","title":"STAR"},{"location":"algorithms/uda/#bnm","text":"Towards Discriminability and Diversity: Batch Nuclear-norm Maximization under Label Insufficient Situations layers.BNMLoss","title":"BNM"},{"location":"algorithms/uda/#mcc","text":"Minimum Class Confusion for Versatile Domain Adaptation layers.MCCLoss","title":"MCC"},{"location":"algorithms/uda/#atdoc","text":"Domain Adaptation with Auxiliary Target Domain-Oriented Classifier hooks.ATDOCHook layers.NeighborhoodAggregation layers.ConfidenceWeights","title":"ATDOC"},{"location":"algorithms/validators/","text":"Validators \u00b6 Deep Embedded Validation \u00b6 Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation validators.DeepEmbeddedValidator Soft Neighborhood Density \u00b6 Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density validators.SNDValidator","title":"Validators"},{"location":"algorithms/validators/#validators","text":"","title":"Validators"},{"location":"algorithms/validators/#deep-embedded-validation","text":"Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation validators.DeepEmbeddedValidator","title":"Deep Embedded Validation"},{"location":"algorithms/validators/#soft-neighborhood-density","text":"Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density validators.SNDValidator","title":"Soft Neighborhood Density"},{"location":"containers/","text":"Containers \u00b6 Containers simplify object creation. Examples \u00b6 Create with import torch from pytorch_adapt.containers import LRSchedulers , Models , Optimizers G = torch . nn . Linear ( 1000 , 100 ) C = torch . nn . Linear ( 100 , 10 ) D = torch . nn . Linear ( 100 , 1 ) models = Models ({ \"G\" : G , \"C\" : C , \"D\" : D }) optimizers = Optimizers (( torch . optim . Adam , { \"lr\" : 0.456 })) schedulers = LRSchedulers (( torch . optim . lr_scheduler . ExponentialLR , { \"gamma\" : 0.99 })) optimizers . create_with ( models ) schedulers . create_with ( optimizers ) # optimizers contains an optimizer for G, C, and D # schedulers contains an LR scheduler for each optimizer print ( models ) print ( optimizers ) print ( schedulers ) Merge more_models = Models ({ \"X\" : torch . nn . Linear ( 20 , 1 )}) models . merge ( more_models ) optimizers = Optimizers (( torch . optim . Adam , { \"lr\" : 0.456 })) special_opt = Optimizers (( torch . optim . SGD , { \"lr\" : 1 }), keys = [ \"G\" , \"X\" ]) optimizers . merge ( special_opt ) optimizers . create_with ( models ) # models contains G, C, D, and X # optimizers: # - the Adam optimizer with lr 0.456 for models C and D # - the SGD optimizer with lr 1 for models G and X print ( models ) print ( optimizers )","title":"Containers"},{"location":"containers/#containers","text":"Containers simplify object creation.","title":"Containers"},{"location":"containers/#examples","text":"Create with import torch from pytorch_adapt.containers import LRSchedulers , Models , Optimizers G = torch . nn . Linear ( 1000 , 100 ) C = torch . nn . Linear ( 100 , 10 ) D = torch . nn . Linear ( 100 , 1 ) models = Models ({ \"G\" : G , \"C\" : C , \"D\" : D }) optimizers = Optimizers (( torch . optim . Adam , { \"lr\" : 0.456 })) schedulers = LRSchedulers (( torch . optim . lr_scheduler . ExponentialLR , { \"gamma\" : 0.99 })) optimizers . create_with ( models ) schedulers . create_with ( optimizers ) # optimizers contains an optimizer for G, C, and D # schedulers contains an LR scheduler for each optimizer print ( models ) print ( optimizers ) print ( schedulers ) Merge more_models = Models ({ \"X\" : torch . nn . Linear ( 20 , 1 )}) models . merge ( more_models ) optimizers = Optimizers (( torch . optim . Adam , { \"lr\" : 0.456 })) special_opt = Optimizers (( torch . optim . SGD , { \"lr\" : 1 }), keys = [ \"G\" , \"X\" ]) optimizers . merge ( special_opt ) optimizers . create_with ( models ) # models contains G, C, D, and X # optimizers: # - the Adam optimizer with lr 0.456 for models C and D # - the SGD optimizer with lr 1 for models G and X print ( models ) print ( optimizers )","title":"Examples"},{"location":"containers/base_container/","text":"pytorch_adapt.containers.base_container \u00b6","title":"BaseContainer"},{"location":"containers/base_container/#pytorch_adapt.containers.base_container","text":"","title":"base_container"},{"location":"containers/lr_schedulers/","text":"pytorch_adapt.containers.lr_schedulers \u00b6","title":"LRSchedulers"},{"location":"containers/lr_schedulers/#pytorch_adapt.containers.lr_schedulers","text":"","title":"lr_schedulers"},{"location":"containers/misc/","text":"pytorch_adapt.containers.misc \u00b6","title":"Misc"},{"location":"containers/misc/#pytorch_adapt.containers.misc","text":"","title":"misc"},{"location":"containers/models/","text":"pytorch_adapt.containers.models \u00b6","title":"Models"},{"location":"containers/models/#pytorch_adapt.containers.models","text":"","title":"models"},{"location":"containers/optimizers/","text":"pytorch_adapt.containers.optimizers \u00b6","title":"Optimizers"},{"location":"containers/optimizers/#pytorch_adapt.containers.optimizers","text":"","title":"optimizers"},{"location":"datasets/","text":"Datasets \u00b6","title":"Datasets"},{"location":"datasets/#datasets","text":"","title":"Datasets"},{"location":"datasets/combined_source_and_target/","text":"pytorch_adapt.datasets.combined_source_and_target \u00b6 CombinedSourceAndTargetDataset \u00b6 Wraps a source dataset and a target dataset. __getitem__ ( self , idx ) special \u00b6 Parameters: Name Type Description Default idx The index of the target dataset. The source index is picked randomly. required Returns: Type Description Dict[str, Any] A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". Source code in pytorch_adapt\\datasets\\combined_source_and_target.py def __getitem__ ( self , idx ) -> Dict [ str , Any ]: \"\"\" Arguments: idx: The index of the target dataset. The source index is picked randomly. Returns: A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". \"\"\" target_data = self . target_dataset [ idx ] src_data = self . source_dataset [ self . get_random_src_idx ()] c_f . assert_dicts_are_disjoint ( src_data , target_data ) return { ** src_data , ** target_data } __init__ ( self , source_dataset , target_dataset ) special \u00b6 Parameters: Name Type Description Default source_dataset SourceDataset required target_dataset TargetDataset required Source code in pytorch_adapt\\datasets\\combined_source_and_target.py def __init__ ( self , source_dataset : SourceDataset , target_dataset : TargetDataset ): \"\"\" Arguments: source_dataset: target_dataset: \"\"\" self . source_dataset = source_dataset self . target_dataset = target_dataset __len__ ( self ) special \u00b6 Returns: Type Description int The length of the target dataset. Source code in pytorch_adapt\\datasets\\combined_source_and_target.py def __len__ ( self ) -> int : \"\"\" Returns: The length of the target dataset. \"\"\" return len ( self . target_dataset )","title":"CombinedSourceAndTarget"},{"location":"datasets/combined_source_and_target/#pytorch_adapt.datasets.combined_source_and_target","text":"","title":"combined_source_and_target"},{"location":"datasets/combined_source_and_target/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset","text":"Wraps a source dataset and a target dataset.","title":"CombinedSourceAndTargetDataset"},{"location":"datasets/combined_source_and_target/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset.__getitem__","text":"Parameters: Name Type Description Default idx The index of the target dataset. The source index is picked randomly. required Returns: Type Description Dict[str, Any] A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". Source code in pytorch_adapt\\datasets\\combined_source_and_target.py def __getitem__ ( self , idx ) -> Dict [ str , Any ]: \"\"\" Arguments: idx: The index of the target dataset. The source index is picked randomly. Returns: A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". \"\"\" target_data = self . target_dataset [ idx ] src_data = self . source_dataset [ self . get_random_src_idx ()] c_f . assert_dicts_are_disjoint ( src_data , target_data ) return { ** src_data , ** target_data }","title":"__getitem__()"},{"location":"datasets/combined_source_and_target/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset.__init__","text":"Parameters: Name Type Description Default source_dataset SourceDataset required target_dataset TargetDataset required Source code in pytorch_adapt\\datasets\\combined_source_and_target.py def __init__ ( self , source_dataset : SourceDataset , target_dataset : TargetDataset ): \"\"\" Arguments: source_dataset: target_dataset: \"\"\" self . source_dataset = source_dataset self . target_dataset = target_dataset","title":"__init__()"},{"location":"datasets/combined_source_and_target/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset.__len__","text":"Returns: Type Description int The length of the target dataset. Source code in pytorch_adapt\\datasets\\combined_source_and_target.py def __len__ ( self ) -> int : \"\"\" Returns: The length of the target dataset. \"\"\" return len ( self . target_dataset )","title":"__len__()"},{"location":"datasets/concat_dataset/","text":"pytorch_adapt.datasets.concat_dataset \u00b6 ConcatDataset \u00b6 Exactly the same as torch.utils.data.ConcatDataset except with a nice __repr__ function.","title":"ConcatDataset"},{"location":"datasets/concat_dataset/#pytorch_adapt.datasets.concat_dataset","text":"","title":"concat_dataset"},{"location":"datasets/concat_dataset/#pytorch_adapt.datasets.concat_dataset.ConcatDataset","text":"Exactly the same as torch.utils.data.ConcatDataset except with a nice __repr__ function.","title":"ConcatDataset"},{"location":"datasets/dataloader_creator/","text":"pytorch_adapt.datasets.dataloader_creator \u00b6 DataloaderCreator \u00b6 This is a factory class for creating dataloaders. The __call__ function takes in keyword arguments which are datasets, and outputs a dictionary of dataloaders (one dataloader for each input dataset). __call__ ( self , ** kwargs ) special \u00b6 Parameters: Name Type Description Default **kwargs keyword arguments mapping from dataset names to datasets. {} Returns: Type Description Dict[str, torch.utils.data.dataloader.DataLoader] a dictionary mapping from dataset names to dataloaders. Source code in pytorch_adapt\\datasets\\dataloader_creator.py def __call__ ( self , ** kwargs ) -> Dict [ str , DataLoader ]: \"\"\" Arguments: **kwargs: keyword arguments mapping from dataset names to datasets. Returns: a dictionary mapping from dataset names to dataloaders. \"\"\" output = {} for k , v in kwargs . items (): if self . all_train : dataloader_kwargs = self . train_kwargs elif self . all_val : dataloader_kwargs = self . val_kwargs elif k in self . train_names : dataloader_kwargs = self . train_kwargs elif k in self . val_names : dataloader_kwargs = self . val_kwargs else : raise ValueError ( f \"Dataset split name must be in { self . train_names } or { self . val_names } , or one of self.all_train or self.all_val must be true\" ) output [ k ] = torch . utils . data . DataLoader ( v , ** dataloader_kwargs ) return output __init__ ( self , train_kwargs = None , val_kwargs = None , train_names = None , val_names = None , all_train = False , all_val = False , batch_size = 32 , num_workers = 0 ) special \u00b6 Parameters: Name Type Description Default train_kwargs Dict[str, Any] The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. None val_kwargs Dict[str, Any] The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. None train_names List[str] A list of the dataset names that are used during training. None val_names List[str] A list of the dataset names that are used during validation. None all_train bool If True, then all input datasets are assumed to be for training, regardless of their names. False all_val bool If True, then all input datasets are assumed to be for validation, regardless of their names. False batch_size int The default batch_size used in train_kwargs (if not provided) and val_kwargs (if not provided) 32 num_workers int The default num_workers used in train_kwargs (if not provided) and val_kwargs (if not provided) 0 Source code in pytorch_adapt\\datasets\\dataloader_creator.py def __init__ ( self , train_kwargs : Dict [ str , Any ] = None , val_kwargs : Dict [ str , Any ] = None , train_names : List [ str ] = None , val_names : List [ str ] = None , all_train : bool = False , all_val : bool = False , batch_size : int = 32 , num_workers : int = 0 , ): \"\"\" Arguments: train_kwargs: The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. val_kwargs: The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. train_names: A list of the dataset names that are used during training. val_names: A list of the dataset names that are used during validation. all_train: If True, then all input datasets are assumed to be for training, regardless of their names. all_val: If True, then all input datasets are assumed to be for validation, regardless of their names. batch_size: The default ```batch_size``` used in train_kwargs (if not provided) and val_kwargs (if not provided) num_workers: The default ```num_workers``` used in train_kwargs (if not provided) and val_kwargs (if not provided) \"\"\" self . train_kwargs = c_f . default ( train_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : True , \"drop_last\" : True , }, ) self . val_kwargs = c_f . default ( val_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : False , \"drop_last\" : False , }, ) self . train_names = c_f . default ( train_names , [ \"train\" ]) self . val_names = c_f . default ( val_names , [ \"src_train\" , \"target_train\" , \"src_val\" , \"target_val\" ] ) if not set ( self . train_names ) . isdisjoint ( self . val_names ): raise ValueError ( f \"train_names { self . train_names } must be disjoint from val_names { self . val_names } \" ) if all_train and all_val : raise ValueError ( \"all_train and all_val cannot both be True\" ) self . all_train = all_train self . all_val = all_val","title":"DataloaderCreator"},{"location":"datasets/dataloader_creator/#pytorch_adapt.datasets.dataloader_creator","text":"","title":"dataloader_creator"},{"location":"datasets/dataloader_creator/#pytorch_adapt.datasets.dataloader_creator.DataloaderCreator","text":"This is a factory class for creating dataloaders. The __call__ function takes in keyword arguments which are datasets, and outputs a dictionary of dataloaders (one dataloader for each input dataset).","title":"DataloaderCreator"},{"location":"datasets/dataloader_creator/#pytorch_adapt.datasets.dataloader_creator.DataloaderCreator.__call__","text":"Parameters: Name Type Description Default **kwargs keyword arguments mapping from dataset names to datasets. {} Returns: Type Description Dict[str, torch.utils.data.dataloader.DataLoader] a dictionary mapping from dataset names to dataloaders. Source code in pytorch_adapt\\datasets\\dataloader_creator.py def __call__ ( self , ** kwargs ) -> Dict [ str , DataLoader ]: \"\"\" Arguments: **kwargs: keyword arguments mapping from dataset names to datasets. Returns: a dictionary mapping from dataset names to dataloaders. \"\"\" output = {} for k , v in kwargs . items (): if self . all_train : dataloader_kwargs = self . train_kwargs elif self . all_val : dataloader_kwargs = self . val_kwargs elif k in self . train_names : dataloader_kwargs = self . train_kwargs elif k in self . val_names : dataloader_kwargs = self . val_kwargs else : raise ValueError ( f \"Dataset split name must be in { self . train_names } or { self . val_names } , or one of self.all_train or self.all_val must be true\" ) output [ k ] = torch . utils . data . DataLoader ( v , ** dataloader_kwargs ) return output","title":"__call__()"},{"location":"datasets/dataloader_creator/#pytorch_adapt.datasets.dataloader_creator.DataloaderCreator.__init__","text":"Parameters: Name Type Description Default train_kwargs Dict[str, Any] The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. None val_kwargs Dict[str, Any] The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. None train_names List[str] A list of the dataset names that are used during training. None val_names List[str] A list of the dataset names that are used during validation. None all_train bool If True, then all input datasets are assumed to be for training, regardless of their names. False all_val bool If True, then all input datasets are assumed to be for validation, regardless of their names. False batch_size int The default batch_size used in train_kwargs (if not provided) and val_kwargs (if not provided) 32 num_workers int The default num_workers used in train_kwargs (if not provided) and val_kwargs (if not provided) 0 Source code in pytorch_adapt\\datasets\\dataloader_creator.py def __init__ ( self , train_kwargs : Dict [ str , Any ] = None , val_kwargs : Dict [ str , Any ] = None , train_names : List [ str ] = None , val_names : List [ str ] = None , all_train : bool = False , all_val : bool = False , batch_size : int = 32 , num_workers : int = 0 , ): \"\"\" Arguments: train_kwargs: The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. val_kwargs: The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. train_names: A list of the dataset names that are used during training. val_names: A list of the dataset names that are used during validation. all_train: If True, then all input datasets are assumed to be for training, regardless of their names. all_val: If True, then all input datasets are assumed to be for validation, regardless of their names. batch_size: The default ```batch_size``` used in train_kwargs (if not provided) and val_kwargs (if not provided) num_workers: The default ```num_workers``` used in train_kwargs (if not provided) and val_kwargs (if not provided) \"\"\" self . train_kwargs = c_f . default ( train_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : True , \"drop_last\" : True , }, ) self . val_kwargs = c_f . default ( val_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : False , \"drop_last\" : False , }, ) self . train_names = c_f . default ( train_names , [ \"train\" ]) self . val_names = c_f . default ( val_names , [ \"src_train\" , \"target_train\" , \"src_val\" , \"target_val\" ] ) if not set ( self . train_names ) . isdisjoint ( self . val_names ): raise ValueError ( f \"train_names { self . train_names } must be disjoint from val_names { self . val_names } \" ) if all_train and all_val : raise ValueError ( \"all_train and all_val cannot both be True\" ) self . all_train = all_train self . all_val = all_val","title":"__init__()"},{"location":"datasets/domainnet/","text":"pytorch_adapt.datasets.domainnet \u00b6 DomainNet \u00b6 A large dataset used in \"Moment Matching for Multi-Source Domain Adaptation\". It consists of 345 classes in 6 domains: clipart, infograph, painting, quickdraw, real, sketch __init__ ( self , root , domain , train , transform , ** kwargs ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 6 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 6 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" { domain } _ { name } .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 33525 , \"test\" : 14604 }[ name ], \"infograph\" : { \"train\" : 36023 , \"test\" : 15582 }[ name ], \"painting\" : { \"train\" : 50416 , \"test\" : 21850 }[ name ], \"quickdraw\" : { \"train\" : 120750 , \"test\" : 51750 }[ name ], \"real\" : { \"train\" : 120906 , \"test\" : 52041 }[ name ], \"sketch\" : { \"train\" : 48212 , \"test\" : 20916 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform DomainNet126 \u00b6 A custom train/test split of DomainNet126Full. __init__ ( self , root , domain , train , transform , ** kwargs ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 4 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" { domain } 126_ { name } .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 14962 , \"test\" : 3741 }[ name ], \"painting\" : { \"train\" : 25201 , \"test\" : 6301 }[ name ], \"real\" : { \"train\" : 56286 , \"test\" : 14072 }[ name ], \"sketch\" : { \"train\" : 19665 , \"test\" : 4917 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform DomainNet126Full \u00b6 A subset of DomainNet consisting of 126 classes and 4 domains: clipart, painting, real, sketch __init__ ( self , root , domain , transform , ** kwargs ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 4 domains required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py def __init__ ( self , root : str , domain : str , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) filenames = [ f \"labeled_source_images_ { domain } \" , f \"labeled_target_images_ { domain } _1\" , f \"labeled_target_images_ { domain } _3\" , f \"unlabeled_target_images_ { domain } _1\" , f \"unlabeled_target_images_ { domain } _3\" , f \"validation_target_images_ { domain } _3\" , ] filenames = [ os . path . join ( root , \"domainnet\" , f \" { f } .txt\" ) for f in filenames ] img_dir = os . path . join ( root , \"domainnet\" ) content = OrderedDict () for f in filenames : with open ( f ) as fff : for line in fff : path , label = line . rstrip () . split ( \" \" ) content [ path ] = label self . img_paths = [ os . path . join ( img_dir , x ) for x in content . keys ()] check_img_paths ( img_dir , self . img_paths , domain ) self . labels = [ int ( x ) for x in content . values ()] self . transform = transform","title":"DomainNet"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet","text":"","title":"domainnet"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet","text":"A large dataset used in \"Moment Matching for Multi-Source Domain Adaptation\". It consists of 345 classes in 6 domains: clipart, infograph, painting, quickdraw, real, sketch","title":"DomainNet"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 6 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 6 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" { domain } _ { name } .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 33525 , \"test\" : 14604 }[ name ], \"infograph\" : { \"train\" : 36023 , \"test\" : 15582 }[ name ], \"painting\" : { \"train\" : 50416 , \"test\" : 21850 }[ name ], \"quickdraw\" : { \"train\" : 120750 , \"test\" : 51750 }[ name ], \"real\" : { \"train\" : 120906 , \"test\" : 52041 }[ name ], \"sketch\" : { \"train\" : 48212 , \"test\" : 20916 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"__init__()"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet126","text":"A custom train/test split of DomainNet126Full.","title":"DomainNet126"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet126.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 4 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" { domain } 126_ { name } .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 14962 , \"test\" : 3741 }[ name ], \"painting\" : { \"train\" : 25201 , \"test\" : 6301 }[ name ], \"real\" : { \"train\" : 56286 , \"test\" : 14072 }[ name ], \"sketch\" : { \"train\" : 19665 , \"test\" : 4917 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"__init__()"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet126Full","text":"A subset of DomainNet consisting of 126 classes and 4 domains: clipart, painting, real, sketch","title":"DomainNet126Full"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet126Full.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 4 domains required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py def __init__ ( self , root : str , domain : str , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) filenames = [ f \"labeled_source_images_ { domain } \" , f \"labeled_target_images_ { domain } _1\" , f \"labeled_target_images_ { domain } _3\" , f \"unlabeled_target_images_ { domain } _1\" , f \"unlabeled_target_images_ { domain } _3\" , f \"validation_target_images_ { domain } _3\" , ] filenames = [ os . path . join ( root , \"domainnet\" , f \" { f } .txt\" ) for f in filenames ] img_dir = os . path . join ( root , \"domainnet\" ) content = OrderedDict () for f in filenames : with open ( f ) as fff : for line in fff : path , label = line . rstrip () . split ( \" \" ) content [ path ] = label self . img_paths = [ os . path . join ( img_dir , x ) for x in content . keys ()] check_img_paths ( img_dir , self . img_paths , domain ) self . labels = [ int ( x ) for x in content . values ()] self . transform = transform","title":"__init__()"},{"location":"datasets/mnistm/","text":"pytorch_adapt.datasets.mnistm \u00b6 MNISTM \u00b6 The dataset used in \"Domain-Adversarial Training of Neural Networks\". It consists of colored MNIST digits. __init__ ( self , root , train , transform , ** kwargs ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/mnist_m required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\mnistm.py def __init__ ( self , root : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/mnist_m``` train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = \"MNISTM\" , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ { name } _labels.txt\" ) img_dir = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ { name } \" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_length ( self , { \"train\" : 59001 , \"test\" : 9001 }[ name ]) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"MNISTM"},{"location":"datasets/mnistm/#pytorch_adapt.datasets.mnistm","text":"","title":"mnistm"},{"location":"datasets/mnistm/#pytorch_adapt.datasets.mnistm.MNISTM","text":"The dataset used in \"Domain-Adversarial Training of Neural Networks\". It consists of colored MNIST digits.","title":"MNISTM"},{"location":"datasets/mnistm/#pytorch_adapt.datasets.mnistm.MNISTM.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/mnist_m required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\mnistm.py def __init__ ( self , root : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/mnist_m``` train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = \"MNISTM\" , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ { name } _labels.txt\" ) img_dir = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ { name } \" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_length ( self , { \"train\" : 59001 , \"test\" : 9001 }[ name ]) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"__init__()"},{"location":"datasets/office31/","text":"pytorch_adapt.datasets.office31 \u00b6 Office31 \u00b6 A custom train/test split of Office31Full. __init__ ( self , root , domain , train , transform , ** kwargs ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/office31 required domain str One of the 3 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\office31.py def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of the 3 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"office31\" , f \" { domain } _ { name } .txt\" ) img_dir = os . path . join ( root , \"office31\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"amazon\" : { \"train\" : 2253 , \"test\" : 564 }[ name ], \"dslr\" : { \"train\" : 398 , \"test\" : 100 }[ name ], \"webcam\" : { \"train\" : 636 , \"test\" : 159 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform Office31Full \u00b6 A small dataset consisting of 31 classes in 3 domains: amazon, dslr, webcam. __init__ ( self , root , domain , transform ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/office31 required domain str One of the 3 domains required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\office31.py def __init__ ( self , root : str , domain : str , transform ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of the 3 domains transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain ) self . transform = transform self . dataset = torch_datasets . ImageFolder ( os . path . join ( root , \"office31\" , domain , \"images\" ), transform = self . transform ) check_length ( self , { \"amazon\" : 2817 , \"dslr\" : 498 , \"webcam\" : 795 }[ domain ])","title":"Office31"},{"location":"datasets/office31/#pytorch_adapt.datasets.office31","text":"","title":"office31"},{"location":"datasets/office31/#pytorch_adapt.datasets.office31.Office31","text":"A custom train/test split of Office31Full.","title":"Office31"},{"location":"datasets/office31/#pytorch_adapt.datasets.office31.Office31.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/office31 required domain str One of the 3 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\office31.py def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of the 3 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"office31\" , f \" { domain } _ { name } .txt\" ) img_dir = os . path . join ( root , \"office31\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"amazon\" : { \"train\" : 2253 , \"test\" : 564 }[ name ], \"dslr\" : { \"train\" : 398 , \"test\" : 100 }[ name ], \"webcam\" : { \"train\" : 636 , \"test\" : 159 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"__init__()"},{"location":"datasets/office31/#pytorch_adapt.datasets.office31.Office31Full","text":"A small dataset consisting of 31 classes in 3 domains: amazon, dslr, webcam.","title":"Office31Full"},{"location":"datasets/office31/#pytorch_adapt.datasets.office31.Office31Full.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/office31 required domain str One of the 3 domains required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\office31.py def __init__ ( self , root : str , domain : str , transform ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of the 3 domains transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain ) self . transform = transform self . dataset = torch_datasets . ImageFolder ( os . path . join ( root , \"office31\" , domain , \"images\" ), transform = self . transform ) check_length ( self , { \"amazon\" : 2817 , \"dslr\" : 498 , \"webcam\" : 795 }[ domain ])","title":"__init__()"},{"location":"datasets/pseudo_labeled_dataset/","text":"pytorch_adapt.datasets.pseudo_labeled_dataset \u00b6 PseudoLabeledDataset \u00b6 This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) . The label returned by the wrapped dataset is discarded, and the pseudo label is returned instead. __getitem__ ( self , idx ) special \u00b6 Returns: Type Description A dictionary with keys \"src_imgs\" (the data) \"src_domain\" (the integer representing the domain) \"src_labels\" (the pseudo label) \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py def __getitem__ ( self , idx : int ): \"\"\" Returns: A dictionary with keys: - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the pseudo label) - \"src_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : self . pseudo_labels [ idx ], \"src_sample_idx\" : idx , } __init__ ( self , dataset , pseudo_labels , domain = 0 ) special \u00b6 Parameters: Name Type Description Default dataset Dataset The dataset to wrap required pseudo_labels List[int] The class labels that will be used instead of the labels contained in self.dataset required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py def __init__ ( self , dataset : Dataset , pseudo_labels : List [ int ], domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap pseudo_labels: The class labels that will be used instead of the labels contained in self.dataset domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain ) if len ( self . dataset ) != len ( pseudo_labels ): raise ValueError ( \"len(dataset) must equal len(pseudo_labels)\" ) self . pseudo_labels = pseudo_labels","title":"PseudoLabeledDataset"},{"location":"datasets/pseudo_labeled_dataset/#pytorch_adapt.datasets.pseudo_labeled_dataset","text":"","title":"pseudo_labeled_dataset"},{"location":"datasets/pseudo_labeled_dataset/#pytorch_adapt.datasets.pseudo_labeled_dataset.PseudoLabeledDataset","text":"This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) . The label returned by the wrapped dataset is discarded, and the pseudo label is returned instead.","title":"PseudoLabeledDataset"},{"location":"datasets/pseudo_labeled_dataset/#pytorch_adapt.datasets.pseudo_labeled_dataset.PseudoLabeledDataset.__getitem__","text":"Returns: Type Description A dictionary with keys \"src_imgs\" (the data) \"src_domain\" (the integer representing the domain) \"src_labels\" (the pseudo label) \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py def __getitem__ ( self , idx : int ): \"\"\" Returns: A dictionary with keys: - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the pseudo label) - \"src_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : self . pseudo_labels [ idx ], \"src_sample_idx\" : idx , }","title":"__getitem__()"},{"location":"datasets/pseudo_labeled_dataset/#pytorch_adapt.datasets.pseudo_labeled_dataset.PseudoLabeledDataset.__init__","text":"Parameters: Name Type Description Default dataset Dataset The dataset to wrap required pseudo_labels List[int] The class labels that will be used instead of the labels contained in self.dataset required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py def __init__ ( self , dataset : Dataset , pseudo_labels : List [ int ], domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap pseudo_labels: The class labels that will be used instead of the labels contained in self.dataset domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain ) if len ( self . dataset ) != len ( pseudo_labels ): raise ValueError ( \"len(dataset) must equal len(pseudo_labels)\" ) self . pseudo_labels = pseudo_labels","title":"__init__()"},{"location":"datasets/source_dataset/","text":"pytorch_adapt.datasets.source_dataset \u00b6 SourceDataset \u00b6 This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) . __getitem__ ( self , idx ) special \u00b6 Returns: Type Description Dict[str, Any] A dictionary with keys: \"src_imgs\" (the data) \"src_domain\" (the integer representing the domain) \"src_labels\" (the class label) \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\source_dataset.py def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys: - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the class label) - \"src_sample_idx\" (idx) \"\"\" img , src_labels = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : src_labels , \"src_sample_idx\" : idx , } __init__ ( self , dataset , domain = 0 ) special \u00b6 Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\source_dataset.py def __init__ ( self , dataset : Dataset , domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain )","title":"SourceDataset"},{"location":"datasets/source_dataset/#pytorch_adapt.datasets.source_dataset","text":"","title":"source_dataset"},{"location":"datasets/source_dataset/#pytorch_adapt.datasets.source_dataset.SourceDataset","text":"This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) .","title":"SourceDataset"},{"location":"datasets/source_dataset/#pytorch_adapt.datasets.source_dataset.SourceDataset.__getitem__","text":"Returns: Type Description Dict[str, Any] A dictionary with keys: \"src_imgs\" (the data) \"src_domain\" (the integer representing the domain) \"src_labels\" (the class label) \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\source_dataset.py def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys: - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the class label) - \"src_sample_idx\" (idx) \"\"\" img , src_labels = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : src_labels , \"src_sample_idx\" : idx , }","title":"__getitem__()"},{"location":"datasets/source_dataset/#pytorch_adapt.datasets.source_dataset.SourceDataset.__init__","text":"Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\source_dataset.py def __init__ ( self , dataset : Dataset , domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain )","title":"__init__()"},{"location":"datasets/target_dataset/","text":"pytorch_adapt.datasets.target_dataset \u00b6 TargetDataset \u00b6 This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) . __getitem__ ( self , idx ) special \u00b6 Returns: Type Description Dict[str, Any] A dictionary with keys: \"target_imgs\" (the data) \"target_domain\" (the integer representing the domain) \"target_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\target_dataset.py def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys: - \"target_imgs\" (the data) - \"target_domain\" (the integer representing the domain) - \"target_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"target_imgs\" : img , \"target_domain\" : self . domain , \"target_sample_idx\" : idx , } __init__ ( self , dataset , domain = 1 ) special \u00b6 Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 1 Source code in pytorch_adapt\\datasets\\target_dataset.py def __init__ ( self , dataset : Dataset , domain : int = 1 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain )","title":"TargetDataset"},{"location":"datasets/target_dataset/#pytorch_adapt.datasets.target_dataset","text":"","title":"target_dataset"},{"location":"datasets/target_dataset/#pytorch_adapt.datasets.target_dataset.TargetDataset","text":"This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) .","title":"TargetDataset"},{"location":"datasets/target_dataset/#pytorch_adapt.datasets.target_dataset.TargetDataset.__getitem__","text":"Returns: Type Description Dict[str, Any] A dictionary with keys: \"target_imgs\" (the data) \"target_domain\" (the integer representing the domain) \"target_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\target_dataset.py def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys: - \"target_imgs\" (the data) - \"target_domain\" (the integer representing the domain) - \"target_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"target_imgs\" : img , \"target_domain\" : self . domain , \"target_sample_idx\" : idx , }","title":"__getitem__()"},{"location":"datasets/target_dataset/#pytorch_adapt.datasets.target_dataset.TargetDataset.__init__","text":"Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 1 Source code in pytorch_adapt\\datasets\\target_dataset.py def __init__ ( self , dataset : Dataset , domain : int = 1 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain )","title":"__init__()"},{"location":"frameworks/","text":"","title":"Frameworks"},{"location":"frameworks/ignite/","text":"","title":"Ignite"},{"location":"frameworks/ignite/ignite/","text":"pytorch_adapt.frameworks.ignite.ignite \u00b6","title":"Ignite"},{"location":"frameworks/ignite/ignite/#pytorch_adapt.frameworks.ignite.ignite","text":"","title":"ignite"},{"location":"frameworks/ignite/loggers/","text":"pytorch_adapt.frameworks.ignite.loggers \u00b6","title":"Loggers"},{"location":"frameworks/ignite/loggers/#pytorch_adapt.frameworks.ignite.loggers","text":"","title":"loggers"},{"location":"hooks/","text":"Hooks \u00b6 Hooks are the main building block of this library. Every hook is a callable that takes in 2 arguments that represent the current context: A dictionary of previously computed losses. A dictionary of everything else that has been previously computed or passed in. The purpose of the context is to compute data only when necessary. For example, to compute a classification loss, a hook will need logits. If these logits are not available in the context, then they are computed, added to the context, and then used to compute the loss. If they are already in the context, then only the loss is computed.","title":"Hooks"},{"location":"hooks/#hooks","text":"Hooks are the main building block of this library. Every hook is a callable that takes in 2 arguments that represent the current context: A dictionary of previously computed losses. A dictionary of everything else that has been previously computed or passed in. The purpose of the context is to compute data only when necessary. For example, to compute a classification loss, a hook will need logits. If these logits are not available in the context, then they are computed, added to the context, and then used to compute the loss. If they are already in the context, then only the loss is computed.","title":"Hooks"},{"location":"hooks/adda/","text":"pytorch_adapt.hooks.adda \u00b6 ADDAHook \u00b6 Implementation of Adversarial Discriminative Domain Adaptation . Extends GANHook . __init__ ( self , threshold = 0.6 , pre_g = None , post_g = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default threshold float In each training iteration, the generator is only updated if the discriminator's accuracy is greater than threshold . 0.6 Source code in pytorch_adapt\\hooks\\adda.py def __init__ ( self , threshold : float = 0.6 , pre_g = None , post_g = None , ** kwargs ): \"\"\" Arguments: threshold: In each training iteration, the generator is only updated if the discriminator's accuracy is greater than ```threshold```. \"\"\" [ pre_g , post_g ] = c_f . many_default ([ pre_g , post_g ], [[], []]) sf_frozen = FrozenModelHook ( FeaturesHook ( detach = True , domains = [ \"src\" ]), \"G\" ) tf_all = FeaturesWithGradAndDetachedHook ( model_name = \"T\" , domains = [ \"target\" ]) pre_d = ChainHook ( sf_frozen , tf_all ) num_pre_g = len ( pre_g ) gen_conditions = [ TrueHook () for _ in range ( num_pre_g + len ( post_g ) + 2 )] # generator condition, classifier condition gen_conditions [ num_pre_g : num_pre_g + 2 ] = [ StrongDHook ( threshold ), FalseHook (), ] super () . __init__ ( pre_d = [ pre_d ], pre_g = pre_g , post_g = post_g , gen_conditions = gen_conditions , gen_domains = [ \"target\" ], c_hook = EmptyHook (), ** kwargs )","title":"ADDAHook"},{"location":"hooks/adda/#pytorch_adapt.hooks.adda","text":"","title":"adda"},{"location":"hooks/adda/#pytorch_adapt.hooks.adda.ADDAHook","text":"Implementation of Adversarial Discriminative Domain Adaptation . Extends GANHook .","title":"ADDAHook"},{"location":"hooks/adda/#pytorch_adapt.hooks.adda.ADDAHook.__init__","text":"Parameters: Name Type Description Default threshold float In each training iteration, the generator is only updated if the discriminator's accuracy is greater than threshold . 0.6 Source code in pytorch_adapt\\hooks\\adda.py def __init__ ( self , threshold : float = 0.6 , pre_g = None , post_g = None , ** kwargs ): \"\"\" Arguments: threshold: In each training iteration, the generator is only updated if the discriminator's accuracy is greater than ```threshold```. \"\"\" [ pre_g , post_g ] = c_f . many_default ([ pre_g , post_g ], [[], []]) sf_frozen = FrozenModelHook ( FeaturesHook ( detach = True , domains = [ \"src\" ]), \"G\" ) tf_all = FeaturesWithGradAndDetachedHook ( model_name = \"T\" , domains = [ \"target\" ]) pre_d = ChainHook ( sf_frozen , tf_all ) num_pre_g = len ( pre_g ) gen_conditions = [ TrueHook () for _ in range ( num_pre_g + len ( post_g ) + 2 )] # generator condition, classifier condition gen_conditions [ num_pre_g : num_pre_g + 2 ] = [ StrongDHook ( threshold ), FalseHook (), ] super () . __init__ ( pre_d = [ pre_d ], pre_g = pre_g , post_g = post_g , gen_conditions = gen_conditions , gen_domains = [ \"target\" ], c_hook = EmptyHook (), ** kwargs )","title":"__init__()"},{"location":"hooks/atdoc/","text":"pytorch_adapt.hooks.atdoc \u00b6 ATDOCHook \u00b6 Creates pseudo labels for the target domain using k-nearest neighbors. Then computes a classification loss based on these pseudo labels. Implementation of Domain Adaptation with Auxiliary Target Domain-Oriented Classifier . __init__ ( self , dataset_size , feature_dim , num_classes , k = 5 , loss_fn = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default dataset_size The number of samples in the target dataset. required feature_dim The feature dimensionality, i.e at each iteration the features should be size (N, D) where N is batch size and D is feature_dim . required num_classes The number of class labels in the target dataset. required k The number of nearest neighbors used to determine each sample's pseudolabel 5 loss_fn The classification loss function. If None it defaults to torch.nn.CrossEntropyLoss . None Source code in pytorch_adapt\\hooks\\atdoc.py def __init__ ( self , dataset_size , feature_dim , num_classes , k = 5 , loss_fn = None , ** kwargs ): \"\"\" Arguments: dataset_size: The number of samples in the target dataset. feature_dim: The feature dimensionality, i.e at each iteration the features should be size ```(N, D)``` where N is batch size and D is ```feature_dim```. num_classes: The number of class labels in the target dataset. k: The number of nearest neighbors used to determine each sample's pseudolabel loss_fn: The classification loss function. If ```None``` it defaults to ```torch.nn.CrossEntropyLoss```. \"\"\" super () . __init__ ( ** kwargs ) self . labeler = NeighborhoodAggregation ( dataset_size , feature_dim , num_classes , k = k ) self . weighter = ConfidenceWeights () self . loss_fn = c_f . default ( loss_fn , torch . nn . CrossEntropyLoss , { \"reduction\" : \"none\" } ) self . hook = FeaturesAndLogitsHook ( domains = [ \"target\" ])","title":"ATDOCHook"},{"location":"hooks/atdoc/#pytorch_adapt.hooks.atdoc","text":"","title":"atdoc"},{"location":"hooks/atdoc/#pytorch_adapt.hooks.atdoc.ATDOCHook","text":"Creates pseudo labels for the target domain using k-nearest neighbors. Then computes a classification loss based on these pseudo labels. Implementation of Domain Adaptation with Auxiliary Target Domain-Oriented Classifier .","title":"ATDOCHook"},{"location":"hooks/atdoc/#pytorch_adapt.hooks.atdoc.ATDOCHook.__init__","text":"Parameters: Name Type Description Default dataset_size The number of samples in the target dataset. required feature_dim The feature dimensionality, i.e at each iteration the features should be size (N, D) where N is batch size and D is feature_dim . required num_classes The number of class labels in the target dataset. required k The number of nearest neighbors used to determine each sample's pseudolabel 5 loss_fn The classification loss function. If None it defaults to torch.nn.CrossEntropyLoss . None Source code in pytorch_adapt\\hooks\\atdoc.py def __init__ ( self , dataset_size , feature_dim , num_classes , k = 5 , loss_fn = None , ** kwargs ): \"\"\" Arguments: dataset_size: The number of samples in the target dataset. feature_dim: The feature dimensionality, i.e at each iteration the features should be size ```(N, D)``` where N is batch size and D is ```feature_dim```. num_classes: The number of class labels in the target dataset. k: The number of nearest neighbors used to determine each sample's pseudolabel loss_fn: The classification loss function. If ```None``` it defaults to ```torch.nn.CrossEntropyLoss```. \"\"\" super () . __init__ ( ** kwargs ) self . labeler = NeighborhoodAggregation ( dataset_size , feature_dim , num_classes , k = k ) self . weighter = ConfidenceWeights () self . loss_fn = c_f . default ( loss_fn , torch . nn . CrossEntropyLoss , { \"reduction\" : \"none\" } ) self . hook = FeaturesAndLogitsHook ( domains = [ \"target\" ])","title":"__init__()"},{"location":"hooks/cdan/","text":"pytorch_adapt.hooks.cdan \u00b6 CDANHook \u00b6 Implementation of Conditional Adversarial Domain Adaptation Extends GANHook .","title":"CDANHook"},{"location":"hooks/cdan/#pytorch_adapt.hooks.cdan","text":"","title":"cdan"},{"location":"hooks/cdan/#pytorch_adapt.hooks.cdan.CDANHook","text":"Implementation of Conditional Adversarial Domain Adaptation Extends GANHook .","title":"CDANHook"},{"location":"hooks/dann/","text":"pytorch_adapt.hooks.dann \u00b6 DANNHook \u00b6 Implementation of Domain-Adversarial Training of Neural Networks . This includes the model optimization step. __init__ ( self , opts , weighter = None , reducer = None , pre = None , pre_d = None , post_d = None , pre_g = None , post_g = None , gradient_reversal = None , use_logits = False , f_hook = None , d_hook = None , c_hook = None , domain_loss_hook = None , d_hook_allowed = '_dlogits$' , ** kwargs ) special \u00b6 Parameters: Name Type Description Default opts List of optimizers for updating the models. required weighter Weights the losses before backpropagation. If None then it defaults to MeanWeighter None reducer Reduces loss tensors. If None then it defaults to MeanReducer None pre List of hooks that will be executed at the very beginning of each iteration. None pre_d List of hooks that will be executed after gradient reversal, but before the domain loss. None post_d List of hooks that will be executed after gradient reversal, and after the domain loss. None pre_g List of hooks that will be executed outside of the gradient reversal step, and before the generator and classifier loss. None post_g List of hooks that will be executed after the generator and classifier losses. None gradient_reversal Called before all D hooks, including pre_d . None use_logits If True , then D receives the output of C instead of the output of G. False f_hook The hook used for computing features and logits. If None then it defaults to FeaturesForDomainLossHook None d_hook The hook used for computing discriminator logits. If None then it defaults to DLogitsHook None c_hook The hook used for computing the classifiers's loss. If None then it defaults to CLossHook None domain_loss_hook The hook used for computing the domain loss. If None then it defaults to DomainLossHook . None d_hook_allowed A regex string that specifies the allowed output names of the discriminator block. '_dlogits$' Source code in pytorch_adapt\\hooks\\dann.py def __init__ ( self , opts , weighter = None , reducer = None , pre = None , pre_d = None , post_d = None , pre_g = None , post_g = None , gradient_reversal = None , use_logits = False , f_hook = None , d_hook = None , c_hook = None , domain_loss_hook = None , d_hook_allowed = \"_dlogits$\" , ** kwargs ): \"\"\" Arguments: opts: List of optimizers for updating the models. weighter: Weights the losses before backpropagation. If ```None``` then it defaults to [```MeanWeighter```][pytorch_adapt.weighters.mean_weighter.MeanWeighter] reducer: Reduces loss tensors. If ```None``` then it defaults to [```MeanReducer```][pytorch_adapt.hooks.reducers.MeanReducer] pre: List of hooks that will be executed at the very beginning of each iteration. pre_d: List of hooks that will be executed after gradient reversal, but before the domain loss. post_d: List of hooks that will be executed after gradient reversal, and after the domain loss. pre_g: List of hooks that will be executed outside of the gradient reversal step, and before the generator and classifier loss. post_g: List of hooks that will be executed after the generator and classifier losses. gradient_reversal: Called before all D hooks, including ```pre_d```. use_logits: If ```True```, then D receives the output of C instead of the output of G. f_hook: The hook used for computing features and logits. If ```None``` then it defaults to [```FeaturesForDomainLossHook```][pytorch_adapt.hooks.domain.FeaturesForDomainLossHook] d_hook: The hook used for computing discriminator logits. If ```None``` then it defaults to [```DLogitsHook```][pytorch_adapt.hooks.features.DLogitsHook] c_hook: The hook used for computing the classifiers's loss. If ```None``` then it defaults to [```CLossHook```][pytorch_adapt.hooks.classification.CLossHook] domain_loss_hook: The hook used for computing the domain loss. If ```None``` then it defaults to [```DomainLossHook```][pytorch_adapt.hooks.domain.DomainLossHook]. d_hook_allowed: A regex string that specifies the allowed output names of the discriminator block. \"\"\" super () . __init__ ( ** kwargs ) [ pre , pre_d , post_d , pre_g , post_g ] = c_f . many_default ( [ pre , pre_d , post_d , pre_g , post_g ], [[], [], [], [], []] ) f_hook = c_f . default ( f_hook , FeaturesForDomainLossHook , { \"use_logits\" : use_logits } ) gradient_reversal = c_f . default ( gradient_reversal , GradientReversalHook , { \"apply_to\" : f_hook . out_keys } ) c_hook = c_f . default ( c_hook , CLossHook , {}) domain_loss_hook = c_f . default ( domain_loss_hook , DomainLossHook , { \"f_hook\" : f_hook , \"d_hook\" : d_hook } ) disc_hook = AssertHook ( OnlyNewOutputsHook ( ChainHook ( gradient_reversal , * pre_d , domain_loss_hook , * post_d , overwrite = [ 1 ], ) ), d_hook_allowed , ) gen_hook = ChainHook ( * pre_g , c_hook , * post_g ) hook = ChainHook ( * pre , f_hook , disc_hook , gen_hook ) hook = OptimizerHook ( hook , opts , weighter , reducer ) s_hook = SummaryHook ({ \"total_loss\" : hook }) self . hook = ChainHook ( hook , s_hook )","title":"DANNHook"},{"location":"hooks/dann/#pytorch_adapt.hooks.dann","text":"","title":"dann"},{"location":"hooks/dann/#pytorch_adapt.hooks.dann.DANNHook","text":"Implementation of Domain-Adversarial Training of Neural Networks . This includes the model optimization step.","title":"DANNHook"},{"location":"hooks/dann/#pytorch_adapt.hooks.dann.DANNHook.__init__","text":"Parameters: Name Type Description Default opts List of optimizers for updating the models. required weighter Weights the losses before backpropagation. If None then it defaults to MeanWeighter None reducer Reduces loss tensors. If None then it defaults to MeanReducer None pre List of hooks that will be executed at the very beginning of each iteration. None pre_d List of hooks that will be executed after gradient reversal, but before the domain loss. None post_d List of hooks that will be executed after gradient reversal, and after the domain loss. None pre_g List of hooks that will be executed outside of the gradient reversal step, and before the generator and classifier loss. None post_g List of hooks that will be executed after the generator and classifier losses. None gradient_reversal Called before all D hooks, including pre_d . None use_logits If True , then D receives the output of C instead of the output of G. False f_hook The hook used for computing features and logits. If None then it defaults to FeaturesForDomainLossHook None d_hook The hook used for computing discriminator logits. If None then it defaults to DLogitsHook None c_hook The hook used for computing the classifiers's loss. If None then it defaults to CLossHook None domain_loss_hook The hook used for computing the domain loss. If None then it defaults to DomainLossHook . None d_hook_allowed A regex string that specifies the allowed output names of the discriminator block. '_dlogits$' Source code in pytorch_adapt\\hooks\\dann.py def __init__ ( self , opts , weighter = None , reducer = None , pre = None , pre_d = None , post_d = None , pre_g = None , post_g = None , gradient_reversal = None , use_logits = False , f_hook = None , d_hook = None , c_hook = None , domain_loss_hook = None , d_hook_allowed = \"_dlogits$\" , ** kwargs ): \"\"\" Arguments: opts: List of optimizers for updating the models. weighter: Weights the losses before backpropagation. If ```None``` then it defaults to [```MeanWeighter```][pytorch_adapt.weighters.mean_weighter.MeanWeighter] reducer: Reduces loss tensors. If ```None``` then it defaults to [```MeanReducer```][pytorch_adapt.hooks.reducers.MeanReducer] pre: List of hooks that will be executed at the very beginning of each iteration. pre_d: List of hooks that will be executed after gradient reversal, but before the domain loss. post_d: List of hooks that will be executed after gradient reversal, and after the domain loss. pre_g: List of hooks that will be executed outside of the gradient reversal step, and before the generator and classifier loss. post_g: List of hooks that will be executed after the generator and classifier losses. gradient_reversal: Called before all D hooks, including ```pre_d```. use_logits: If ```True```, then D receives the output of C instead of the output of G. f_hook: The hook used for computing features and logits. If ```None``` then it defaults to [```FeaturesForDomainLossHook```][pytorch_adapt.hooks.domain.FeaturesForDomainLossHook] d_hook: The hook used for computing discriminator logits. If ```None``` then it defaults to [```DLogitsHook```][pytorch_adapt.hooks.features.DLogitsHook] c_hook: The hook used for computing the classifiers's loss. If ```None``` then it defaults to [```CLossHook```][pytorch_adapt.hooks.classification.CLossHook] domain_loss_hook: The hook used for computing the domain loss. If ```None``` then it defaults to [```DomainLossHook```][pytorch_adapt.hooks.domain.DomainLossHook]. d_hook_allowed: A regex string that specifies the allowed output names of the discriminator block. \"\"\" super () . __init__ ( ** kwargs ) [ pre , pre_d , post_d , pre_g , post_g ] = c_f . many_default ( [ pre , pre_d , post_d , pre_g , post_g ], [[], [], [], [], []] ) f_hook = c_f . default ( f_hook , FeaturesForDomainLossHook , { \"use_logits\" : use_logits } ) gradient_reversal = c_f . default ( gradient_reversal , GradientReversalHook , { \"apply_to\" : f_hook . out_keys } ) c_hook = c_f . default ( c_hook , CLossHook , {}) domain_loss_hook = c_f . default ( domain_loss_hook , DomainLossHook , { \"f_hook\" : f_hook , \"d_hook\" : d_hook } ) disc_hook = AssertHook ( OnlyNewOutputsHook ( ChainHook ( gradient_reversal , * pre_d , domain_loss_hook , * post_d , overwrite = [ 1 ], ) ), d_hook_allowed , ) gen_hook = ChainHook ( * pre_g , c_hook , * post_g ) hook = ChainHook ( * pre , f_hook , disc_hook , gen_hook ) hook = OptimizerHook ( hook , opts , weighter , reducer ) s_hook = SummaryHook ({ \"total_loss\" : hook }) self . hook = ChainHook ( hook , s_hook )","title":"__init__()"},{"location":"hooks/domain_confusion/","text":"pytorch_adapt.hooks.domain_confusion \u00b6 DomainConfusionHook \u00b6 Implementation of Simultaneous Deep Transfer Across Domains and Tasks Extends GANHook .","title":"DomainConfusionHook"},{"location":"hooks/domain_confusion/#pytorch_adapt.hooks.domain_confusion","text":"","title":"domain_confusion"},{"location":"hooks/domain_confusion/#pytorch_adapt.hooks.domain_confusion.DomainConfusionHook","text":"Implementation of Simultaneous Deep Transfer Across Domains and Tasks Extends GANHook .","title":"DomainConfusionHook"},{"location":"hooks/gan/","text":"pytorch_adapt.hooks.gan \u00b6 GANHook \u00b6 A generic GAN architecture for domain adaptation. This includes the model optimization steps. __init__ ( self , d_opts , g_opts , d_weighter = None , d_reducer = None , g_weighter = None , g_reducer = None , pre_d = None , post_d = None , pre_g = None , post_g = None , use_logits = False , disc_hook = None , gen_hook = None , disc_f_hook = None , gen_f_hook = None , disc_d_hook = None , gen_d_hook = None , c_hook = None , disc_conditions = None , disc_alts = None , gen_conditions = None , gen_alts = None , disc_domains = None , gen_domains = None , disc_domain_loss_fn = None , gen_domain_loss_fn = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default d_opts List of optimizers for the D phase. required g_opts List of optimizers for the G phase. required d_weighter A loss weighter for the D phase. If None then MeanWeighter is used. None d_reducer A loss reducer for the D phase. If None then MeanReducer is used. None g_weighter A loss weighter for the G phase. If None then MeanWeighter is used. None g_reducer A loss reducer for the G phase. If None then MeanReducer is used. None pre_d List of hooks that will be executed at the very beginning of the D phase. None post_d List of hooks that will be executed at the end of the D phase, but before the optimizers are called. None pre_g List of hooks that will be executed at the very beginning of the G phase. None post_g List of hooks that will be executed at the end of the G phase, but before the optimizers are called. None use_logits If True , then D receives the output of C instead of the output of G. False disc_hook The hook used for computing the discriminator's domain loss. If None then DomainLossHook is used. None gen_hook The hook used for computing the generator's domain loss. If None then DomainLossHook is used. None c_hook The hook used for computing the classifiers's loss. If None then CLossHook is used. None disc_conditions The condition hooks used in the ChainHook for the D phase. None disc_alts The alt hooks used in the ChainHook for the D phase. None gen_conditions The condition hooks used in the ChainHook for the G phase. None gen_alts The alt hooks used in the ChainHook for the G phase. None disc_domains The domains used to compute the discriminator's domain loss. If None , then [\"src\", \"target\"] is used. None gen_domains The domains used to compute the generators's domain loss. If None , then [\"src\", \"target\"] is used. None disc_domain_loss_fn The loss function used to compute the discriminator's domain loss. If None then torch.nn.BCEWithLogitsLoss is used. None gen_domain_loss_fn The loss function used to compute the generator's domain loss. If None then torch.nn.BCEWithLogitsLoss is used. None Source code in pytorch_adapt\\hooks\\gan.py def __init__ ( self , d_opts , g_opts , d_weighter = None , d_reducer = None , g_weighter = None , g_reducer = None , pre_d = None , post_d = None , pre_g = None , post_g = None , use_logits = False , disc_hook = None , gen_hook = None , disc_f_hook = None , gen_f_hook = None , disc_d_hook = None , gen_d_hook = None , c_hook = None , disc_conditions = None , disc_alts = None , gen_conditions = None , gen_alts = None , disc_domains = None , gen_domains = None , disc_domain_loss_fn = None , gen_domain_loss_fn = None , ** kwargs ): \"\"\" Arguments: d_opts: List of optimizers for the D phase. g_opts: List of optimizers for the G phase. d_weighter: A loss weighter for the D phase. If ```None``` then ```MeanWeighter``` is used. d_reducer: A loss reducer for the D phase. If ```None``` then ```MeanReducer``` is used. g_weighter: A loss weighter for the G phase. If ```None``` then ```MeanWeighter``` is used. g_reducer: A loss reducer for the G phase. If ```None``` then ```MeanReducer``` is used. pre_d: List of hooks that will be executed at the very beginning of the D phase. post_d: List of hooks that will be executed at the end of the D phase, but before the optimizers are called. pre_g: List of hooks that will be executed at the very beginning of the G phase. post_g: List of hooks that will be executed at the end of the G phase, but before the optimizers are called. use_logits: If ```True```, then D receives the output of C instead of the output of G. disc_hook: The hook used for computing the discriminator's domain loss. If ```None``` then ```DomainLossHook``` is used. gen_hook: The hook used for computing the generator's domain loss. If ```None``` then ```DomainLossHook``` is used. c_hook: The hook used for computing the classifiers's loss. If ```None``` then ```CLossHook``` is used. disc_conditions: The condition hooks used in the ```ChainHook``` for the D phase. disc_alts: The alt hooks used in the ```ChainHook``` for the D phase. gen_conditions: The condition hooks used in the ```ChainHook``` for the G phase. gen_alts: The alt hooks used in the ```ChainHook``` for the G phase. disc_domains: The domains used to compute the discriminator's domain loss. If ```None```, then ```[\"src\", \"target\"]``` is used. gen_domains: The domains used to compute the generators's domain loss. If ```None```, then ```[\"src\", \"target\"]``` is used. disc_domain_loss_fn: The loss function used to compute the discriminator's domain loss. If ```None``` then ```torch.nn.BCEWithLogitsLoss``` is used. gen_domain_loss_fn: The loss function used to compute the generator's domain loss. If ```None``` then ```torch.nn.BCEWithLogitsLoss``` is used. \"\"\" super () . __init__ ( ** kwargs ) [ pre_d , post_d , pre_g , post_g ] = c_f . many_default ( [ pre_d , post_d , pre_g , post_g ], [[], [], [], []] ) disc_f_hook = c_f . default ( disc_f_hook , FeaturesForDomainLossHook , { \"detach\" : True , \"use_logits\" : use_logits , \"domains\" : disc_domains }, ) gen_f_hook = c_f . default ( gen_f_hook , FeaturesForDomainLossHook , { \"use_logits\" : use_logits , \"domains\" : gen_domains }, ) c_hook = c_f . default ( c_hook , CLossHook , {}) disc_hook = c_f . default ( disc_hook , DomainLossHook , { \"d_loss_fn\" : disc_domain_loss_fn , \"loss_prefix\" : \"d_\" , \"detach_features\" : True , \"f_hook\" : disc_f_hook , \"d_hook\" : disc_d_hook , \"domains\" : disc_domains , }, ) gen_hook = c_f . default ( gen_hook , DomainLossHook , { \"d_loss_fn\" : gen_domain_loss_fn , \"loss_prefix\" : \"g_\" , \"reverse_labels\" : True , \"f_hook\" : gen_f_hook , \"d_hook\" : gen_d_hook , \"domains\" : gen_domains , }, ) # use gen_f_hook to get undetached features first disc_hook = ChainHook ( * pre_d , gen_f_hook , disc_hook , * post_d , conditions = disc_conditions , alts = disc_alts ) gen_hook = ChainHook ( * pre_g , gen_hook , c_hook , * post_g , conditions = gen_conditions , alts = gen_alts ) disc_hook = OptimizerHook ( disc_hook , d_opts , d_weighter , d_reducer ) gen_hook = OptimizerHook ( gen_hook , g_opts , g_weighter , g_reducer ) s_hook = SummaryHook ({ \"d_loss\" : disc_hook , \"g_loss\" : gen_hook }) self . hook = ChainHook ( disc_hook , gen_hook , s_hook )","title":"GANHook"},{"location":"hooks/gan/#pytorch_adapt.hooks.gan","text":"","title":"gan"},{"location":"hooks/gan/#pytorch_adapt.hooks.gan.GANHook","text":"A generic GAN architecture for domain adaptation. This includes the model optimization steps.","title":"GANHook"},{"location":"hooks/gan/#pytorch_adapt.hooks.gan.GANHook.__init__","text":"Parameters: Name Type Description Default d_opts List of optimizers for the D phase. required g_opts List of optimizers for the G phase. required d_weighter A loss weighter for the D phase. If None then MeanWeighter is used. None d_reducer A loss reducer for the D phase. If None then MeanReducer is used. None g_weighter A loss weighter for the G phase. If None then MeanWeighter is used. None g_reducer A loss reducer for the G phase. If None then MeanReducer is used. None pre_d List of hooks that will be executed at the very beginning of the D phase. None post_d List of hooks that will be executed at the end of the D phase, but before the optimizers are called. None pre_g List of hooks that will be executed at the very beginning of the G phase. None post_g List of hooks that will be executed at the end of the G phase, but before the optimizers are called. None use_logits If True , then D receives the output of C instead of the output of G. False disc_hook The hook used for computing the discriminator's domain loss. If None then DomainLossHook is used. None gen_hook The hook used for computing the generator's domain loss. If None then DomainLossHook is used. None c_hook The hook used for computing the classifiers's loss. If None then CLossHook is used. None disc_conditions The condition hooks used in the ChainHook for the D phase. None disc_alts The alt hooks used in the ChainHook for the D phase. None gen_conditions The condition hooks used in the ChainHook for the G phase. None gen_alts The alt hooks used in the ChainHook for the G phase. None disc_domains The domains used to compute the discriminator's domain loss. If None , then [\"src\", \"target\"] is used. None gen_domains The domains used to compute the generators's domain loss. If None , then [\"src\", \"target\"] is used. None disc_domain_loss_fn The loss function used to compute the discriminator's domain loss. If None then torch.nn.BCEWithLogitsLoss is used. None gen_domain_loss_fn The loss function used to compute the generator's domain loss. If None then torch.nn.BCEWithLogitsLoss is used. None Source code in pytorch_adapt\\hooks\\gan.py def __init__ ( self , d_opts , g_opts , d_weighter = None , d_reducer = None , g_weighter = None , g_reducer = None , pre_d = None , post_d = None , pre_g = None , post_g = None , use_logits = False , disc_hook = None , gen_hook = None , disc_f_hook = None , gen_f_hook = None , disc_d_hook = None , gen_d_hook = None , c_hook = None , disc_conditions = None , disc_alts = None , gen_conditions = None , gen_alts = None , disc_domains = None , gen_domains = None , disc_domain_loss_fn = None , gen_domain_loss_fn = None , ** kwargs ): \"\"\" Arguments: d_opts: List of optimizers for the D phase. g_opts: List of optimizers for the G phase. d_weighter: A loss weighter for the D phase. If ```None``` then ```MeanWeighter``` is used. d_reducer: A loss reducer for the D phase. If ```None``` then ```MeanReducer``` is used. g_weighter: A loss weighter for the G phase. If ```None``` then ```MeanWeighter``` is used. g_reducer: A loss reducer for the G phase. If ```None``` then ```MeanReducer``` is used. pre_d: List of hooks that will be executed at the very beginning of the D phase. post_d: List of hooks that will be executed at the end of the D phase, but before the optimizers are called. pre_g: List of hooks that will be executed at the very beginning of the G phase. post_g: List of hooks that will be executed at the end of the G phase, but before the optimizers are called. use_logits: If ```True```, then D receives the output of C instead of the output of G. disc_hook: The hook used for computing the discriminator's domain loss. If ```None``` then ```DomainLossHook``` is used. gen_hook: The hook used for computing the generator's domain loss. If ```None``` then ```DomainLossHook``` is used. c_hook: The hook used for computing the classifiers's loss. If ```None``` then ```CLossHook``` is used. disc_conditions: The condition hooks used in the ```ChainHook``` for the D phase. disc_alts: The alt hooks used in the ```ChainHook``` for the D phase. gen_conditions: The condition hooks used in the ```ChainHook``` for the G phase. gen_alts: The alt hooks used in the ```ChainHook``` for the G phase. disc_domains: The domains used to compute the discriminator's domain loss. If ```None```, then ```[\"src\", \"target\"]``` is used. gen_domains: The domains used to compute the generators's domain loss. If ```None```, then ```[\"src\", \"target\"]``` is used. disc_domain_loss_fn: The loss function used to compute the discriminator's domain loss. If ```None``` then ```torch.nn.BCEWithLogitsLoss``` is used. gen_domain_loss_fn: The loss function used to compute the generator's domain loss. If ```None``` then ```torch.nn.BCEWithLogitsLoss``` is used. \"\"\" super () . __init__ ( ** kwargs ) [ pre_d , post_d , pre_g , post_g ] = c_f . many_default ( [ pre_d , post_d , pre_g , post_g ], [[], [], [], []] ) disc_f_hook = c_f . default ( disc_f_hook , FeaturesForDomainLossHook , { \"detach\" : True , \"use_logits\" : use_logits , \"domains\" : disc_domains }, ) gen_f_hook = c_f . default ( gen_f_hook , FeaturesForDomainLossHook , { \"use_logits\" : use_logits , \"domains\" : gen_domains }, ) c_hook = c_f . default ( c_hook , CLossHook , {}) disc_hook = c_f . default ( disc_hook , DomainLossHook , { \"d_loss_fn\" : disc_domain_loss_fn , \"loss_prefix\" : \"d_\" , \"detach_features\" : True , \"f_hook\" : disc_f_hook , \"d_hook\" : disc_d_hook , \"domains\" : disc_domains , }, ) gen_hook = c_f . default ( gen_hook , DomainLossHook , { \"d_loss_fn\" : gen_domain_loss_fn , \"loss_prefix\" : \"g_\" , \"reverse_labels\" : True , \"f_hook\" : gen_f_hook , \"d_hook\" : gen_d_hook , \"domains\" : gen_domains , }, ) # use gen_f_hook to get undetached features first disc_hook = ChainHook ( * pre_d , gen_f_hook , disc_hook , * post_d , conditions = disc_conditions , alts = disc_alts ) gen_hook = ChainHook ( * pre_g , gen_hook , c_hook , * post_g , conditions = gen_conditions , alts = gen_alts ) disc_hook = OptimizerHook ( disc_hook , d_opts , d_weighter , d_reducer ) gen_hook = OptimizerHook ( gen_hook , g_opts , g_weighter , g_reducer ) s_hook = SummaryHook ({ \"d_loss\" : disc_hook , \"g_loss\" : gen_hook }) self . hook = ChainHook ( disc_hook , gen_hook , s_hook )","title":"__init__()"},{"location":"hooks/gvb/","text":"pytorch_adapt.hooks.gvb \u00b6 GVBHook \u00b6 Implementation of Gradually Vanishing Bridge for Adversarial Domain Adaptation","title":"GVBHook"},{"location":"hooks/gvb/#pytorch_adapt.hooks.gvb","text":"","title":"gvb"},{"location":"hooks/gvb/#pytorch_adapt.hooks.gvb.GVBHook","text":"Implementation of Gradually Vanishing Bridge for Adversarial Domain Adaptation","title":"GVBHook"},{"location":"hooks/mcd/","text":"pytorch_adapt.hooks.mcd \u00b6 MCDHook \u00b6 Implementation of Maximum Classifier Discrepancy for Unsupervised Domain Adaptation .","title":"MCDHook"},{"location":"hooks/mcd/#pytorch_adapt.hooks.mcd","text":"","title":"mcd"},{"location":"hooks/mcd/#pytorch_adapt.hooks.mcd.MCDHook","text":"Implementation of Maximum Classifier Discrepancy for Unsupervised Domain Adaptation .","title":"MCDHook"},{"location":"hooks/optimizer/","text":"pytorch_adapt.hooks.optimizer \u00b6 OptimizerHook \u00b6 Executes the wrapped hook Zeros all gradients Backpropagates the loss Steps the optimizer __init__ ( self , hook , optimizers , weighter = None , reducer = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook the hook that computes the losses required optimizers List[torch.optim.optimizer.Optimizer] a list of optimizers that will be used to update model weights required weighter BaseWeighter weights the returned losses and outputs a single value on which .backward() is called. If None , then it defaults to MeanWeighter . None reducer BaseReducer a hook that reduces any unreduced losses to a single value. If None , then it defaults to MeanReducer . None Source code in pytorch_adapt\\hooks\\optimizer.py def __init__ ( self , hook : BaseHook , optimizers : List [ torch . optim . Optimizer ], weighter : BaseWeighter = None , reducer : BaseReducer = None , ** kwargs ): \"\"\" Arguments: hook: the hook that computes the losses optimizers: a list of optimizers that will be used to update model weights weighter: weights the returned losses and outputs a single value on which ```.backward()``` is called. If ```None```, then it defaults to [```MeanWeighter```][pytorch_adapt.weighters.mean_weighter.MeanWeighter]. reducer: a hook that reduces any unreduced losses to a single value. If ```None```, then it defaults to [```MeanReducer```][pytorch_adapt.hooks.reducers.MeanReducer]. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . optimizers = optimizers self . weighter = c_f . default ( weighter , MeanWeighter , {}) self . reducer = c_f . default ( reducer , MeanReducer , {}) self . loss_components = {} SummaryHook \u00b6 Repackages losses into a dictionary format useful for logging. This should be used only at the very end of each iteration, i.e. it should be the last sub-hook in a ChainHook . __init__ ( self , optimizers , ** kwargs ) special \u00b6 Parameters: Name Type Description Default optimizers Dict[str, pytorch_adapt.hooks.optimizer.OptimizerHook] A dictionary of optimizer hooks. The losses computed inside these hooks will be packaged into nested dictionaries. required Source code in pytorch_adapt\\hooks\\optimizer.py def __init__ ( self , optimizers : Dict [ str , OptimizerHook ], ** kwargs ): \"\"\" Arguments: optimizers: A dictionary of optimizer hooks. The losses computed inside these hooks will be packaged into nested dictionaries. \"\"\" super () . __init__ ( ** kwargs ) self . optimizers = optimizers","title":"OptimizerHook"},{"location":"hooks/optimizer/#pytorch_adapt.hooks.optimizer","text":"","title":"optimizer"},{"location":"hooks/optimizer/#pytorch_adapt.hooks.optimizer.OptimizerHook","text":"Executes the wrapped hook Zeros all gradients Backpropagates the loss Steps the optimizer","title":"OptimizerHook"},{"location":"hooks/optimizer/#pytorch_adapt.hooks.optimizer.OptimizerHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook the hook that computes the losses required optimizers List[torch.optim.optimizer.Optimizer] a list of optimizers that will be used to update model weights required weighter BaseWeighter weights the returned losses and outputs a single value on which .backward() is called. If None , then it defaults to MeanWeighter . None reducer BaseReducer a hook that reduces any unreduced losses to a single value. If None , then it defaults to MeanReducer . None Source code in pytorch_adapt\\hooks\\optimizer.py def __init__ ( self , hook : BaseHook , optimizers : List [ torch . optim . Optimizer ], weighter : BaseWeighter = None , reducer : BaseReducer = None , ** kwargs ): \"\"\" Arguments: hook: the hook that computes the losses optimizers: a list of optimizers that will be used to update model weights weighter: weights the returned losses and outputs a single value on which ```.backward()``` is called. If ```None```, then it defaults to [```MeanWeighter```][pytorch_adapt.weighters.mean_weighter.MeanWeighter]. reducer: a hook that reduces any unreduced losses to a single value. If ```None```, then it defaults to [```MeanReducer```][pytorch_adapt.hooks.reducers.MeanReducer]. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . optimizers = optimizers self . weighter = c_f . default ( weighter , MeanWeighter , {}) self . reducer = c_f . default ( reducer , MeanReducer , {}) self . loss_components = {}","title":"__init__()"},{"location":"hooks/optimizer/#pytorch_adapt.hooks.optimizer.SummaryHook","text":"Repackages losses into a dictionary format useful for logging. This should be used only at the very end of each iteration, i.e. it should be the last sub-hook in a ChainHook .","title":"SummaryHook"},{"location":"hooks/optimizer/#pytorch_adapt.hooks.optimizer.SummaryHook.__init__","text":"Parameters: Name Type Description Default optimizers Dict[str, pytorch_adapt.hooks.optimizer.OptimizerHook] A dictionary of optimizer hooks. The losses computed inside these hooks will be packaged into nested dictionaries. required Source code in pytorch_adapt\\hooks\\optimizer.py def __init__ ( self , optimizers : Dict [ str , OptimizerHook ], ** kwargs ): \"\"\" Arguments: optimizers: A dictionary of optimizer hooks. The losses computed inside these hooks will be packaged into nested dictionaries. \"\"\" super () . __init__ ( ** kwargs ) self . optimizers = optimizers","title":"__init__()"},{"location":"hooks/rtn/","text":"pytorch_adapt.hooks.rtn \u00b6 RTNHook \u00b6 Implementation of Unsupervised Domain Adaptation with Residual Transfer Networks .","title":"RTNHook"},{"location":"hooks/rtn/#pytorch_adapt.hooks.rtn","text":"","title":"rtn"},{"location":"hooks/rtn/#pytorch_adapt.hooks.rtn.RTNHook","text":"Implementation of Unsupervised Domain Adaptation with Residual Transfer Networks .","title":"RTNHook"},{"location":"hooks/symnets/","text":"pytorch_adapt.hooks.symnets \u00b6 SymNetsHook \u00b6 Implementation of Domain-Symmetric Networks for Adversarial Domain Adaptation .","title":"SymNetsHook"},{"location":"hooks/symnets/#pytorch_adapt.hooks.symnets","text":"","title":"symnets"},{"location":"hooks/symnets/#pytorch_adapt.hooks.symnets.SymNetsHook","text":"Implementation of Domain-Symmetric Networks for Adversarial Domain Adaptation .","title":"SymNetsHook"},{"location":"hooks/vada/","text":"pytorch_adapt.hooks.vada \u00b6 VADAHook \u00b6 Implementation of VADA from A DIRT-T Approach to Unsupervised Domain Adaptation . VATHook \u00b6 Applies the VATLoss .","title":"VADAHook"},{"location":"hooks/vada/#pytorch_adapt.hooks.vada","text":"","title":"vada"},{"location":"hooks/vada/#pytorch_adapt.hooks.vada.VADAHook","text":"Implementation of VADA from A DIRT-T Approach to Unsupervised Domain Adaptation .","title":"VADAHook"},{"location":"hooks/vada/#pytorch_adapt.hooks.vada.VATHook","text":"Applies the VATLoss .","title":"VATHook"},{"location":"hooks/validate/","text":"pytorch_adapt.hooks.validate \u00b6 validate_hook ( hook , available_keys = None , depth = 0 , model_counts = None ) \u00b6 Parameters: Name Type Description Default hook the hook to validate required available_keys a list of keys that the context will start with. None Returns: Type Description Dict[str, int] A dictionary with each model's forward call count. Source code in pytorch_adapt\\hooks\\validate.py def validate_hook ( hook , available_keys = None , depth = 0 , model_counts = None ) -> Dict [ str , int ]: \"\"\" Arguments: hook: the hook to validate available_keys: a list of keys that the context will start with. Returns: A dictionary with each model's ```forward``` call count. \"\"\" c_f . LOGGER . debug ( f \"VALIDATE: { ' ' * depth }{ c_f . cls_name ( hook ) } \" ) available_keys = c_f . default ( available_keys , []) model_counts = c_f . default ( model_counts , defaultdict ( int )) if isinstance ( available_keys , list ): available_keys = set ( available_keys ) if isinstance ( hook , ChainHook ): hooks = hook . hooks for i in range ( 0 , len ( hooks )): validate_hook ( hooks [ i ], available_keys , depth + 1 , model_counts ) elif isinstance ( hook , ParallelHook ): hooks = hook . hooks for i in range ( 0 , len ( hooks )): curr_available_keys = copy . deepcopy ( available_keys ) validate_hook ( hooks [ i ], curr_available_keys , depth + 1 , model_counts ) elif isinstance ( hook , RepeatHook ): for _ in range ( hook . n ): curr_available_keys = copy . deepcopy ( available_keys ) validate_hook ( hook . hook , curr_available_keys , depth + 1 , model_counts ) else : check_keys_are_present ( hook , hook . in_keys , list ( available_keys ), \"in_keys\" , \"available_keys\" ) check_keys_are_present ( hook , list ( hook . key_map . keys ()), list ( available_keys ), \"key_map\" , \"available_keys\" , ) all_hooks = c_f . attrs_of_type ( hook , BaseHook ) for h in all_hooks . values (): validate_hook ( h , available_keys , depth + 1 , model_counts ) update_model_counts ( hook , available_keys , model_counts ) available_keys . update ( set ( hook . out_keys )) return model_counts","title":"Validate"},{"location":"hooks/validate/#pytorch_adapt.hooks.validate","text":"","title":"validate"},{"location":"hooks/validate/#pytorch_adapt.hooks.validate.validate_hook","text":"Parameters: Name Type Description Default hook the hook to validate required available_keys a list of keys that the context will start with. None Returns: Type Description Dict[str, int] A dictionary with each model's forward call count. Source code in pytorch_adapt\\hooks\\validate.py def validate_hook ( hook , available_keys = None , depth = 0 , model_counts = None ) -> Dict [ str , int ]: \"\"\" Arguments: hook: the hook to validate available_keys: a list of keys that the context will start with. Returns: A dictionary with each model's ```forward``` call count. \"\"\" c_f . LOGGER . debug ( f \"VALIDATE: { ' ' * depth }{ c_f . cls_name ( hook ) } \" ) available_keys = c_f . default ( available_keys , []) model_counts = c_f . default ( model_counts , defaultdict ( int )) if isinstance ( available_keys , list ): available_keys = set ( available_keys ) if isinstance ( hook , ChainHook ): hooks = hook . hooks for i in range ( 0 , len ( hooks )): validate_hook ( hooks [ i ], available_keys , depth + 1 , model_counts ) elif isinstance ( hook , ParallelHook ): hooks = hook . hooks for i in range ( 0 , len ( hooks )): curr_available_keys = copy . deepcopy ( available_keys ) validate_hook ( hooks [ i ], curr_available_keys , depth + 1 , model_counts ) elif isinstance ( hook , RepeatHook ): for _ in range ( hook . n ): curr_available_keys = copy . deepcopy ( available_keys ) validate_hook ( hook . hook , curr_available_keys , depth + 1 , model_counts ) else : check_keys_are_present ( hook , hook . in_keys , list ( available_keys ), \"in_keys\" , \"available_keys\" ) check_keys_are_present ( hook , list ( hook . key_map . keys ()), list ( available_keys ), \"key_map\" , \"available_keys\" , ) all_hooks = c_f . attrs_of_type ( hook , BaseHook ) for h in all_hooks . values (): validate_hook ( h , available_keys , depth + 1 , model_counts ) update_model_counts ( hook , available_keys , model_counts ) available_keys . update ( set ( hook . out_keys )) return model_counts","title":"validate_hook()"},{"location":"hooks/aligners/aligner_hook/","text":"pytorch_adapt.hooks.aligners \u00b6 AlignerHook \u00b6 Computes an alignment loss (e.g MMD) based on features from two domains. __init__ ( self , loss_fn = None , hook = None , layer = 'features' , ** kwargs ) special \u00b6 Parameters: Name Type Description Default loss_fn Callable[[torch.Tensor, torch.Tensor], torch.Tensor] a function that computes a distance between two tensors. If None , it defaults to MMDLoss . None hook BaseHook the hook for computing features None layer str the layer for which the loss is computed. Must be either \"features\" or \"logits\" . 'features' Source code in pytorch_adapt\\hooks\\aligners.py def __init__ ( self , loss_fn : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , hook : BaseHook = None , layer : str = \"features\" , ** kwargs , ): \"\"\" Arguments: loss_fn: a function that computes a distance between two tensors. If ```None```, it defaults to [```MMDLoss```][pytorch_adapt.layers.mmd_loss.MMDLoss]. hook: the hook for computing features layer: the layer for which the loss is computed. Must be either ```\"features\"``` or ```\"logits\"```. \"\"\" super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , MMDLoss , {}) if layer == \"features\" : default_hook = FeaturesHook elif layer == \"logits\" : default_hook = FeaturesAndLogitsHook else : raise ValueError ( \"AlignerHook layer must be 'features' or 'logits'\" ) self . hook = c_f . default ( hook , default_hook , {}) self . layer = layer","title":"AlignerHook"},{"location":"hooks/aligners/aligner_hook/#pytorch_adapt.hooks.aligners","text":"","title":"aligners"},{"location":"hooks/aligners/aligner_hook/#pytorch_adapt.hooks.aligners.AlignerHook","text":"Computes an alignment loss (e.g MMD) based on features from two domains.","title":"AlignerHook"},{"location":"hooks/aligners/aligner_hook/#pytorch_adapt.hooks.aligners.AlignerHook.__init__","text":"Parameters: Name Type Description Default loss_fn Callable[[torch.Tensor, torch.Tensor], torch.Tensor] a function that computes a distance between two tensors. If None , it defaults to MMDLoss . None hook BaseHook the hook for computing features None layer str the layer for which the loss is computed. Must be either \"features\" or \"logits\" . 'features' Source code in pytorch_adapt\\hooks\\aligners.py def __init__ ( self , loss_fn : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , hook : BaseHook = None , layer : str = \"features\" , ** kwargs , ): \"\"\" Arguments: loss_fn: a function that computes a distance between two tensors. If ```None```, it defaults to [```MMDLoss```][pytorch_adapt.layers.mmd_loss.MMDLoss]. hook: the hook for computing features layer: the layer for which the loss is computed. Must be either ```\"features\"``` or ```\"logits\"```. \"\"\" super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , MMDLoss , {}) if layer == \"features\" : default_hook = FeaturesHook elif layer == \"logits\" : default_hook = FeaturesAndLogitsHook else : raise ValueError ( \"AlignerHook layer must be 'features' or 'logits'\" ) self . hook = c_f . default ( hook , default_hook , {}) self . layer = layer","title":"__init__()"},{"location":"hooks/aligners/aligner_plus_c_hook/","text":"pytorch_adapt.hooks.aligners \u00b6 AlignerPlusCHook \u00b6 Computes an alignment loss plus a classification loss, and then optimizes the models.","title":"AlignerPlusCHook"},{"location":"hooks/aligners/aligner_plus_c_hook/#pytorch_adapt.hooks.aligners","text":"","title":"aligners"},{"location":"hooks/aligners/aligner_plus_c_hook/#pytorch_adapt.hooks.aligners.AlignerPlusCHook","text":"Computes an alignment loss plus a classification loss, and then optimizes the models.","title":"AlignerPlusCHook"},{"location":"hooks/aligners/features_logits_aligner_hook/","text":"pytorch_adapt.hooks.aligners \u00b6 FeaturesLogitsAlignerHook \u00b6 This chains together an AlignerHook for \"features\" followed by an AlignerHook for \"logits\" . __init__ ( self , loss_fn = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default loss_fn Callable[[torch.Tensor, torch.Tensor], torch.Tensor] The loss used by both aligner hooks. None Source code in pytorch_adapt\\hooks\\aligners.py def __init__ ( self , loss_fn : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , ** kwargs , ): \"\"\" Arguments: loss_fn: The loss used by both aligner hooks. \"\"\" super () . __init__ ( ** kwargs ) loss_fn = c_f . default ( loss_fn , MMDLoss , {}) a1_hook = AlignerHook ( loss_fn , layer = \"features\" ) a2_hook = AlignerHook ( loss_fn , layer = \"logits\" ) self . hook = ChainHook ( a1_hook , a2_hook )","title":"FeaturesLogitsAlignerHook"},{"location":"hooks/aligners/features_logits_aligner_hook/#pytorch_adapt.hooks.aligners","text":"","title":"aligners"},{"location":"hooks/aligners/features_logits_aligner_hook/#pytorch_adapt.hooks.aligners.FeaturesLogitsAlignerHook","text":"This chains together an AlignerHook for \"features\" followed by an AlignerHook for \"logits\" .","title":"FeaturesLogitsAlignerHook"},{"location":"hooks/aligners/features_logits_aligner_hook/#pytorch_adapt.hooks.aligners.FeaturesLogitsAlignerHook.__init__","text":"Parameters: Name Type Description Default loss_fn Callable[[torch.Tensor, torch.Tensor], torch.Tensor] The loss used by both aligner hooks. None Source code in pytorch_adapt\\hooks\\aligners.py def __init__ ( self , loss_fn : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , ** kwargs , ): \"\"\" Arguments: loss_fn: The loss used by both aligner hooks. \"\"\" super () . __init__ ( ** kwargs ) loss_fn = c_f . default ( loss_fn , MMDLoss , {}) a1_hook = AlignerHook ( loss_fn , layer = \"features\" ) a2_hook = AlignerHook ( loss_fn , layer = \"logits\" ) self . hook = ChainHook ( a1_hook , a2_hook )","title":"__init__()"},{"location":"hooks/aligners/joint_aligner_hook/","text":"pytorch_adapt.hooks.aligners \u00b6 JointAlignerHook \u00b6 Computes a joint alignment loss (e.g Joint MMD) based on multiple features from two domains. The default setting is to use the features and logits from the source and target domains. __init__ ( self , loss_fn = None , hook = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default loss_fn Callable[[List[torch.Tensor], List[torch.Tensor]], torch.Tensor] a function that computes a distance between two lists of tensors. If None , it defaults to MMDLoss . None hook BaseHook the hook for computing features and logits None Source code in pytorch_adapt\\hooks\\aligners.py def __init__ ( self , loss_fn : Callable [ [ List [ torch . Tensor ], List [ torch . Tensor ]], torch . Tensor ] = None , hook : BaseHook = None , ** kwargs , ): \"\"\" Arguments: loss_fn: a function that computes a distance between two **lists** of tensors. If ```None```, it defaults to [```MMDLoss```][pytorch_adapt.layers.mmd_loss.MMDLoss]. hook: the hook for computing features and logits \"\"\" super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , MMDLoss , {}) self . hook = c_f . default ( hook , FeaturesAndLogitsHook , {})","title":"JointAlignerHook"},{"location":"hooks/aligners/joint_aligner_hook/#pytorch_adapt.hooks.aligners","text":"","title":"aligners"},{"location":"hooks/aligners/joint_aligner_hook/#pytorch_adapt.hooks.aligners.JointAlignerHook","text":"Computes a joint alignment loss (e.g Joint MMD) based on multiple features from two domains. The default setting is to use the features and logits from the source and target domains.","title":"JointAlignerHook"},{"location":"hooks/aligners/joint_aligner_hook/#pytorch_adapt.hooks.aligners.JointAlignerHook.__init__","text":"Parameters: Name Type Description Default loss_fn Callable[[List[torch.Tensor], List[torch.Tensor]], torch.Tensor] a function that computes a distance between two lists of tensors. If None , it defaults to MMDLoss . None hook BaseHook the hook for computing features and logits None Source code in pytorch_adapt\\hooks\\aligners.py def __init__ ( self , loss_fn : Callable [ [ List [ torch . Tensor ], List [ torch . Tensor ]], torch . Tensor ] = None , hook : BaseHook = None , ** kwargs , ): \"\"\" Arguments: loss_fn: a function that computes a distance between two **lists** of tensors. If ```None```, it defaults to [```MMDLoss```][pytorch_adapt.layers.mmd_loss.MMDLoss]. hook: the hook for computing features and logits \"\"\" super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , MMDLoss , {}) self . hook = c_f . default ( hook , FeaturesAndLogitsHook , {})","title":"__init__()"},{"location":"hooks/base/base_condition_hook/","text":"pytorch_adapt.hooks.base \u00b6 BaseConditionHook \u00b6 The base class for hooks that return a boolean","title":"BaseConditionHook"},{"location":"hooks/base/base_condition_hook/#pytorch_adapt.hooks.base","text":"","title":"base"},{"location":"hooks/base/base_condition_hook/#pytorch_adapt.hooks.base.BaseConditionHook","text":"The base class for hooks that return a boolean","title":"BaseConditionHook"},{"location":"hooks/base/base_hook/","text":"pytorch_adapt.hooks.base \u00b6 BaseHook \u00b6 All hooks extend BaseHook __init__ ( self , loss_prefix = '' , loss_suffix = '' , out_prefix = '' , out_suffix = '' , key_map = None ) special \u00b6 Parameters: Name Type Description Default loss_prefix str prepended to all new loss keys '' loss_suffix str appended to all new loss keys '' out_prefix str prepended to all new output keys '' out_suffix str appended to all new output keys '' key_map Dict[str, str] a mapping from input_key to new_key . For example, if key_map = {\"A\": \"B\"}, and the input dict to __call__ is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting __call__ , the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. None Source code in pytorch_adapt\\hooks\\base.py def __init__ ( self , loss_prefix : str = \"\" , loss_suffix : str = \"\" , out_prefix : str = \"\" , out_suffix : str = \"\" , key_map : Dict [ str , str ] = None , ): \"\"\" Arguments: loss_prefix: prepended to all new loss keys loss_suffix: appended to all new loss keys out_prefix: prepended to all new output keys out_suffix: appended to all new output keys key_map: a mapping from ```input_key``` to ```new_key```. For example, if key_map = {\"A\": \"B\"}, and the input dict to ```__call__``` is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting ```__call__```, the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. \"\"\" if any ( not isinstance ( x , str ) for x in [ loss_prefix , loss_suffix , out_prefix , out_suffix ] ): raise TypeError ( \"loss prefix/suffix and out prefix/suffix must be strings\" ) self . loss_prefix = loss_prefix self . loss_suffix = loss_suffix self . out_prefix = out_prefix self . out_suffix = out_suffix self . key_map = c_f . default ( key_map , {}) self . in_keys = [] _loss_keys ( self ) private \u00b6 This must be implemented by the child class Returns: Type Description List[str] The names of the losses that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py @abstractmethod def _loss_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the losses that will be added to the context. \"\"\" pass _out_keys ( self ) private \u00b6 This must be implemented by the child class Returns: Type Description List[str] The names of the outputs that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py @abstractmethod def _out_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the outputs that will be added to the context. \"\"\" pass call ( self , losses , inputs ) \u00b6 This must be implemented by the child class Parameters: Name Type Description Default losses Dict[str, Any] previously computed losses required inputs Dict[str, Any] holds everything else: tensors, models etc. required Returns: Type Description Union[Tuple[Dict[str, Any], Dict[str, Any]], bool] Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean Source code in pytorch_adapt\\hooks\\base.py @abstractmethod def call ( self , losses : Dict [ str , Any ], inputs : Dict [ str , Any ] ) -> Union [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]], bool ]: \"\"\" This must be implemented by the child class Arguments: losses: previously computed losses inputs: holds everything else: tensors, models etc. Returns: Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean \"\"\" pass","title":"BaseHook"},{"location":"hooks/base/base_hook/#pytorch_adapt.hooks.base","text":"","title":"base"},{"location":"hooks/base/base_hook/#pytorch_adapt.hooks.base.BaseHook","text":"All hooks extend BaseHook","title":"BaseHook"},{"location":"hooks/base/base_hook/#pytorch_adapt.hooks.base.BaseHook.__init__","text":"Parameters: Name Type Description Default loss_prefix str prepended to all new loss keys '' loss_suffix str appended to all new loss keys '' out_prefix str prepended to all new output keys '' out_suffix str appended to all new output keys '' key_map Dict[str, str] a mapping from input_key to new_key . For example, if key_map = {\"A\": \"B\"}, and the input dict to __call__ is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting __call__ , the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. None Source code in pytorch_adapt\\hooks\\base.py def __init__ ( self , loss_prefix : str = \"\" , loss_suffix : str = \"\" , out_prefix : str = \"\" , out_suffix : str = \"\" , key_map : Dict [ str , str ] = None , ): \"\"\" Arguments: loss_prefix: prepended to all new loss keys loss_suffix: appended to all new loss keys out_prefix: prepended to all new output keys out_suffix: appended to all new output keys key_map: a mapping from ```input_key``` to ```new_key```. For example, if key_map = {\"A\": \"B\"}, and the input dict to ```__call__``` is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting ```__call__```, the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. \"\"\" if any ( not isinstance ( x , str ) for x in [ loss_prefix , loss_suffix , out_prefix , out_suffix ] ): raise TypeError ( \"loss prefix/suffix and out prefix/suffix must be strings\" ) self . loss_prefix = loss_prefix self . loss_suffix = loss_suffix self . out_prefix = out_prefix self . out_suffix = out_suffix self . key_map = c_f . default ( key_map , {}) self . in_keys = []","title":"__init__()"},{"location":"hooks/base/base_hook/#pytorch_adapt.hooks.base.BaseHook._loss_keys","text":"This must be implemented by the child class Returns: Type Description List[str] The names of the losses that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py @abstractmethod def _loss_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the losses that will be added to the context. \"\"\" pass","title":"_loss_keys()"},{"location":"hooks/base/base_hook/#pytorch_adapt.hooks.base.BaseHook._out_keys","text":"This must be implemented by the child class Returns: Type Description List[str] The names of the outputs that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py @abstractmethod def _out_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the outputs that will be added to the context. \"\"\" pass","title":"_out_keys()"},{"location":"hooks/base/base_hook/#pytorch_adapt.hooks.base.BaseHook.call","text":"This must be implemented by the child class Parameters: Name Type Description Default losses Dict[str, Any] previously computed losses required inputs Dict[str, Any] holds everything else: tensors, models etc. required Returns: Type Description Union[Tuple[Dict[str, Any], Dict[str, Any]], bool] Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean Source code in pytorch_adapt\\hooks\\base.py @abstractmethod def call ( self , losses : Dict [ str , Any ], inputs : Dict [ str , Any ] ) -> Union [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]], bool ]: \"\"\" This must be implemented by the child class Arguments: losses: previously computed losses inputs: holds everything else: tensors, models etc. Returns: Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean \"\"\" pass","title":"call()"},{"location":"hooks/base/base_wrapper_hook/","text":"pytorch_adapt.hooks.base \u00b6 BaseWrapperHook \u00b6 A simple wrapper for calling self.hook , which should be defined in the child's __init__ function.","title":"BaseWrapperHook"},{"location":"hooks/base/base_wrapper_hook/#pytorch_adapt.hooks.base","text":"","title":"base"},{"location":"hooks/base/base_wrapper_hook/#pytorch_adapt.hooks.base.BaseWrapperHook","text":"A simple wrapper for calling self.hook , which should be defined in the child's __init__ function.","title":"BaseWrapperHook"},{"location":"hooks/classification/classifier_hook/","text":"pytorch_adapt.hooks.classification \u00b6 ClassifierHook \u00b6 This computes the classification loss and also optimizes the models.","title":"ClassifierHook"},{"location":"hooks/classification/classifier_hook/#pytorch_adapt.hooks.classification","text":"","title":"classification"},{"location":"hooks/classification/classifier_hook/#pytorch_adapt.hooks.classification.ClassifierHook","text":"This computes the classification loss and also optimizes the models.","title":"ClassifierHook"},{"location":"hooks/classification/closs_hook/","text":"pytorch_adapt.hooks.classification \u00b6 CLossHook \u00b6 Computes a classification loss on the specified tensors. The default setting is to compute the cross entropy loss of the source domain logits. __init__ ( self , loss_fn = None , detach_features = False , f_hook = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default loss_fn Callable[[torch.Tensor, torch.Tensor], torch.Tensor] The classification loss function. If None , it defaults to torch.nn.CrossEntropyLoss . None detach_features bool Whether or not to detach the features, from which logits are computed. False f_hook BaseHook The hook for computing logits. None Source code in pytorch_adapt\\hooks\\classification.py def __init__ ( self , loss_fn : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , detach_features : bool = False , f_hook : BaseHook = None , ** kwargs , ): \"\"\" Arguments: loss_fn: The classification loss function. If ```None```, it defaults to ```torch.nn.CrossEntropyLoss```. detach_features: Whether or not to detach the features, from which logits are computed. f_hook: The hook for computing logits. \"\"\" super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , torch . nn . CrossEntropyLoss , { \"reduction\" : \"none\" } ) self . hook = c_f . default ( f_hook , FeaturesAndLogitsHook , { \"domains\" : [ \"src\" ], \"detach_features\" : detach_features }, )","title":"CLossHook"},{"location":"hooks/classification/closs_hook/#pytorch_adapt.hooks.classification","text":"","title":"classification"},{"location":"hooks/classification/closs_hook/#pytorch_adapt.hooks.classification.CLossHook","text":"Computes a classification loss on the specified tensors. The default setting is to compute the cross entropy loss of the source domain logits.","title":"CLossHook"},{"location":"hooks/classification/closs_hook/#pytorch_adapt.hooks.classification.CLossHook.__init__","text":"Parameters: Name Type Description Default loss_fn Callable[[torch.Tensor, torch.Tensor], torch.Tensor] The classification loss function. If None , it defaults to torch.nn.CrossEntropyLoss . None detach_features bool Whether or not to detach the features, from which logits are computed. False f_hook BaseHook The hook for computing logits. None Source code in pytorch_adapt\\hooks\\classification.py def __init__ ( self , loss_fn : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , detach_features : bool = False , f_hook : BaseHook = None , ** kwargs , ): \"\"\" Arguments: loss_fn: The classification loss function. If ```None```, it defaults to ```torch.nn.CrossEntropyLoss```. detach_features: Whether or not to detach the features, from which logits are computed. f_hook: The hook for computing logits. \"\"\" super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , torch . nn . CrossEntropyLoss , { \"reduction\" : \"none\" } ) self . hook = c_f . default ( f_hook , FeaturesAndLogitsHook , { \"domains\" : [ \"src\" ], \"detach_features\" : detach_features }, )","title":"__init__()"},{"location":"hooks/classification/finetuner_hook/","text":"pytorch_adapt.hooks.classification \u00b6 FinetunerHook \u00b6 This is the same as ClassifierHook , but it freezes the generator model (\"G\").","title":"FinetunerHook"},{"location":"hooks/classification/finetuner_hook/#pytorch_adapt.hooks.classification","text":"","title":"classification"},{"location":"hooks/classification/finetuner_hook/#pytorch_adapt.hooks.classification.FinetunerHook","text":"This is the same as ClassifierHook , but it freezes the generator model (\"G\").","title":"FinetunerHook"},{"location":"hooks/classification/softmax_hook/","text":"pytorch_adapt.hooks.classification \u00b6 SoftmaxHook \u00b6 Applies torch.nn.Softmax(dim=1) to the specified inputs. Extends ApplyFnHook","title":"SoftmaxHook"},{"location":"hooks/classification/softmax_hook/#pytorch_adapt.hooks.classification","text":"","title":"classification"},{"location":"hooks/classification/softmax_hook/#pytorch_adapt.hooks.classification.SoftmaxHook","text":"Applies torch.nn.Softmax(dim=1) to the specified inputs. Extends ApplyFnHook","title":"SoftmaxHook"},{"location":"hooks/classification/softmax_locally_hook/","text":"pytorch_adapt.hooks.classification \u00b6 SoftmaxLocallyHook \u00b6 Applies torch.nn.Softmax(dim=1) to the specifieid inputs, which are overwritten, but only inside this hook. __init__ ( self , apply_to , * hooks , ** kwargs ) special \u00b6 Parameters: Name Type Description Default apply_to List[str] list of names of tensors that softmax will be applied to. required hooks BaseHook the hooks that will receive the softmaxed tensors. () Source code in pytorch_adapt\\hooks\\classification.py def __init__ ( self , apply_to : List [ str ], * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: apply_to: list of names of tensors that softmax will be applied to. hooks: the hooks that will receive the softmaxed tensors. \"\"\" super () . __init__ ( ** kwargs ) s_hook = SoftmaxHook ( apply_to = apply_to ) self . hook = OnlyNewOutputsHook ( ChainHook ( s_hook , * hooks , overwrite = True ))","title":"SoftmaxLocallyHook"},{"location":"hooks/classification/softmax_locally_hook/#pytorch_adapt.hooks.classification","text":"","title":"classification"},{"location":"hooks/classification/softmax_locally_hook/#pytorch_adapt.hooks.classification.SoftmaxLocallyHook","text":"Applies torch.nn.Softmax(dim=1) to the specifieid inputs, which are overwritten, but only inside this hook.","title":"SoftmaxLocallyHook"},{"location":"hooks/classification/softmax_locally_hook/#pytorch_adapt.hooks.classification.SoftmaxLocallyHook.__init__","text":"Parameters: Name Type Description Default apply_to List[str] list of names of tensors that softmax will be applied to. required hooks BaseHook the hooks that will receive the softmaxed tensors. () Source code in pytorch_adapt\\hooks\\classification.py def __init__ ( self , apply_to : List [ str ], * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: apply_to: list of names of tensors that softmax will be applied to. hooks: the hooks that will receive the softmaxed tensors. \"\"\" super () . __init__ ( ** kwargs ) s_hook = SoftmaxHook ( apply_to = apply_to ) self . hook = OnlyNewOutputsHook ( ChainHook ( s_hook , * hooks , overwrite = True ))","title":"__init__()"},{"location":"hooks/conditions/strong_d_hook/","text":"pytorch_adapt.hooks.conditions \u00b6 StrongDHook \u00b6 Returns True if the discriminator's accuracy is higher than some threshold. Extends BaseConditionHook __init__ ( self , threshold = 0.6 , ** kwargs ) special \u00b6 Parameters: Name Type Description Default threshold float The discriminator's accuracy must be higher than this threshold for the hook to return True . 0.6 Source code in pytorch_adapt\\hooks\\conditions.py def __init__ ( self , threshold : float = 0.6 , ** kwargs ): \"\"\" Arguments: threshold: The discriminator's accuracy must be higher than this threshold for the hook to return ```True```. \"\"\" super () . __init__ ( ** kwargs ) self . accuracy_fn = SufficientAccuracy ( threshold = threshold , to_probs_func = torch . nn . Sigmoid () ) self . hook = FeaturesChainHook ( FeaturesHook ( detach = True ), DLogitsHook ( detach = True ) )","title":"StrongDHook"},{"location":"hooks/conditions/strong_d_hook/#pytorch_adapt.hooks.conditions","text":"","title":"conditions"},{"location":"hooks/conditions/strong_d_hook/#pytorch_adapt.hooks.conditions.StrongDHook","text":"Returns True if the discriminator's accuracy is higher than some threshold. Extends BaseConditionHook","title":"StrongDHook"},{"location":"hooks/conditions/strong_d_hook/#pytorch_adapt.hooks.conditions.StrongDHook.__init__","text":"Parameters: Name Type Description Default threshold float The discriminator's accuracy must be higher than this threshold for the hook to return True . 0.6 Source code in pytorch_adapt\\hooks\\conditions.py def __init__ ( self , threshold : float = 0.6 , ** kwargs ): \"\"\" Arguments: threshold: The discriminator's accuracy must be higher than this threshold for the hook to return ```True```. \"\"\" super () . __init__ ( ** kwargs ) self . accuracy_fn = SufficientAccuracy ( threshold = threshold , to_probs_func = torch . nn . Sigmoid () ) self . hook = FeaturesChainHook ( FeaturesHook ( detach = True ), DLogitsHook ( detach = True ) )","title":"__init__()"},{"location":"hooks/domain/domain_loss_hook/","text":"pytorch_adapt.hooks.domain \u00b6 DomainLossHook \u00b6 Computes the loss of a discriminator's output with respect to domain labels. __init__ ( self , d_loss_fn = None , detach_features = False , reverse_labels = False , domains = None , f_hook = None , d_hook = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default d_loss_fn The loss applied to the discriminator's logits. If None it defaults to torch.nn.BCEWithLogitsLoss . None detach_features If True , the input to the discriminator will be detached first. False reverse_labels If True , the \"src\" and \"target\" domain labels will be swapped. False domains The domains to apply the loss to. If None it defaults to [\"src\", \"target\"] . None f_hook The hook for computing the input to the discriminator. None d_hook The hook for computing the discriminator logits. None Source code in pytorch_adapt\\hooks\\domain.py def __init__ ( self , d_loss_fn = None , detach_features = False , reverse_labels = False , domains = None , f_hook = None , d_hook = None , ** kwargs , ): \"\"\" Arguments: d_loss_fn: The loss applied to the discriminator's logits. If ```None``` it defaults to ```torch.nn.BCEWithLogitsLoss```. detach_features: If ```True```, the input to the discriminator will be detached first. reverse_labels: If ```True```, the ```\"src\"``` and ```\"target\"``` domain labels will be swapped. domains: The domains to apply the loss to. If ```None``` it defaults to ```[\"src\", \"target\"]```. f_hook: The hook for computing the input to the discriminator. d_hook: The hook for computing the discriminator logits. \"\"\" super () . __init__ ( ** kwargs ) self . d_loss_fn = c_f . default ( d_loss_fn , torch . nn . BCEWithLogitsLoss , { \"reduction\" : \"none\" } ) self . reverse_labels = reverse_labels self . domains = c_f . default ( domains , [ \"src\" , \"target\" ]) f_hook = c_f . default ( f_hook , FeaturesForDomainLossHook , { \"detach\" : detach_features , \"domains\" : domains }, ) d_hook = c_f . default ( d_hook , DLogitsHook , { \"domains\" : domains }) f_out = f_hook . last_hook_out_keys d_in = d_hook . in_keys d_hook . set_in_keys ( f_out ) self . check_fhook_dhook_keys ( f_hook , d_hook , detach_features ) self . hook = ChainHook ( f_hook , d_hook ) self . in_keys = self . hook . in_keys + [ \"src_domain\" , \"target_domain\" ]","title":"DomainLossHook"},{"location":"hooks/domain/domain_loss_hook/#pytorch_adapt.hooks.domain","text":"","title":"domain"},{"location":"hooks/domain/domain_loss_hook/#pytorch_adapt.hooks.domain.DomainLossHook","text":"Computes the loss of a discriminator's output with respect to domain labels.","title":"DomainLossHook"},{"location":"hooks/domain/domain_loss_hook/#pytorch_adapt.hooks.domain.DomainLossHook.__init__","text":"Parameters: Name Type Description Default d_loss_fn The loss applied to the discriminator's logits. If None it defaults to torch.nn.BCEWithLogitsLoss . None detach_features If True , the input to the discriminator will be detached first. False reverse_labels If True , the \"src\" and \"target\" domain labels will be swapped. False domains The domains to apply the loss to. If None it defaults to [\"src\", \"target\"] . None f_hook The hook for computing the input to the discriminator. None d_hook The hook for computing the discriminator logits. None Source code in pytorch_adapt\\hooks\\domain.py def __init__ ( self , d_loss_fn = None , detach_features = False , reverse_labels = False , domains = None , f_hook = None , d_hook = None , ** kwargs , ): \"\"\" Arguments: d_loss_fn: The loss applied to the discriminator's logits. If ```None``` it defaults to ```torch.nn.BCEWithLogitsLoss```. detach_features: If ```True```, the input to the discriminator will be detached first. reverse_labels: If ```True```, the ```\"src\"``` and ```\"target\"``` domain labels will be swapped. domains: The domains to apply the loss to. If ```None``` it defaults to ```[\"src\", \"target\"]```. f_hook: The hook for computing the input to the discriminator. d_hook: The hook for computing the discriminator logits. \"\"\" super () . __init__ ( ** kwargs ) self . d_loss_fn = c_f . default ( d_loss_fn , torch . nn . BCEWithLogitsLoss , { \"reduction\" : \"none\" } ) self . reverse_labels = reverse_labels self . domains = c_f . default ( domains , [ \"src\" , \"target\" ]) f_hook = c_f . default ( f_hook , FeaturesForDomainLossHook , { \"detach\" : detach_features , \"domains\" : domains }, ) d_hook = c_f . default ( d_hook , DLogitsHook , { \"domains\" : domains }) f_out = f_hook . last_hook_out_keys d_in = d_hook . in_keys d_hook . set_in_keys ( f_out ) self . check_fhook_dhook_keys ( f_hook , d_hook , detach_features ) self . hook = ChainHook ( f_hook , d_hook ) self . in_keys = self . hook . in_keys + [ \"src_domain\" , \"target_domain\" ]","title":"__init__()"},{"location":"hooks/domain/features_for_domain_loss_hook/","text":"pytorch_adapt.hooks.domain \u00b6 FeaturesForDomainLossHook \u00b6 A FeaturesChainHook that has options specific to DomainLossHook . __init__ ( self , f_hook = None , l_hook = None , use_logits = False , domains = None , detach = False , ** kwargs ) special \u00b6 Parameters: Name Type Description Default f_hook hook for computing features None l_hook hook for computing logits. This will be used only if use_logits is True . None use_logits If True , the logits hook is executed after the features hook. False domains the domains for which features will be computed. None detach If True , all outputs will be detached from the autograd graph. False Source code in pytorch_adapt\\hooks\\domain.py def __init__ ( self , f_hook = None , l_hook = None , use_logits = False , domains = None , detach = False , ** kwargs , ): \"\"\" Arguments: f_hook: hook for computing features l_hook: hook for computing logits. This will be used only if ```use_logits``` is ```True```. use_logits: If ```True```, the logits hook is executed after the features hook. domains: the domains for which features will be computed. detach: If ```True```, all outputs will be detached from the autograd graph. \"\"\" hooks = [ c_f . default ( f_hook , FeaturesHook ( detach = detach , domains = domains ), ) ] if use_logits : hooks . append ( c_f . default ( l_hook , LogitsHook ( detach = detach , domains = domains )) ) super () . __init__ ( * hooks , ** kwargs )","title":"FeaturesForDomainLossHook"},{"location":"hooks/domain/features_for_domain_loss_hook/#pytorch_adapt.hooks.domain","text":"","title":"domain"},{"location":"hooks/domain/features_for_domain_loss_hook/#pytorch_adapt.hooks.domain.FeaturesForDomainLossHook","text":"A FeaturesChainHook that has options specific to DomainLossHook .","title":"FeaturesForDomainLossHook"},{"location":"hooks/domain/features_for_domain_loss_hook/#pytorch_adapt.hooks.domain.FeaturesForDomainLossHook.__init__","text":"Parameters: Name Type Description Default f_hook hook for computing features None l_hook hook for computing logits. This will be used only if use_logits is True . None use_logits If True , the logits hook is executed after the features hook. False domains the domains for which features will be computed. None detach If True , all outputs will be detached from the autograd graph. False Source code in pytorch_adapt\\hooks\\domain.py def __init__ ( self , f_hook = None , l_hook = None , use_logits = False , domains = None , detach = False , ** kwargs , ): \"\"\" Arguments: f_hook: hook for computing features l_hook: hook for computing logits. This will be used only if ```use_logits``` is ```True```. use_logits: If ```True```, the logits hook is executed after the features hook. domains: the domains for which features will be computed. detach: If ```True```, all outputs will be detached from the autograd graph. \"\"\" hooks = [ c_f . default ( f_hook , FeaturesHook ( detach = detach , domains = domains ), ) ] if use_logits : hooks . append ( c_f . default ( l_hook , LogitsHook ( detach = detach , domains = domains )) ) super () . __init__ ( * hooks , ** kwargs )","title":"__init__()"},{"location":"hooks/features/base_features_hook/","text":"pytorch_adapt.hooks.features \u00b6 BaseFeaturesHook \u00b6 This hook: Checks to see if specific tensors are in the context Exits if the tensors are already in the context Otherwise computes those tensors using the appropriate inputs and models, and adds them to the context. __init__ ( self , model_name , in_suffixes = None , out_suffixes = None , domains = None , detach = False , ** kwargs ) special \u00b6 Parameters: Name Type Description Default model_name str The name of the model that will be used to compute any missing tensors. required in_suffixes List[str] The suffixes of the names of the inputs to the model. For example if: domains = [\"src\", \"target\"] in_suffixes = [\"_imgs_features\"] then the model will be given [\"src_imgs_features\", \"target_imgs_features\"] . None out_suffixes List[str] The suffixes of the names of the outputs of the model. Output suffixes are appended to the input name. For example, if domains = [\"src\", \"target\"] in_suffixes = [\"_imgs_features\"] out_suffixes = [\"_logits\"] then the output keys will be [\"src_imgs_features_logits\", \"target_imgs_features_logits\"] None domains List[str] The names of the domains to use. If None , this defaults to [\"src\", \"target\"] . None detach bool If True , then the output will be detached from the autograd graph. Any output that is detached will have \"_detached\" appended to its name in the context. False Source code in pytorch_adapt\\hooks\\features.py def __init__ ( self , model_name : str , in_suffixes : List [ str ] = None , out_suffixes : List [ str ] = None , domains : List [ str ] = None , detach : bool = False , ** kwargs , ): \"\"\" Arguments: model_name: The name of the model that will be used to compute any missing tensors. in_suffixes: The suffixes of the names of the inputs to the model. For example if: - ```domains = [\"src\", \"target\"]``` - ```in_suffixes = [\"_imgs_features\"]``` then the model will be given - ```[\"src_imgs_features\", \"target_imgs_features\"]```. out_suffixes: The suffixes of the names of the outputs of the model. Output suffixes are appended to the input name. For example, if - ```domains = [\"src\", \"target\"]``` - ```in_suffixes = [\"_imgs_features\"]``` - ```out_suffixes = [\"_logits\"]``` then the output keys will be - ```[\"src_imgs_features_logits\", \"target_imgs_features_logits\"]``` domains: The names of the domains to use. If ```None```, this defaults to ```[\"src\", \"target\"]```. detach: If ```True```, then the output will be detached from the autograd graph. Any output that is detached will have ```\"_detached\"``` appended to its name in the context. \"\"\" super () . __init__ ( ** kwargs ) self . model_name = model_name self . domains = c_f . default ( domains , [ \"src\" , \"target\" ]) self . init_detach_mode ( detach ) self . init_suffixes ( in_suffixes , out_suffixes )","title":"BaseFeaturesHook"},{"location":"hooks/features/base_features_hook/#pytorch_adapt.hooks.features","text":"","title":"features"},{"location":"hooks/features/base_features_hook/#pytorch_adapt.hooks.features.BaseFeaturesHook","text":"This hook: Checks to see if specific tensors are in the context Exits if the tensors are already in the context Otherwise computes those tensors using the appropriate inputs and models, and adds them to the context.","title":"BaseFeaturesHook"},{"location":"hooks/features/base_features_hook/#pytorch_adapt.hooks.features.BaseFeaturesHook.__init__","text":"Parameters: Name Type Description Default model_name str The name of the model that will be used to compute any missing tensors. required in_suffixes List[str] The suffixes of the names of the inputs to the model. For example if: domains = [\"src\", \"target\"] in_suffixes = [\"_imgs_features\"] then the model will be given [\"src_imgs_features\", \"target_imgs_features\"] . None out_suffixes List[str] The suffixes of the names of the outputs of the model. Output suffixes are appended to the input name. For example, if domains = [\"src\", \"target\"] in_suffixes = [\"_imgs_features\"] out_suffixes = [\"_logits\"] then the output keys will be [\"src_imgs_features_logits\", \"target_imgs_features_logits\"] None domains List[str] The names of the domains to use. If None , this defaults to [\"src\", \"target\"] . None detach bool If True , then the output will be detached from the autograd graph. Any output that is detached will have \"_detached\" appended to its name in the context. False Source code in pytorch_adapt\\hooks\\features.py def __init__ ( self , model_name : str , in_suffixes : List [ str ] = None , out_suffixes : List [ str ] = None , domains : List [ str ] = None , detach : bool = False , ** kwargs , ): \"\"\" Arguments: model_name: The name of the model that will be used to compute any missing tensors. in_suffixes: The suffixes of the names of the inputs to the model. For example if: - ```domains = [\"src\", \"target\"]``` - ```in_suffixes = [\"_imgs_features\"]``` then the model will be given - ```[\"src_imgs_features\", \"target_imgs_features\"]```. out_suffixes: The suffixes of the names of the outputs of the model. Output suffixes are appended to the input name. For example, if - ```domains = [\"src\", \"target\"]``` - ```in_suffixes = [\"_imgs_features\"]``` - ```out_suffixes = [\"_logits\"]``` then the output keys will be - ```[\"src_imgs_features_logits\", \"target_imgs_features_logits\"]``` domains: The names of the domains to use. If ```None```, this defaults to ```[\"src\", \"target\"]```. detach: If ```True```, then the output will be detached from the autograd graph. Any output that is detached will have ```\"_detached\"``` appended to its name in the context. \"\"\" super () . __init__ ( ** kwargs ) self . model_name = model_name self . domains = c_f . default ( domains , [ \"src\" , \"target\" ]) self . init_detach_mode ( detach ) self . init_suffixes ( in_suffixes , out_suffixes )","title":"__init__()"},{"location":"hooks/features/combined_features_hook/","text":"pytorch_adapt.hooks.features \u00b6 CombinedFeaturesHook \u00b6 Default input/output context names: Model: \"feature_combiner\" Inputs: [\"src_imgs_features\", \"src_imgs_features_logits\", \"target_imgs_features\", \"target_imgs_features_logits\"] Outputs: [\"src_imgs_features_AND_src_imgs_features_logits_combined\", \"target_imgs_features_AND_target_imgs_features_logits_combined\"]","title":"CombinedFeaturesHook"},{"location":"hooks/features/combined_features_hook/#pytorch_adapt.hooks.features","text":"","title":"features"},{"location":"hooks/features/combined_features_hook/#pytorch_adapt.hooks.features.CombinedFeaturesHook","text":"Default input/output context names: Model: \"feature_combiner\" Inputs: [\"src_imgs_features\", \"src_imgs_features_logits\", \"target_imgs_features\", \"target_imgs_features_logits\"] Outputs: [\"src_imgs_features_AND_src_imgs_features_logits_combined\", \"target_imgs_features_AND_target_imgs_features_logits_combined\"]","title":"CombinedFeaturesHook"},{"location":"hooks/features/dlogits_hook/","text":"pytorch_adapt.hooks.features \u00b6 DLogitsHook \u00b6 Default input/output context names: Model: \"D\" Inputs: [\"src_imgs_features\", \"target_imgs_features\"] Outputs: [\"src_imgs_features_dlogits\", \"target_imgs_features_dlogits\"]","title":"DLogitsHook"},{"location":"hooks/features/dlogits_hook/#pytorch_adapt.hooks.features","text":"","title":"features"},{"location":"hooks/features/dlogits_hook/#pytorch_adapt.hooks.features.DLogitsHook","text":"Default input/output context names: Model: \"D\" Inputs: [\"src_imgs_features\", \"target_imgs_features\"] Outputs: [\"src_imgs_features_dlogits\", \"target_imgs_features_dlogits\"]","title":"DLogitsHook"},{"location":"hooks/features/features_and_logits_hook/","text":"pytorch_adapt.hooks.features \u00b6 FeaturesAndLogitsHook \u00b6 Chains together FeaturesHook and LogitsHook . __init__ ( self , domains = None , detach_features = False , detach_logits = False , other_hooks = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default domains List[str] The domains used by both the features and logits hooks. If None , it defaults to [\"src\", \"target\"] None detach_features bool If True , returns features that are detached from the autograd graph. False detach_logits bool If True , returns logits that are detached from the autograd graph. False other_hooks List[pytorch_adapt.hooks.base.BaseHook] A list of hooks that will be called after the features and logits hooks. None Source code in pytorch_adapt\\hooks\\features.py def __init__ ( self , domains : List [ str ] = None , detach_features : bool = False , detach_logits : bool = False , other_hooks : List [ BaseHook ] = None , ** kwargs , ): \"\"\" Arguments: domains: The domains used by both the features and logits hooks. If ```None```, it defaults to ```[\"src\", \"target\"]``` detach_features: If ```True```, returns features that are detached from the autograd graph. detach_logits: If ```True```, returns logits that are detached from the autograd graph. other_hooks: A list of hooks that will be called after the features and logits hooks. \"\"\" features_hook = FeaturesHook ( detach = detach_features , domains = domains ) logits_hook = LogitsHook ( detach = detach_logits , domains = domains ) other_hooks = c_f . default ( other_hooks , []) super () . __init__ ( features_hook , logits_hook , * other_hooks , ** kwargs )","title":"FeaturesAndLogitsHook"},{"location":"hooks/features/features_and_logits_hook/#pytorch_adapt.hooks.features","text":"","title":"features"},{"location":"hooks/features/features_and_logits_hook/#pytorch_adapt.hooks.features.FeaturesAndLogitsHook","text":"Chains together FeaturesHook and LogitsHook .","title":"FeaturesAndLogitsHook"},{"location":"hooks/features/features_and_logits_hook/#pytorch_adapt.hooks.features.FeaturesAndLogitsHook.__init__","text":"Parameters: Name Type Description Default domains List[str] The domains used by both the features and logits hooks. If None , it defaults to [\"src\", \"target\"] None detach_features bool If True , returns features that are detached from the autograd graph. False detach_logits bool If True , returns logits that are detached from the autograd graph. False other_hooks List[pytorch_adapt.hooks.base.BaseHook] A list of hooks that will be called after the features and logits hooks. None Source code in pytorch_adapt\\hooks\\features.py def __init__ ( self , domains : List [ str ] = None , detach_features : bool = False , detach_logits : bool = False , other_hooks : List [ BaseHook ] = None , ** kwargs , ): \"\"\" Arguments: domains: The domains used by both the features and logits hooks. If ```None```, it defaults to ```[\"src\", \"target\"]``` detach_features: If ```True```, returns features that are detached from the autograd graph. detach_logits: If ```True```, returns logits that are detached from the autograd graph. other_hooks: A list of hooks that will be called after the features and logits hooks. \"\"\" features_hook = FeaturesHook ( detach = detach_features , domains = domains ) logits_hook = LogitsHook ( detach = detach_logits , domains = domains ) other_hooks = c_f . default ( other_hooks , []) super () . __init__ ( features_hook , logits_hook , * other_hooks , ** kwargs )","title":"__init__()"},{"location":"hooks/features/features_chain_hook/","text":"pytorch_adapt.hooks.features \u00b6 FeaturesChainHook \u00b6 A special ChainHook for features hooks. It sets each sub-hook's in_keys using the previous sub-hook's out_keys .","title":"FeaturesChainHook"},{"location":"hooks/features/features_chain_hook/#pytorch_adapt.hooks.features","text":"","title":"features"},{"location":"hooks/features/features_chain_hook/#pytorch_adapt.hooks.features.FeaturesChainHook","text":"A special ChainHook for features hooks. It sets each sub-hook's in_keys using the previous sub-hook's out_keys .","title":"FeaturesChainHook"},{"location":"hooks/features/features_hook/","text":"pytorch_adapt.hooks.features \u00b6 FeaturesHook \u00b6 Default input/output context names: Model: \"G\" Inputs: [\"src_imgs\", \"target_imgs\"] Outputs: [\"src_imgs_features\", \"target_imgs_features\"]","title":"FeaturesHook"},{"location":"hooks/features/features_hook/#pytorch_adapt.hooks.features","text":"","title":"features"},{"location":"hooks/features/features_hook/#pytorch_adapt.hooks.features.FeaturesHook","text":"Default input/output context names: Model: \"G\" Inputs: [\"src_imgs\", \"target_imgs\"] Outputs: [\"src_imgs_features\", \"target_imgs_features\"]","title":"FeaturesHook"},{"location":"hooks/features/features_with_grad_and_detached_hook/","text":"pytorch_adapt.hooks.features \u00b6 FeaturesWithGradAndDetachedHook \u00b6 Default input/output context names: Model: \"G\" Inputs: [\"src_imgs\", \"target_imgs\"] Outputs: [\"src_imgs_features\", \"target_imgs_features\", \"src_imgs_features_detached\", \"target_imgs_features_detached\"]","title":"FeaturesWithGradAndDetachedHook"},{"location":"hooks/features/features_with_grad_and_detached_hook/#pytorch_adapt.hooks.features","text":"","title":"features"},{"location":"hooks/features/features_with_grad_and_detached_hook/#pytorch_adapt.hooks.features.FeaturesWithGradAndDetachedHook","text":"Default input/output context names: Model: \"G\" Inputs: [\"src_imgs\", \"target_imgs\"] Outputs: [\"src_imgs_features\", \"target_imgs_features\", \"src_imgs_features_detached\", \"target_imgs_features_detached\"]","title":"FeaturesWithGradAndDetachedHook"},{"location":"hooks/features/frozen_model_hook/","text":"pytorch_adapt.hooks.features \u00b6 FrozenModelHook \u00b6 Sets model to eval() mode, and does all computations with gradients turned off. __init__ ( self , hook , model_name , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The wrapped hook which computes all losses and outputs. required model_name str The name of the model that will be set to eval() mode. required Source code in pytorch_adapt\\hooks\\features.py def __init__ ( self , hook : BaseHook , model_name : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook which computes all losses and outputs. model_name: The name of the model that will be set to eval() mode. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . model_name = model_name","title":"FrozenModelHook"},{"location":"hooks/features/frozen_model_hook/#pytorch_adapt.hooks.features","text":"","title":"features"},{"location":"hooks/features/frozen_model_hook/#pytorch_adapt.hooks.features.FrozenModelHook","text":"Sets model to eval() mode, and does all computations with gradients turned off.","title":"FrozenModelHook"},{"location":"hooks/features/frozen_model_hook/#pytorch_adapt.hooks.features.FrozenModelHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The wrapped hook which computes all losses and outputs. required model_name str The name of the model that will be set to eval() mode. required Source code in pytorch_adapt\\hooks\\features.py def __init__ ( self , hook : BaseHook , model_name : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook which computes all losses and outputs. model_name: The name of the model that will be set to eval() mode. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . model_name = model_name","title":"__init__()"},{"location":"hooks/features/logits_hook/","text":"pytorch_adapt.hooks.features \u00b6 LogitsHook \u00b6 Default input/output context names: Model: \"C\" Inputs: [\"src_imgs_features\", \"target_imgs_features\"] Outputs: [\"src_imgs_features_logits\", \"target_imgs_features_logits\"]","title":"LogitsHook"},{"location":"hooks/features/logits_hook/#pytorch_adapt.hooks.features","text":"","title":"features"},{"location":"hooks/features/logits_hook/#pytorch_adapt.hooks.features.LogitsHook","text":"Default input/output context names: Model: \"C\" Inputs: [\"src_imgs_features\", \"target_imgs_features\"] Outputs: [\"src_imgs_features_logits\", \"target_imgs_features_logits\"]","title":"LogitsHook"},{"location":"hooks/reducers/base_reducer/","text":"pytorch_adapt.hooks.reducers \u00b6 BaseReducer \u00b6 Converts an unreduced loss tensor into a single number. In other words, if the loss tensor has shape (N,) , the reducer converts it to shape (1,) . __init__ ( self , apply_to = None , default_reducer = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default apply_to List[str] list of loss names to apply reduction to None default_reducer BaseReducer a reducer to use for losses that are not already reduced and are also not specified in apply_to . If None , then no action is taken. None Source code in pytorch_adapt\\hooks\\reducers.py def __init__ ( self , apply_to : List [ str ] = None , default_reducer : \"BaseReducer\" = None , ** kwargs , ): \"\"\" Arguments: apply_to: list of loss names to apply reduction to default_reducer: a reducer to use for losses that are not already reduced and are also not specified in ```apply_to```. If ```None```, then no action is taken. \"\"\" super () . __init__ ( ** kwargs ) self . apply_to = apply_to self . default_reducer = default_reducer self . curr_loss_keys = []","title":"BaseReducer"},{"location":"hooks/reducers/base_reducer/#pytorch_adapt.hooks.reducers","text":"","title":"reducers"},{"location":"hooks/reducers/base_reducer/#pytorch_adapt.hooks.reducers.BaseReducer","text":"Converts an unreduced loss tensor into a single number. In other words, if the loss tensor has shape (N,) , the reducer converts it to shape (1,) .","title":"BaseReducer"},{"location":"hooks/reducers/base_reducer/#pytorch_adapt.hooks.reducers.BaseReducer.__init__","text":"Parameters: Name Type Description Default apply_to List[str] list of loss names to apply reduction to None default_reducer BaseReducer a reducer to use for losses that are not already reduced and are also not specified in apply_to . If None , then no action is taken. None Source code in pytorch_adapt\\hooks\\reducers.py def __init__ ( self , apply_to : List [ str ] = None , default_reducer : \"BaseReducer\" = None , ** kwargs , ): \"\"\" Arguments: apply_to: list of loss names to apply reduction to default_reducer: a reducer to use for losses that are not already reduced and are also not specified in ```apply_to```. If ```None```, then no action is taken. \"\"\" super () . __init__ ( ** kwargs ) self . apply_to = apply_to self . default_reducer = default_reducer self . curr_loss_keys = []","title":"__init__()"},{"location":"hooks/reducers/entropy_reducer/","text":"pytorch_adapt.hooks.reducers \u00b6 EntropyReducer \u00b6 Implementation of \"entropy conditioning\" from Conditional Adversarial Domain Adaptation . It weights loss elements using EntropyWeights . The entropy weights are derived from classifier logits. __init__ ( self , f_hook = None , domains = None , entropy_weights_fn = None , detach_weights = True , ** kwargs ) special \u00b6 Parameters: Name Type Description Default f_hook BaseHook the hook for computing logits from which entropy weights are derived None domains List[str] the domains that f_hook should compute for None entropy_weights_fn Callable[[torch.Tensor], torch.Tensor] the function for computing the weights that will be multiplied with the unreduced losses. None detach_weights bool If True , the entropy weights are detached from the autograd graph True Source code in pytorch_adapt\\hooks\\reducers.py def __init__ ( self , f_hook : BaseHook = None , domains : List [ str ] = None , entropy_weights_fn : Callable [[ torch . Tensor ], torch . Tensor ] = None , detach_weights : bool = True , ** kwargs , ): \"\"\" Arguments: f_hook: the hook for computing logits from which entropy weights are derived domains: the domains that ```f_hook``` should compute for entropy_weights_fn: the function for computing the weights that will be multiplied with the unreduced losses. detach_weights: If ```True```, the entropy weights are detached from the autograd graph \"\"\" super () . __init__ ( ** kwargs ) src_regex = \"^ {0} _|_ {0} $|_ {0} _|^ {0} $\" . format ( \"src\" ) target_regex = \"^ {0} _|_ {0} $|_ {0} _|^ {0} $\" . format ( \"target\" ) self . src_regex = re . compile ( src_regex ) self . target_regex = re . compile ( target_regex ) self . entropy_weights_fn = c_f . default ( entropy_weights_fn , EntropyWeights , {}) self . f_hook = c_f . default ( f_hook , FeaturesAndLogitsHook , { \"detach_features\" : detach_weights , \"detach_logits\" : detach_weights , \"domains\" : domains , }, ) self . context = torch . no_grad () if detach_weights else nullcontext ()","title":"EntropyReducer"},{"location":"hooks/reducers/entropy_reducer/#pytorch_adapt.hooks.reducers","text":"","title":"reducers"},{"location":"hooks/reducers/entropy_reducer/#pytorch_adapt.hooks.reducers.EntropyReducer","text":"Implementation of \"entropy conditioning\" from Conditional Adversarial Domain Adaptation . It weights loss elements using EntropyWeights . The entropy weights are derived from classifier logits.","title":"EntropyReducer"},{"location":"hooks/reducers/entropy_reducer/#pytorch_adapt.hooks.reducers.EntropyReducer.__init__","text":"Parameters: Name Type Description Default f_hook BaseHook the hook for computing logits from which entropy weights are derived None domains List[str] the domains that f_hook should compute for None entropy_weights_fn Callable[[torch.Tensor], torch.Tensor] the function for computing the weights that will be multiplied with the unreduced losses. None detach_weights bool If True , the entropy weights are detached from the autograd graph True Source code in pytorch_adapt\\hooks\\reducers.py def __init__ ( self , f_hook : BaseHook = None , domains : List [ str ] = None , entropy_weights_fn : Callable [[ torch . Tensor ], torch . Tensor ] = None , detach_weights : bool = True , ** kwargs , ): \"\"\" Arguments: f_hook: the hook for computing logits from which entropy weights are derived domains: the domains that ```f_hook``` should compute for entropy_weights_fn: the function for computing the weights that will be multiplied with the unreduced losses. detach_weights: If ```True```, the entropy weights are detached from the autograd graph \"\"\" super () . __init__ ( ** kwargs ) src_regex = \"^ {0} _|_ {0} $|_ {0} _|^ {0} $\" . format ( \"src\" ) target_regex = \"^ {0} _|_ {0} $|_ {0} _|^ {0} $\" . format ( \"target\" ) self . src_regex = re . compile ( src_regex ) self . target_regex = re . compile ( target_regex ) self . entropy_weights_fn = c_f . default ( entropy_weights_fn , EntropyWeights , {}) self . f_hook = c_f . default ( f_hook , FeaturesAndLogitsHook , { \"detach_features\" : detach_weights , \"detach_logits\" : detach_weights , \"domains\" : domains , }, ) self . context = torch . no_grad () if detach_weights else nullcontext ()","title":"__init__()"},{"location":"hooks/reducers/mean_reducer/","text":"pytorch_adapt.hooks.reducers \u00b6 MeanReducer \u00b6 Reduces loss elements by taking the mean.","title":"MeanReducer"},{"location":"hooks/reducers/mean_reducer/#pytorch_adapt.hooks.reducers","text":"","title":"reducers"},{"location":"hooks/reducers/mean_reducer/#pytorch_adapt.hooks.reducers.MeanReducer","text":"Reduces loss elements by taking the mean.","title":"MeanReducer"},{"location":"hooks/utils/apply_fn_hook/","text":"pytorch_adapt.hooks.utils \u00b6 ApplyFnHook \u00b6 Applies a function to specific values of the context. __init__ ( self , fn , apply_to , is_loss = False , ** kwargs ) special \u00b6 Parameters: Name Type Description Default fn Callable The function that will be applied to the inputs. required apply_to List[str] fn will be applied to inputs[k] for k in apply_to required is_loss bool If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. False Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , fn : Callable , apply_to : List [ str ], is_loss : bool = False , ** kwargs ): \"\"\" Arguments: fn: The function that will be applied to the inputs. apply_to: fn will be applied to ```inputs[k]``` for k in apply_to is_loss: If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. \"\"\" super () . __init__ ( ** kwargs ) self . fn = fn self . apply_to = apply_to self . is_loss = is_loss","title":"ApplyFnHook"},{"location":"hooks/utils/apply_fn_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/apply_fn_hook/#pytorch_adapt.hooks.utils.ApplyFnHook","text":"Applies a function to specific values of the context.","title":"ApplyFnHook"},{"location":"hooks/utils/apply_fn_hook/#pytorch_adapt.hooks.utils.ApplyFnHook.__init__","text":"Parameters: Name Type Description Default fn Callable The function that will be applied to the inputs. required apply_to List[str] fn will be applied to inputs[k] for k in apply_to required is_loss bool If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. False Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , fn : Callable , apply_to : List [ str ], is_loss : bool = False , ** kwargs ): \"\"\" Arguments: fn: The function that will be applied to the inputs. apply_to: fn will be applied to ```inputs[k]``` for k in apply_to is_loss: If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. \"\"\" super () . __init__ ( ** kwargs ) self . fn = fn self . apply_to = apply_to self . is_loss = is_loss","title":"__init__()"},{"location":"hooks/utils/assert_hook/","text":"pytorch_adapt.hooks.utils \u00b6 AssertHook \u00b6 Asserts that the output keys of a hook match a specified regex string __init__ ( self , hook , allowed , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The wrapped hook required allowed str The output dictionary of hook must have keys that match the allowed regex. required Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , hook : BaseHook , allowed : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook allowed: The output dictionary of ```hook``` must have keys that match the ```allowed``` regex. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook if not isinstance ( allowed , str ): raise TypeError ( \"allowed must be a str\" ) self . allowed = allowed","title":"AssertHook"},{"location":"hooks/utils/assert_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/assert_hook/#pytorch_adapt.hooks.utils.AssertHook","text":"Asserts that the output keys of a hook match a specified regex string","title":"AssertHook"},{"location":"hooks/utils/assert_hook/#pytorch_adapt.hooks.utils.AssertHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The wrapped hook required allowed str The output dictionary of hook must have keys that match the allowed regex. required Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , hook : BaseHook , allowed : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook allowed: The output dictionary of ```hook``` must have keys that match the ```allowed``` regex. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook if not isinstance ( allowed , str ): raise TypeError ( \"allowed must be a str\" ) self . allowed = allowed","title":"__init__()"},{"location":"hooks/utils/chain_hook/","text":"pytorch_adapt.hooks.utils \u00b6 ChainHook \u00b6 Calls multiple hooks sequentially. The Nth hook receives the context accumulated through hooks 0 to N-1. __init__ ( self , * hooks , * , conditions = None , alts = None , overwrite = False , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially. () conditions List[pytorch_adapt.hooks.base.BaseConditionHook] an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. None alts List[pytorch_adapt.hooks.base.BaseHook] an optional list of hooks that will be executed when the corresponding condition hook returns False None overwrite Union[bool, List[int]] If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. False Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , * hooks : BaseHook , conditions : List [ BaseConditionHook ] = None , alts : List [ BaseHook ] = None , overwrite : Union [ bool , List [ int ]] = False , ** kwargs , ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially. conditions: an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. alts: an optional list of hooks that will be executed when the corresponding condition hook returns False overwrite: If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . conditions = c_f . default ( conditions , [ TrueHook () for _ in range ( len ( hooks ))] ) self . alts = c_f . default ( alts , [ ZeroLossHook ( h . loss_keys , h . out_keys ) for h in self . hooks ] ) self . check_alt_keys_match_hook_keys () if not isinstance ( overwrite , ( list , bool )): raise TypeError ( \"overwrite must be a list or bool\" ) self . overwrite = overwrite self . in_keys = self . hooks [ 0 ] . in_keys","title":"ChainHook"},{"location":"hooks/utils/chain_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/chain_hook/#pytorch_adapt.hooks.utils.ChainHook","text":"Calls multiple hooks sequentially. The Nth hook receives the context accumulated through hooks 0 to N-1.","title":"ChainHook"},{"location":"hooks/utils/chain_hook/#pytorch_adapt.hooks.utils.ChainHook.__init__","text":"Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially. () conditions List[pytorch_adapt.hooks.base.BaseConditionHook] an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. None alts List[pytorch_adapt.hooks.base.BaseHook] an optional list of hooks that will be executed when the corresponding condition hook returns False None overwrite Union[bool, List[int]] If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. False Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , * hooks : BaseHook , conditions : List [ BaseConditionHook ] = None , alts : List [ BaseHook ] = None , overwrite : Union [ bool , List [ int ]] = False , ** kwargs , ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially. conditions: an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. alts: an optional list of hooks that will be executed when the corresponding condition hook returns False overwrite: If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . conditions = c_f . default ( conditions , [ TrueHook () for _ in range ( len ( hooks ))] ) self . alts = c_f . default ( alts , [ ZeroLossHook ( h . loss_keys , h . out_keys ) for h in self . hooks ] ) self . check_alt_keys_match_hook_keys () if not isinstance ( overwrite , ( list , bool )): raise TypeError ( \"overwrite must be a list or bool\" ) self . overwrite = overwrite self . in_keys = self . hooks [ 0 ] . in_keys","title":"__init__()"},{"location":"hooks/utils/empty_hook/","text":"pytorch_adapt.hooks.utils \u00b6 EmptyHook \u00b6 Returns two empty dictionaries.","title":"EmptyHook"},{"location":"hooks/utils/empty_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/empty_hook/#pytorch_adapt.hooks.utils.EmptyHook","text":"Returns two empty dictionaries.","title":"EmptyHook"},{"location":"hooks/utils/false_hook/","text":"pytorch_adapt.hooks.utils \u00b6 FalseHook \u00b6 Returns False","title":"FalseHook"},{"location":"hooks/utils/false_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/false_hook/#pytorch_adapt.hooks.utils.FalseHook","text":"Returns False","title":"FalseHook"},{"location":"hooks/utils/multiplier_hook/","text":"pytorch_adapt.hooks.utils \u00b6 MultiplierHook \u00b6 Multiplies every loss by a scalar __init__ ( self , hook , m , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The losses of this hook will be multiplied by m required m float The scalar required Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , hook : BaseHook , m : float , ** kwargs ): \"\"\" Arguments: hook: The losses of this hook will be multiplied by ```m``` m: The scalar \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . m = m","title":"MultiplierHook"},{"location":"hooks/utils/multiplier_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/multiplier_hook/#pytorch_adapt.hooks.utils.MultiplierHook","text":"Multiplies every loss by a scalar","title":"MultiplierHook"},{"location":"hooks/utils/multiplier_hook/#pytorch_adapt.hooks.utils.MultiplierHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The losses of this hook will be multiplied by m required m float The scalar required Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , hook : BaseHook , m : float , ** kwargs ): \"\"\" Arguments: hook: The losses of this hook will be multiplied by ```m``` m: The scalar \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . m = m","title":"__init__()"},{"location":"hooks/utils/not_hook/","text":"pytorch_adapt.hooks.utils \u00b6 NotHook \u00b6 Returns the boolean negation of the wrapped hook. __init__ ( self , hook , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseConditionHook The condition hook that will be negated. required Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , hook : BaseConditionHook , ** kwargs ): \"\"\" Arguments: hook: The condition hook that will be negated. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook","title":"NotHook"},{"location":"hooks/utils/not_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/not_hook/#pytorch_adapt.hooks.utils.NotHook","text":"Returns the boolean negation of the wrapped hook.","title":"NotHook"},{"location":"hooks/utils/not_hook/#pytorch_adapt.hooks.utils.NotHook.__init__","text":"Parameters: Name Type Description Default hook BaseConditionHook The condition hook that will be negated. required Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , hook : BaseConditionHook , ** kwargs ): \"\"\" Arguments: hook: The condition hook that will be negated. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook","title":"__init__()"},{"location":"hooks/utils/only_new_outputs_hook/","text":"pytorch_adapt.hooks.utils \u00b6 OnlyNewOutputsHook \u00b6 Returns only outputs that are not present in the input context. You should use this if you want to change the value of a key passed to self.hook, but not propagate that change to the outside. __init__ ( self , hook , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The hook inside which changes to the context will be allowed. required Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , hook : BaseHook , ** kwargs ): \"\"\" Arguments: hook: The hook inside which changes to the context will be allowed. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook","title":"OnlyNewOutputsHook"},{"location":"hooks/utils/only_new_outputs_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/only_new_outputs_hook/#pytorch_adapt.hooks.utils.OnlyNewOutputsHook","text":"Returns only outputs that are not present in the input context. You should use this if you want to change the value of a key passed to self.hook, but not propagate that change to the outside.","title":"OnlyNewOutputsHook"},{"location":"hooks/utils/only_new_outputs_hook/#pytorch_adapt.hooks.utils.OnlyNewOutputsHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The hook inside which changes to the context will be allowed. required Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , hook : BaseHook , ** kwargs ): \"\"\" Arguments: hook: The hook inside which changes to the context will be allowed. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook","title":"__init__()"},{"location":"hooks/utils/parallel_hook/","text":"pytorch_adapt.hooks.utils \u00b6 ParallelHook \u00b6 Calls multiple hooks while keeping contexts separate. The Nth hook receives the same context as hooks 0 to N-1. All the output contexts are merged at the end. __init__ ( self , * hooks , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. () Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . in_keys = c_f . join_lists ([ h . in_keys for h in self . hooks ])","title":"ParallelHook"},{"location":"hooks/utils/parallel_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/parallel_hook/#pytorch_adapt.hooks.utils.ParallelHook","text":"Calls multiple hooks while keeping contexts separate. The Nth hook receives the same context as hooks 0 to N-1. All the output contexts are merged at the end.","title":"ParallelHook"},{"location":"hooks/utils/parallel_hook/#pytorch_adapt.hooks.utils.ParallelHook.__init__","text":"Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. () Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . in_keys = c_f . join_lists ([ h . in_keys for h in self . hooks ])","title":"__init__()"},{"location":"hooks/utils/repeat_hook/","text":"pytorch_adapt.hooks.utils \u00b6 RepeatHook \u00b6 Executes the wrapped hook n times. __init__ ( self , hook , n , keep_only_last = False , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The hook that will be executed n times required n int The number of times the hook will be executed. required keep_only_last bool If False , the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If True , then only the (losses, outputs) of the final execution will be kept. False Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , hook : BaseHook , n : int , keep_only_last : bool = False , ** kwargs ): \"\"\" Arguments: hook: The hook that will be executed ```n``` times n: The number of times the hook will be executed. keep_only_last: If ```False```, the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If ```True```, then only the (losses, outputs) of the final execution will be kept. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . n = n self . keep_only_last = keep_only_last","title":"RepeatHook"},{"location":"hooks/utils/repeat_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/repeat_hook/#pytorch_adapt.hooks.utils.RepeatHook","text":"Executes the wrapped hook n times.","title":"RepeatHook"},{"location":"hooks/utils/repeat_hook/#pytorch_adapt.hooks.utils.RepeatHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The hook that will be executed n times required n int The number of times the hook will be executed. required keep_only_last bool If False , the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If True , then only the (losses, outputs) of the final execution will be kept. False Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , hook : BaseHook , n : int , keep_only_last : bool = False , ** kwargs ): \"\"\" Arguments: hook: The hook that will be executed ```n``` times n: The number of times the hook will be executed. keep_only_last: If ```False```, the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If ```True```, then only the (losses, outputs) of the final execution will be kept. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . n = n self . keep_only_last = keep_only_last","title":"__init__()"},{"location":"hooks/utils/true_hook/","text":"pytorch_adapt.hooks.utils \u00b6 TrueHook \u00b6 Returns True","title":"TrueHook"},{"location":"hooks/utils/true_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/true_hook/#pytorch_adapt.hooks.utils.TrueHook","text":"Returns True","title":"TrueHook"},{"location":"hooks/utils/zero_loss_hook/","text":"pytorch_adapt.hooks.utils \u00b6 ZeroLossHook \u00b6 Returns only 0 losses and None outputs. __init__ ( self , loss_names , out_names , ** kwargs ) special \u00b6 Parameters: Name Type Description Default loss_names List[str] The keys of the loss dictionary which will have tensor(0.) as its values. required out_names List[str] The keys of the output dictionary which will have None as its values. required Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , loss_names : List [ str ], out_names : List [ str ], ** kwargs ): \"\"\" Arguments: loss_names: The keys of the loss dictionary which will have ```tensor(0.)``` as its values. out_names: The keys of the output dictionary which will have ```None``` as its values. \"\"\" super () . __init__ ( ** kwargs ) self . loss_names = loss_names self . out_names = out_names","title":"ZeroLossHook"},{"location":"hooks/utils/zero_loss_hook/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/zero_loss_hook/#pytorch_adapt.hooks.utils.ZeroLossHook","text":"Returns only 0 losses and None outputs.","title":"ZeroLossHook"},{"location":"hooks/utils/zero_loss_hook/#pytorch_adapt.hooks.utils.ZeroLossHook.__init__","text":"Parameters: Name Type Description Default loss_names List[str] The keys of the loss dictionary which will have tensor(0.) as its values. required out_names List[str] The keys of the output dictionary which will have None as its values. required Source code in pytorch_adapt\\hooks\\utils.py def __init__ ( self , loss_names : List [ str ], out_names : List [ str ], ** kwargs ): \"\"\" Arguments: loss_names: The keys of the loss dictionary which will have ```tensor(0.)``` as its values. out_names: The keys of the output dictionary which will have ```None``` as its values. \"\"\" super () . __init__ ( ** kwargs ) self . loss_names = loss_names self . out_names = out_names","title":"__init__()"},{"location":"layers/","text":"","title":"Layers"},{"location":"layers/abs_loss/","text":"pytorch_adapt.layers.abs_loss \u00b6 AbsLoss \u00b6 The mean absolute value.","title":"AbsLoss"},{"location":"layers/abs_loss/#pytorch_adapt.layers.abs_loss","text":"","title":"abs_loss"},{"location":"layers/abs_loss/#pytorch_adapt.layers.abs_loss.AbsLoss","text":"The mean absolute value.","title":"AbsLoss"},{"location":"layers/adaptive_feature_norm/","text":"pytorch_adapt.layers.adaptive_feature_norm \u00b6 AdaptiveFeatureNorm \u00b6 Implementation of the loss in Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation . Encourages features to gradually have larger and larger L2 norms. __init__ ( self , step_size = 1 ) special \u00b6 Parameters: Name Type Description Default step_size float The desired increase in L2 norm at each iteration. Note that the loss will always be equal to step_size because the goal is always to make the L2 norm step_size larger than whatever the current L2 norm is. 1 Source code in pytorch_adapt\\layers\\adaptive_feature_norm.py def __init__ ( self , step_size : float = 1 ): \"\"\" Arguments: step_size: The desired increase in L2 norm at each iteration. Note that the loss will always be equal to ```step_size``` because the goal is always to make the L2 norm ```step_size``` larger than whatever the current L2 norm is. \"\"\" super () . __init__ () self . step_size = step_size L2PreservedDropout \u00b6 Implementation of the dropout layer described in Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation . Regular dropout preserves the L1 norm of features, whereas this layer preserves the L2 norm. __init__ ( self , p = 0.5 , inplace = False ) special \u00b6 Parameters: Name Type Description Default p float probability of an element to be zeroed 0.5 inplace bool if set to True, will do this operation in-place False Source code in pytorch_adapt\\layers\\adaptive_feature_norm.py def __init__ ( self , p : float = 0.5 , inplace : bool = False ): \"\"\" Arguments: p: probability of an element to be zeroed inplace: if set to True, will do this operation in-place \"\"\" super () . __init__ () self . dropout = torch . nn . Dropout ( p = p , inplace = inplace ) self . scale = math . sqrt ( 1 - p )","title":"AdaptiveFeatureNorm"},{"location":"layers/adaptive_feature_norm/#pytorch_adapt.layers.adaptive_feature_norm","text":"","title":"adaptive_feature_norm"},{"location":"layers/adaptive_feature_norm/#pytorch_adapt.layers.adaptive_feature_norm.AdaptiveFeatureNorm","text":"Implementation of the loss in Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation . Encourages features to gradually have larger and larger L2 norms.","title":"AdaptiveFeatureNorm"},{"location":"layers/adaptive_feature_norm/#pytorch_adapt.layers.adaptive_feature_norm.AdaptiveFeatureNorm.__init__","text":"Parameters: Name Type Description Default step_size float The desired increase in L2 norm at each iteration. Note that the loss will always be equal to step_size because the goal is always to make the L2 norm step_size larger than whatever the current L2 norm is. 1 Source code in pytorch_adapt\\layers\\adaptive_feature_norm.py def __init__ ( self , step_size : float = 1 ): \"\"\" Arguments: step_size: The desired increase in L2 norm at each iteration. Note that the loss will always be equal to ```step_size``` because the goal is always to make the L2 norm ```step_size``` larger than whatever the current L2 norm is. \"\"\" super () . __init__ () self . step_size = step_size","title":"__init__()"},{"location":"layers/adaptive_feature_norm/#pytorch_adapt.layers.adaptive_feature_norm.L2PreservedDropout","text":"Implementation of the dropout layer described in Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation . Regular dropout preserves the L1 norm of features, whereas this layer preserves the L2 norm.","title":"L2PreservedDropout"},{"location":"layers/adaptive_feature_norm/#pytorch_adapt.layers.adaptive_feature_norm.L2PreservedDropout.__init__","text":"Parameters: Name Type Description Default p float probability of an element to be zeroed 0.5 inplace bool if set to True, will do this operation in-place False Source code in pytorch_adapt\\layers\\adaptive_feature_norm.py def __init__ ( self , p : float = 0.5 , inplace : bool = False ): \"\"\" Arguments: p: probability of an element to be zeroed inplace: if set to True, will do this operation in-place \"\"\" super () . __init__ () self . dropout = torch . nn . Dropout ( p = p , inplace = inplace ) self . scale = math . sqrt ( 1 - p )","title":"__init__()"},{"location":"layers/batch_spectral_loss/","text":"pytorch_adapt.layers.batch_spectral_loss \u00b6 BatchSpectralLoss \u00b6 Implementation of the loss in Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation . The loss is the sum of the squares of the first k singular values. __init__ ( self , k = 1 ) special \u00b6 Parameters: Name Type Description Default k int the number of singular values to include in the loss 1 Source code in pytorch_adapt\\layers\\batch_spectral_loss.py def __init__ ( self , k : int = 1 ): \"\"\" Arguments: k: the number of singular values to include in the loss \"\"\" super () . __init__ () self . k = k","title":"BatchSpectralLoss"},{"location":"layers/batch_spectral_loss/#pytorch_adapt.layers.batch_spectral_loss","text":"","title":"batch_spectral_loss"},{"location":"layers/batch_spectral_loss/#pytorch_adapt.layers.batch_spectral_loss.BatchSpectralLoss","text":"Implementation of the loss in Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation . The loss is the sum of the squares of the first k singular values.","title":"BatchSpectralLoss"},{"location":"layers/batch_spectral_loss/#pytorch_adapt.layers.batch_spectral_loss.BatchSpectralLoss.__init__","text":"Parameters: Name Type Description Default k int the number of singular values to include in the loss 1 Source code in pytorch_adapt\\layers\\batch_spectral_loss.py def __init__ ( self , k : int = 1 ): \"\"\" Arguments: k: the number of singular values to include in the loss \"\"\" super () . __init__ () self . k = k","title":"__init__()"},{"location":"layers/bnm_loss/","text":"pytorch_adapt.layers.bnm_loss \u00b6 BNMLoss \u00b6 Implementation of the loss in Towards Discriminability and Diversity: Batch Nuclear-norm Maximization under Label Insufficient Situations .","title":"BNMLoss"},{"location":"layers/bnm_loss/#pytorch_adapt.layers.bnm_loss","text":"","title":"bnm_loss"},{"location":"layers/bnm_loss/#pytorch_adapt.layers.bnm_loss.BNMLoss","text":"Implementation of the loss in Towards Discriminability and Diversity: Batch Nuclear-norm Maximization under Label Insufficient Situations .","title":"BNMLoss"},{"location":"layers/concat_softmax/","text":"pytorch_adapt.layers.concat_softmax \u00b6 ConcatSoftmax \u00b6 Applies softmax to the concatenation of a list of tensors. __init__ ( self , dim = 1 ) special \u00b6 Parameters: Name Type Description Default dim int a dimension along which softmax will be computed 1 Source code in pytorch_adapt\\layers\\concat_softmax.py def __init__ ( self , dim : int = 1 ): \"\"\" Arguments: dim: a dimension along which softmax will be computed \"\"\" super () . __init__ () self . dim = dim forward ( self , * x ) \u00b6 Parameters: Name Type Description Default *x Tensor A sequence of tensors to be concatenated () Source code in pytorch_adapt\\layers\\concat_softmax.py def forward ( self , * x : torch . Tensor ): \"\"\" Arguments: *x: A sequence of tensors to be concatenated \"\"\" all_logits = torch . cat ( x , dim = self . dim ) return torch . nn . functional . softmax ( all_logits , dim = self . dim )","title":"ConcatSoftmax"},{"location":"layers/concat_softmax/#pytorch_adapt.layers.concat_softmax","text":"","title":"concat_softmax"},{"location":"layers/concat_softmax/#pytorch_adapt.layers.concat_softmax.ConcatSoftmax","text":"Applies softmax to the concatenation of a list of tensors.","title":"ConcatSoftmax"},{"location":"layers/concat_softmax/#pytorch_adapt.layers.concat_softmax.ConcatSoftmax.__init__","text":"Parameters: Name Type Description Default dim int a dimension along which softmax will be computed 1 Source code in pytorch_adapt\\layers\\concat_softmax.py def __init__ ( self , dim : int = 1 ): \"\"\" Arguments: dim: a dimension along which softmax will be computed \"\"\" super () . __init__ () self . dim = dim","title":"__init__()"},{"location":"layers/concat_softmax/#pytorch_adapt.layers.concat_softmax.ConcatSoftmax.forward","text":"Parameters: Name Type Description Default *x Tensor A sequence of tensors to be concatenated () Source code in pytorch_adapt\\layers\\concat_softmax.py def forward ( self , * x : torch . Tensor ): \"\"\" Arguments: *x: A sequence of tensors to be concatenated \"\"\" all_logits = torch . cat ( x , dim = self . dim ) return torch . nn . functional . softmax ( all_logits , dim = self . dim )","title":"forward()"},{"location":"layers/confidence_weights/","text":"pytorch_adapt.layers.confidence_weights \u00b6 ConfidenceWeights \u00b6 Returns the max value along each row of the input, followed by an optional normalization function. The output of this can be used to weight classification losses by the \"confidence\" of the predictions. __init__ ( self , normalizer = None ) special \u00b6 Parameters: Name Type Description Default normalizer Callable[[torch.Tensor], torch.Tensor] A callable for normalizing (e.g. min-max normalization) the weights. If None , then no normalization is used. None Source code in pytorch_adapt\\layers\\confidence_weights.py def __init__ ( self , normalizer : Callable [[ torch . Tensor ], torch . Tensor ] = None ): \"\"\" Arguments: normalizer: A callable for normalizing (e.g. min-max normalization) the weights. If ```None```, then no normalization is used. \"\"\" super () . __init__ () self . normalizer = c_f . default ( normalizer , NoNormalizer ())","title":"ConfidenceWeights"},{"location":"layers/confidence_weights/#pytorch_adapt.layers.confidence_weights","text":"","title":"confidence_weights"},{"location":"layers/confidence_weights/#pytorch_adapt.layers.confidence_weights.ConfidenceWeights","text":"Returns the max value along each row of the input, followed by an optional normalization function. The output of this can be used to weight classification losses by the \"confidence\" of the predictions.","title":"ConfidenceWeights"},{"location":"layers/confidence_weights/#pytorch_adapt.layers.confidence_weights.ConfidenceWeights.__init__","text":"Parameters: Name Type Description Default normalizer Callable[[torch.Tensor], torch.Tensor] A callable for normalizing (e.g. min-max normalization) the weights. If None , then no normalization is used. None Source code in pytorch_adapt\\layers\\confidence_weights.py def __init__ ( self , normalizer : Callable [[ torch . Tensor ], torch . Tensor ] = None ): \"\"\" Arguments: normalizer: A callable for normalizing (e.g. min-max normalization) the weights. If ```None```, then no normalization is used. \"\"\" super () . __init__ () self . normalizer = c_f . default ( normalizer , NoNormalizer ())","title":"__init__()"},{"location":"layers/coral_loss/","text":"pytorch_adapt.layers.coral_loss \u00b6 CORALLoss \u00b6 Implementation of Deep CORAL: Correlation Alignment for Deep Domain Adaptation forward ( self , x , y ) \u00b6 Parameters: Name Type Description Default x Tensor features from one domain required y Tensor features from the other domain required Source code in pytorch_adapt\\layers\\coral_loss.py def forward ( self , x : torch . Tensor , y : torch . Tensor ): \"\"\" Arguments: x: features from one domain y: features from the other domain \"\"\" embedding_size = x . shape [ 1 ] cx = covariance ( x ) cy = covariance ( y ) squared_fro_norm = torch . linalg . norm ( cx - cy , ord = \"fro\" ) ** 2 return squared_fro_norm / ( 4 * ( embedding_size ** 2 ))","title":"CORALLoss"},{"location":"layers/coral_loss/#pytorch_adapt.layers.coral_loss","text":"","title":"coral_loss"},{"location":"layers/coral_loss/#pytorch_adapt.layers.coral_loss.CORALLoss","text":"Implementation of Deep CORAL: Correlation Alignment for Deep Domain Adaptation","title":"CORALLoss"},{"location":"layers/coral_loss/#pytorch_adapt.layers.coral_loss.CORALLoss.forward","text":"Parameters: Name Type Description Default x Tensor features from one domain required y Tensor features from the other domain required Source code in pytorch_adapt\\layers\\coral_loss.py def forward ( self , x : torch . Tensor , y : torch . Tensor ): \"\"\" Arguments: x: features from one domain y: features from the other domain \"\"\" embedding_size = x . shape [ 1 ] cx = covariance ( x ) cy = covariance ( y ) squared_fro_norm = torch . linalg . norm ( cx - cy , ord = \"fro\" ) ** 2 return squared_fro_norm / ( 4 * ( embedding_size ** 2 ))","title":"forward()"},{"location":"layers/diversity_loss/","text":"pytorch_adapt.layers.diversity_loss \u00b6 DiversityLoss \u00b6 Encourages predictions to be uniform, batch wise. Takes logits (before softmax) as input. For example: A tensor with a large loss: torch.tensor([[1e4, 0, 0], [1e4, 0, 0], [1e4, 0, 0]]) A tensor with a small loss: torch.tensor([[1e4, 0, 0], [0, 1e4, 0], [0, 0, 1e4]])","title":"DiversityLoss"},{"location":"layers/diversity_loss/#pytorch_adapt.layers.diversity_loss","text":"","title":"diversity_loss"},{"location":"layers/diversity_loss/#pytorch_adapt.layers.diversity_loss.DiversityLoss","text":"Encourages predictions to be uniform, batch wise. Takes logits (before softmax) as input. For example: A tensor with a large loss: torch.tensor([[1e4, 0, 0], [1e4, 0, 0], [1e4, 0, 0]]) A tensor with a small loss: torch.tensor([[1e4, 0, 0], [0, 1e4, 0], [0, 0, 1e4]])","title":"DiversityLoss"},{"location":"layers/do_nothing_optimizer/","text":"pytorch_adapt.layers.do_nothing_optimizer \u00b6 DoNothingOptimizer \u00b6 An optimizer that doesn't do anything, i.e. step and zero_grad are empty functions.","title":"DoNothingOptimizer"},{"location":"layers/do_nothing_optimizer/#pytorch_adapt.layers.do_nothing_optimizer","text":"","title":"do_nothing_optimizer"},{"location":"layers/do_nothing_optimizer/#pytorch_adapt.layers.do_nothing_optimizer.DoNothingOptimizer","text":"An optimizer that doesn't do anything, i.e. step and zero_grad are empty functions.","title":"DoNothingOptimizer"},{"location":"layers/entropy_loss/","text":"pytorch_adapt.layers.entropy_loss \u00b6 EntropyLoss \u00b6 Encourages low entropy predictions, or in other words, \"confident\" predictions. __init__ ( self , after_softmax = False , return_mean = True ) special \u00b6 Parameters: Name Type Description Default after_softmax bool If True , then the rows of the input are assumed to already have softmax applied to them. False return_mean bool If True , the mean entropy will be returned. If False , the entropy per row of the input will be returned. True Source code in pytorch_adapt\\layers\\entropy_loss.py def __init__ ( self , after_softmax : bool = False , return_mean : bool = True ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. return_mean: If ```True```, the mean entropy will be returned. If ```False```, the entropy per row of the input will be returned. \"\"\" super () . __init__ () self . after_softmax = after_softmax self . return_mean = return_mean forward ( self , logits ) \u00b6 Parameters: Name Type Description Default logits Tensor Raw logits if self.after_softmax is False. Otherwise each row should be predictions that sum up to 1. required Source code in pytorch_adapt\\layers\\entropy_loss.py def forward ( self , logits : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: logits: Raw logits if ```self.after_softmax``` is False. Otherwise each row should be predictions that sum up to 1. \"\"\" entropies = get_entropy ( logits , self . after_softmax ) if self . return_mean : return torch . mean ( entropies ) return entropies","title":"EntropyLoss"},{"location":"layers/entropy_loss/#pytorch_adapt.layers.entropy_loss","text":"","title":"entropy_loss"},{"location":"layers/entropy_loss/#pytorch_adapt.layers.entropy_loss.EntropyLoss","text":"Encourages low entropy predictions, or in other words, \"confident\" predictions.","title":"EntropyLoss"},{"location":"layers/entropy_loss/#pytorch_adapt.layers.entropy_loss.EntropyLoss.__init__","text":"Parameters: Name Type Description Default after_softmax bool If True , then the rows of the input are assumed to already have softmax applied to them. False return_mean bool If True , the mean entropy will be returned. If False , the entropy per row of the input will be returned. True Source code in pytorch_adapt\\layers\\entropy_loss.py def __init__ ( self , after_softmax : bool = False , return_mean : bool = True ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. return_mean: If ```True```, the mean entropy will be returned. If ```False```, the entropy per row of the input will be returned. \"\"\" super () . __init__ () self . after_softmax = after_softmax self . return_mean = return_mean","title":"__init__()"},{"location":"layers/entropy_loss/#pytorch_adapt.layers.entropy_loss.EntropyLoss.forward","text":"Parameters: Name Type Description Default logits Tensor Raw logits if self.after_softmax is False. Otherwise each row should be predictions that sum up to 1. required Source code in pytorch_adapt\\layers\\entropy_loss.py def forward ( self , logits : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: logits: Raw logits if ```self.after_softmax``` is False. Otherwise each row should be predictions that sum up to 1. \"\"\" entropies = get_entropy ( logits , self . after_softmax ) if self . return_mean : return torch . mean ( entropies ) return entropies","title":"forward()"},{"location":"layers/entropy_weights/","text":"pytorch_adapt.layers.entropy_weights \u00b6 EntropyWeights \u00b6 Implementation of entropy weighting described in Conditional Adversarial Domain Adaptation . Computes the entropy ( x ) per row of the input, and returns 1+exp(-x) . This can be used to weight losses, such that the most confidently scored samples have a higher weighting. __init__ ( self , after_softmax = False , normalizer = None ) special \u00b6 Parameters: Name Type Description Default after_softmax bool If True , then the rows of the input are assumed to already have softmax applied to them. False normalizer Callable[[torch.Tensor], torch.Tensor] A callable for normalizing (e.g. min-max normalization) the weights. If None , then sum normalization is used. None Source code in pytorch_adapt\\layers\\entropy_weights.py def __init__ ( self , after_softmax : bool = False , normalizer : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. normalizer: A callable for normalizing (e.g. min-max normalization) the weights. If ```None```, then sum normalization is used. \"\"\" super () . __init__ () self . after_softmax = after_softmax self . normalizer = c_f . default ( normalizer , SumNormalizer , {}) forward ( self , logits ) \u00b6 Parameters: Name Type Description Default logits Tensor Raw logits if self.after_softmax is False. Otherwise each row should be predictions that sum up to 1. required Source code in pytorch_adapt\\layers\\entropy_weights.py def forward ( self , logits : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: logits: Raw logits if ```self.after_softmax``` is False. Otherwise each row should be predictions that sum up to 1. \"\"\" return entropy_weights ( logits , self . after_softmax , self . normalizer )","title":"EntropyWeights"},{"location":"layers/entropy_weights/#pytorch_adapt.layers.entropy_weights","text":"","title":"entropy_weights"},{"location":"layers/entropy_weights/#pytorch_adapt.layers.entropy_weights.EntropyWeights","text":"Implementation of entropy weighting described in Conditional Adversarial Domain Adaptation . Computes the entropy ( x ) per row of the input, and returns 1+exp(-x) . This can be used to weight losses, such that the most confidently scored samples have a higher weighting.","title":"EntropyWeights"},{"location":"layers/entropy_weights/#pytorch_adapt.layers.entropy_weights.EntropyWeights.__init__","text":"Parameters: Name Type Description Default after_softmax bool If True , then the rows of the input are assumed to already have softmax applied to them. False normalizer Callable[[torch.Tensor], torch.Tensor] A callable for normalizing (e.g. min-max normalization) the weights. If None , then sum normalization is used. None Source code in pytorch_adapt\\layers\\entropy_weights.py def __init__ ( self , after_softmax : bool = False , normalizer : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. normalizer: A callable for normalizing (e.g. min-max normalization) the weights. If ```None```, then sum normalization is used. \"\"\" super () . __init__ () self . after_softmax = after_softmax self . normalizer = c_f . default ( normalizer , SumNormalizer , {})","title":"__init__()"},{"location":"layers/entropy_weights/#pytorch_adapt.layers.entropy_weights.EntropyWeights.forward","text":"Parameters: Name Type Description Default logits Tensor Raw logits if self.after_softmax is False. Otherwise each row should be predictions that sum up to 1. required Source code in pytorch_adapt\\layers\\entropy_weights.py def forward ( self , logits : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: logits: Raw logits if ```self.after_softmax``` is False. Otherwise each row should be predictions that sum up to 1. \"\"\" return entropy_weights ( logits , self . after_softmax , self . normalizer )","title":"forward()"},{"location":"layers/gradient_reversal/","text":"pytorch_adapt.layers.gradient_reversal \u00b6 GradientReversal \u00b6 Implementation of the gradient reversal layer described in Domain-Adversarial Training of Neural Networks , which 'leaves the input unchanged during forward propagation and reverses the gradient by multiplying it by a negative scalar during backpropagation.' __init__ ( self , weight = 1.0 ) special \u00b6 Parameters: Name Type Description Default weight float The gradients will be multiplied by -weight during the backward pass. 1.0 Source code in pytorch_adapt\\layers\\gradient_reversal.py def __init__ ( self , weight : float = 1.0 ): \"\"\" Arguments: weight: The gradients will be multiplied by ```-weight``` during the backward pass. \"\"\" super () . __init__ () self . register_buffer ( \"weight\" , torch . tensor ([ weight ])) pml_cf . add_to_recordable_attributes ( self , \"weight\" )","title":"GradientReversal"},{"location":"layers/gradient_reversal/#pytorch_adapt.layers.gradient_reversal","text":"","title":"gradient_reversal"},{"location":"layers/gradient_reversal/#pytorch_adapt.layers.gradient_reversal.GradientReversal","text":"Implementation of the gradient reversal layer described in Domain-Adversarial Training of Neural Networks , which 'leaves the input unchanged during forward propagation and reverses the gradient by multiplying it by a negative scalar during backpropagation.'","title":"GradientReversal"},{"location":"layers/gradient_reversal/#pytorch_adapt.layers.gradient_reversal.GradientReversal.__init__","text":"Parameters: Name Type Description Default weight float The gradients will be multiplied by -weight during the backward pass. 1.0 Source code in pytorch_adapt\\layers\\gradient_reversal.py def __init__ ( self , weight : float = 1.0 ): \"\"\" Arguments: weight: The gradients will be multiplied by ```-weight``` during the backward pass. \"\"\" super () . __init__ () self . register_buffer ( \"weight\" , torch . tensor ([ weight ])) pml_cf . add_to_recordable_attributes ( self , \"weight\" )","title":"__init__()"},{"location":"layers/mcc_loss/","text":"pytorch_adapt.layers.mcc_loss \u00b6 MCCLoss \u00b6 Implementation of Minimum Class Confusion for Versatile Domain Adaptation . __init__ ( self , T = 1 , entropy_weighter = None ) special \u00b6 Parameters: Name Type Description Default T float softmax temperature applied to the input target logits 1 entropy_weighter Callable[[torch.Tensor], torch.Tensor] a function that returns a weight for each sample. The weights are used in the process of computing the class confusion tensor as described in the paper. If None , then layers.EntropyWeights is used. None Source code in pytorch_adapt\\layers\\mcc_loss.py def __init__ ( self , T : float = 1 , entropy_weighter : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: T: softmax temperature applied to the input target logits entropy_weighter: a function that returns a weight for each sample. The weights are used in the process of computing the class confusion tensor as described in the paper. If ```None```, then ```layers.EntropyWeights``` is used. \"\"\" super () . __init__ () self . T = T self . entropy_weighter = c_f . default ( entropy_weighter , EntropyWeights ( after_softmax = True , normalizer = SumNormalizer ( scale_by_batch_size = True ) ), ) forward ( self , x ) \u00b6 Parameters: Name Type Description Default x Tensor target logits required Source code in pytorch_adapt\\layers\\mcc_loss.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: target logits \"\"\" Y = torch . nn . functional . softmax ( x / self . T , dim = 1 ) H_weights = self . entropy_weighter ( Y . detach ()) C = torch . linalg . multi_dot ([ Y . t (), torch . diag ( H_weights ), Y ]) C = C / torch . sum ( C , dim = 1 ) return ( torch . sum ( C ) - torch . trace ( C )) / C . shape [ 0 ]","title":"MCCLoss"},{"location":"layers/mcc_loss/#pytorch_adapt.layers.mcc_loss","text":"","title":"mcc_loss"},{"location":"layers/mcc_loss/#pytorch_adapt.layers.mcc_loss.MCCLoss","text":"Implementation of Minimum Class Confusion for Versatile Domain Adaptation .","title":"MCCLoss"},{"location":"layers/mcc_loss/#pytorch_adapt.layers.mcc_loss.MCCLoss.__init__","text":"Parameters: Name Type Description Default T float softmax temperature applied to the input target logits 1 entropy_weighter Callable[[torch.Tensor], torch.Tensor] a function that returns a weight for each sample. The weights are used in the process of computing the class confusion tensor as described in the paper. If None , then layers.EntropyWeights is used. None Source code in pytorch_adapt\\layers\\mcc_loss.py def __init__ ( self , T : float = 1 , entropy_weighter : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: T: softmax temperature applied to the input target logits entropy_weighter: a function that returns a weight for each sample. The weights are used in the process of computing the class confusion tensor as described in the paper. If ```None```, then ```layers.EntropyWeights``` is used. \"\"\" super () . __init__ () self . T = T self . entropy_weighter = c_f . default ( entropy_weighter , EntropyWeights ( after_softmax = True , normalizer = SumNormalizer ( scale_by_batch_size = True ) ), )","title":"__init__()"},{"location":"layers/mcc_loss/#pytorch_adapt.layers.mcc_loss.MCCLoss.forward","text":"Parameters: Name Type Description Default x Tensor target logits required Source code in pytorch_adapt\\layers\\mcc_loss.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: target logits \"\"\" Y = torch . nn . functional . softmax ( x / self . T , dim = 1 ) H_weights = self . entropy_weighter ( Y . detach ()) C = torch . linalg . multi_dot ([ Y . t (), torch . diag ( H_weights ), Y ]) C = C / torch . sum ( C , dim = 1 ) return ( torch . sum ( C ) - torch . trace ( C )) / C . shape [ 0 ]","title":"forward()"},{"location":"layers/mcd_loss/","text":"pytorch_adapt.layers.mcd_loss \u00b6 MCDLoss \u00b6 Implementation of the loss function used in Maximum Classifier Discrepancy for Unsupervised Domain Adaptation . __init__ ( self , dist_fn = None ) special \u00b6 Parameters: Name Type Description Default dist_fn Callable[[torch.Tensor], torch.Tensor] Computes the mean distance between two softmaxed tensors. If None , then torch.nn.L1Loss is used. None Source code in pytorch_adapt\\layers\\mcd_loss.py def __init__ ( self , dist_fn : Callable [[ torch . Tensor ], torch . Tensor ] = None ): \"\"\" Arguments: dist_fn: Computes the mean distance between two softmaxed tensors. If ```None```, then ```torch.nn.L1Loss``` is used. \"\"\" super () . __init__ () self . dist_fn = c_f . default ( dist_fn , torch . nn . L1Loss , {}) forward ( self , x , y ) \u00b6 Parameters: Name Type Description Default x Tensor a batch of class logits required y Tensor the other batch of class logits required Returns: Type Description Tensor The discrepancy between the two batches of class logits. Source code in pytorch_adapt\\layers\\mcd_loss.py def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: a batch of class logits y: the other batch of class logits Returns: The discrepancy between the two batches of class logits. \"\"\" return mcd_loss ( x , y , self . dist_fn )","title":"MCDLoss"},{"location":"layers/mcd_loss/#pytorch_adapt.layers.mcd_loss","text":"","title":"mcd_loss"},{"location":"layers/mcd_loss/#pytorch_adapt.layers.mcd_loss.MCDLoss","text":"Implementation of the loss function used in Maximum Classifier Discrepancy for Unsupervised Domain Adaptation .","title":"MCDLoss"},{"location":"layers/mcd_loss/#pytorch_adapt.layers.mcd_loss.MCDLoss.__init__","text":"Parameters: Name Type Description Default dist_fn Callable[[torch.Tensor], torch.Tensor] Computes the mean distance between two softmaxed tensors. If None , then torch.nn.L1Loss is used. None Source code in pytorch_adapt\\layers\\mcd_loss.py def __init__ ( self , dist_fn : Callable [[ torch . Tensor ], torch . Tensor ] = None ): \"\"\" Arguments: dist_fn: Computes the mean distance between two softmaxed tensors. If ```None```, then ```torch.nn.L1Loss``` is used. \"\"\" super () . __init__ () self . dist_fn = c_f . default ( dist_fn , torch . nn . L1Loss , {})","title":"__init__()"},{"location":"layers/mcd_loss/#pytorch_adapt.layers.mcd_loss.MCDLoss.forward","text":"Parameters: Name Type Description Default x Tensor a batch of class logits required y Tensor the other batch of class logits required Returns: Type Description Tensor The discrepancy between the two batches of class logits. Source code in pytorch_adapt\\layers\\mcd_loss.py def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: a batch of class logits y: the other batch of class logits Returns: The discrepancy between the two batches of class logits. \"\"\" return mcd_loss ( x , y , self . dist_fn )","title":"forward()"},{"location":"layers/mmd_loss/","text":"pytorch_adapt.layers.mmd_loss \u00b6 MMDLoss \u00b6 Implementation of Learning Transferable Features with Deep Adaptation Networks Deep Transfer Learning with Joint Adaptation Networks . __init__ ( self , kernel_scales = 1 , mmd_type = 'linear' ) special \u00b6 Parameters: Name Type Description Default kernel_scales Union[float, torch.Tensor] The kernel bandwidth is scaled by this amount. If a tensor, then multiple kernel bandwidths are used. 1 mmd_type str 'linear' or 'quadratic'. 'linear' uses the linear estimate of MK-MMD. 'linear' Source code in pytorch_adapt\\layers\\mmd_loss.py def __init__ ( self , kernel_scales : Union [ float , torch . Tensor ] = 1 , mmd_type : str = \"linear\" ): \"\"\" Arguments: kernel_scales: The kernel bandwidth is scaled by this amount. If a tensor, then multiple kernel bandwidths are used. mmd_type: 'linear' or 'quadratic'. 'linear' uses the linear estimate of MK-MMD. \"\"\" super () . __init__ () self . kernel_scales = kernel_scales self . dist_func = LpDistance ( normalize_embeddings = False , p = 2 , power = 2 ) self . mmd_type = mmd_type if mmd_type == \"linear\" : self . mmd_func = l_u . get_mmd_linear elif mmd_type == \"quadratic\" : self . mmd_func = l_u . get_mmd_quadratic else : raise ValueError ( \"mmd_type must be either linear or quadratic\" ) forward ( self , x , y ) \u00b6 Parameters: Name Type Description Default x Union[torch.Tensor, List[torch.Tensor]] features or a list of features from one domain. required y Union[torch.Tensor, List[torch.Tensor]] features or a list of features from the other domain. required Returns: Type Description Tensor MMD if the inputs are tensors, and Joint MMD (JMMD) if the inputs are lists of tensors. Source code in pytorch_adapt\\layers\\mmd_loss.py def forward ( self , x : Union [ torch . Tensor , List [ torch . Tensor ]], y : Union [ torch . Tensor , List [ torch . Tensor ]], ) -> torch . Tensor : \"\"\" Arguments: x: features or a list of features from one domain. y: features or a list of features from the other domain. Returns: MMD if the inputs are tensors, and Joint MMD (JMMD) if the inputs are lists of tensors. \"\"\" xx , yy , zz , scale = l_u . get_mmd_dist_mats ( x , y , self . dist_func ) if torch . is_tensor ( self . kernel_scales ): s = scale [ 0 ] if c_f . is_list_or_tuple ( scale ) else scale self . kernel_scales = pml_cf . to_device ( self . kernel_scales , s , dtype = s . dtype ) if c_f . is_list_or_tuple ( scale ): for i in range ( len ( scale )): scale [ i ] = scale [ i ] * self . kernel_scales else : scale = scale * self . kernel_scales return self . mmd_func ( xx , yy , zz , scale )","title":"MMDLoss"},{"location":"layers/mmd_loss/#pytorch_adapt.layers.mmd_loss","text":"","title":"mmd_loss"},{"location":"layers/mmd_loss/#pytorch_adapt.layers.mmd_loss.MMDLoss","text":"Implementation of Learning Transferable Features with Deep Adaptation Networks Deep Transfer Learning with Joint Adaptation Networks .","title":"MMDLoss"},{"location":"layers/mmd_loss/#pytorch_adapt.layers.mmd_loss.MMDLoss.__init__","text":"Parameters: Name Type Description Default kernel_scales Union[float, torch.Tensor] The kernel bandwidth is scaled by this amount. If a tensor, then multiple kernel bandwidths are used. 1 mmd_type str 'linear' or 'quadratic'. 'linear' uses the linear estimate of MK-MMD. 'linear' Source code in pytorch_adapt\\layers\\mmd_loss.py def __init__ ( self , kernel_scales : Union [ float , torch . Tensor ] = 1 , mmd_type : str = \"linear\" ): \"\"\" Arguments: kernel_scales: The kernel bandwidth is scaled by this amount. If a tensor, then multiple kernel bandwidths are used. mmd_type: 'linear' or 'quadratic'. 'linear' uses the linear estimate of MK-MMD. \"\"\" super () . __init__ () self . kernel_scales = kernel_scales self . dist_func = LpDistance ( normalize_embeddings = False , p = 2 , power = 2 ) self . mmd_type = mmd_type if mmd_type == \"linear\" : self . mmd_func = l_u . get_mmd_linear elif mmd_type == \"quadratic\" : self . mmd_func = l_u . get_mmd_quadratic else : raise ValueError ( \"mmd_type must be either linear or quadratic\" )","title":"__init__()"},{"location":"layers/mmd_loss/#pytorch_adapt.layers.mmd_loss.MMDLoss.forward","text":"Parameters: Name Type Description Default x Union[torch.Tensor, List[torch.Tensor]] features or a list of features from one domain. required y Union[torch.Tensor, List[torch.Tensor]] features or a list of features from the other domain. required Returns: Type Description Tensor MMD if the inputs are tensors, and Joint MMD (JMMD) if the inputs are lists of tensors. Source code in pytorch_adapt\\layers\\mmd_loss.py def forward ( self , x : Union [ torch . Tensor , List [ torch . Tensor ]], y : Union [ torch . Tensor , List [ torch . Tensor ]], ) -> torch . Tensor : \"\"\" Arguments: x: features or a list of features from one domain. y: features or a list of features from the other domain. Returns: MMD if the inputs are tensors, and Joint MMD (JMMD) if the inputs are lists of tensors. \"\"\" xx , yy , zz , scale = l_u . get_mmd_dist_mats ( x , y , self . dist_func ) if torch . is_tensor ( self . kernel_scales ): s = scale [ 0 ] if c_f . is_list_or_tuple ( scale ) else scale self . kernel_scales = pml_cf . to_device ( self . kernel_scales , s , dtype = s . dtype ) if c_f . is_list_or_tuple ( scale ): for i in range ( len ( scale )): scale [ i ] = scale [ i ] * self . kernel_scales else : scale = scale * self . kernel_scales return self . mmd_func ( xx , yy , zz , scale )","title":"forward()"},{"location":"layers/model_with_bridge/","text":"pytorch_adapt.layers.model_with_bridge \u00b6 ModelWithBridge \u00b6 Implementation of the bridge architecture described in Gradually Vanishing Bridge for Adversarial Domain Adaptation . __init__ ( self , model , bridge = None ) special \u00b6 Parameters: Name Type Description Default model Module Any pytorch model. required bridge Module A model which has the same input/output sizes as model . If None , then the bridge is formed by copying model , and randomly reinitialization all its parameters. None Source code in pytorch_adapt\\layers\\model_with_bridge.py def __init__ ( self , model : torch . nn . Module , bridge : torch . nn . Module = None ): \"\"\" Arguments: model: Any pytorch model. bridge: A model which has the same input/output sizes as ```model```. If ```None```, then the bridge is formed by copying ```model```, and randomly reinitialization all its parameters. \"\"\" super () . __init__ () self . model = model if bridge is None : bridge = c_f . reinit ( copy . deepcopy ( model )) self . bridge = bridge forward ( self , x , return_bridge = False ) \u00b6 Parameters: Name Type Description Default x Tensor The input to both self.model and self.bridge . required return_bridge bool Whether or not to return the bridge output in addition to the model - bridge output False Returns: Type Description Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] If return_bridge = False , then return just model - bridge . If return_bridge = True , then return a tuple of (model - bridge), bridge Source code in pytorch_adapt\\layers\\model_with_bridge.py def forward ( self , x : torch . Tensor , return_bridge : bool = False ) -> Union [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]]: \"\"\" Arguments: x: The input to both ```self.model``` and ```self.bridge```. return_bridge: Whether or not to return the bridge output in addition to the ```model - bridge``` output Returns: If ```return_bridge = False```, then return just ```model - bridge```. If ```return_bridge = True```, then return a tuple of ```(model - bridge), bridge``` \"\"\" y = self . model ( x ) z = self . bridge ( x ) output = y - z if return_bridge : return output , z return output","title":"ModelWithBridge"},{"location":"layers/model_with_bridge/#pytorch_adapt.layers.model_with_bridge","text":"","title":"model_with_bridge"},{"location":"layers/model_with_bridge/#pytorch_adapt.layers.model_with_bridge.ModelWithBridge","text":"Implementation of the bridge architecture described in Gradually Vanishing Bridge for Adversarial Domain Adaptation .","title":"ModelWithBridge"},{"location":"layers/model_with_bridge/#pytorch_adapt.layers.model_with_bridge.ModelWithBridge.__init__","text":"Parameters: Name Type Description Default model Module Any pytorch model. required bridge Module A model which has the same input/output sizes as model . If None , then the bridge is formed by copying model , and randomly reinitialization all its parameters. None Source code in pytorch_adapt\\layers\\model_with_bridge.py def __init__ ( self , model : torch . nn . Module , bridge : torch . nn . Module = None ): \"\"\" Arguments: model: Any pytorch model. bridge: A model which has the same input/output sizes as ```model```. If ```None```, then the bridge is formed by copying ```model```, and randomly reinitialization all its parameters. \"\"\" super () . __init__ () self . model = model if bridge is None : bridge = c_f . reinit ( copy . deepcopy ( model )) self . bridge = bridge","title":"__init__()"},{"location":"layers/model_with_bridge/#pytorch_adapt.layers.model_with_bridge.ModelWithBridge.forward","text":"Parameters: Name Type Description Default x Tensor The input to both self.model and self.bridge . required return_bridge bool Whether or not to return the bridge output in addition to the model - bridge output False Returns: Type Description Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] If return_bridge = False , then return just model - bridge . If return_bridge = True , then return a tuple of (model - bridge), bridge Source code in pytorch_adapt\\layers\\model_with_bridge.py def forward ( self , x : torch . Tensor , return_bridge : bool = False ) -> Union [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]]: \"\"\" Arguments: x: The input to both ```self.model``` and ```self.bridge```. return_bridge: Whether or not to return the bridge output in addition to the ```model - bridge``` output Returns: If ```return_bridge = False```, then return just ```model - bridge```. If ```return_bridge = True```, then return a tuple of ```(model - bridge), bridge``` \"\"\" y = self . model ( x ) z = self . bridge ( x ) output = y - z if return_bridge : return output , z return output","title":"forward()"},{"location":"layers/multiple_models/","text":"pytorch_adapt.layers.multiple_models \u00b6 MultipleModels \u00b6 Wraps a list of models, and returns their outputs as a list of tensors __init__ ( self , * models ) special \u00b6 Parameters: Name Type Description Default models Module The models to be wrapped. () Source code in pytorch_adapt\\layers\\multiple_models.py def __init__ ( self , * models : torch . nn . Module ): \"\"\" Arguments: models: The models to be wrapped. \"\"\" super () . __init__ () self . models = torch . nn . ModuleList ( models ) forward ( self , x ) \u00b6 Parameters: Name Type Description Default x Tensor the input to each model required Returns: Type Description List[Any] A list containing the output of each model. Source code in pytorch_adapt\\layers\\multiple_models.py def forward ( self , x : torch . Tensor ) -> List [ Any ]: \"\"\" Arguments: x: the input to each model Returns: A list containing the output of each model. \"\"\" outputs = [] for m in self . models : outputs . append ( m ( x )) return outputs","title":"MultipleModels"},{"location":"layers/multiple_models/#pytorch_adapt.layers.multiple_models","text":"","title":"multiple_models"},{"location":"layers/multiple_models/#pytorch_adapt.layers.multiple_models.MultipleModels","text":"Wraps a list of models, and returns their outputs as a list of tensors","title":"MultipleModels"},{"location":"layers/multiple_models/#pytorch_adapt.layers.multiple_models.MultipleModels.__init__","text":"Parameters: Name Type Description Default models Module The models to be wrapped. () Source code in pytorch_adapt\\layers\\multiple_models.py def __init__ ( self , * models : torch . nn . Module ): \"\"\" Arguments: models: The models to be wrapped. \"\"\" super () . __init__ () self . models = torch . nn . ModuleList ( models )","title":"__init__()"},{"location":"layers/multiple_models/#pytorch_adapt.layers.multiple_models.MultipleModels.forward","text":"Parameters: Name Type Description Default x Tensor the input to each model required Returns: Type Description List[Any] A list containing the output of each model. Source code in pytorch_adapt\\layers\\multiple_models.py def forward ( self , x : torch . Tensor ) -> List [ Any ]: \"\"\" Arguments: x: the input to each model Returns: A list containing the output of each model. \"\"\" outputs = [] for m in self . models : outputs . append ( m ( x )) return outputs","title":"forward()"},{"location":"layers/neighborhood_aggregation/","text":"pytorch_adapt.layers.neighborhood_aggregation \u00b6 NeighborhoodAggregation \u00b6 Implementation of the pseudo labeling step in Domain Adaptation with Auxiliary Target Domain-Oriented Classifier . __init__ ( self , dataset_size , feature_dim , num_classes , k = 5 , T = 0.5 ) special \u00b6 Parameters: Name Type Description Default dataset_size int The number of samples in the target dataset. required feature_dim int The feature dimensionality, i.e at each iteration the features should be size (N, D) where N is batch size and D is feature_dim . required num_classes int The number of class labels in the target dataset. required k int The number of nearest neighbors used to determine each sample's pseudolabel 5 T float The softmax temperature used when storing predictions in memory. 0.5 Source code in pytorch_adapt\\layers\\neighborhood_aggregation.py def __init__ ( self , dataset_size : int , feature_dim : int , num_classes : int , k : int = 5 , T : float = 0.5 , ): \"\"\" Arguments: dataset_size: The number of samples in the target dataset. feature_dim: The feature dimensionality, i.e at each iteration the features should be size ```(N, D)``` where N is batch size and D is ```feature_dim```. num_classes: The number of class labels in the target dataset. k: The number of nearest neighbors used to determine each sample's pseudolabel T: The softmax temperature used when storing predictions in memory. \"\"\" super () . __init__ () self . register_buffer ( \"feat_memory\" , F . normalize ( torch . rand ( dataset_size , feature_dim )) ) self . register_buffer ( \"pred_memory\" , torch . ones ( dataset_size , num_classes ) / num_classes ) self . k = k self . T = T forward ( self , features , logits = None , update = False , idx = None ) \u00b6 Parameters: Name Type Description Default features Tensor The features to compute pseudolabels for. required logits Tensor The logits from which predictions will be computed and stored in memory. Required if update = True None update bool If True, the current batch of predictions is added to the memory bank. False idx Tensor A tensor containing the dataset indices that produced each row of features . None Source code in pytorch_adapt\\layers\\neighborhood_aggregation.py def forward ( self , features : torch . Tensor , logits : torch . Tensor = None , update : bool = False , idx : torch . Tensor = None , ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Arguments: features: The features to compute pseudolabels for. logits: The logits from which predictions will be computed and stored in memory. Required if ```update = True``` update: If True, the current batch of predictions is added to the memory bank. idx: A tensor containing the dataset indices that produced each row of ```features```. \"\"\" # move to device if necessary self . feat_memory = pml_cf . to_device ( self . feat_memory , features ) self . pred_memory = pml_cf . to_device ( self . pred_memory , features ) with torch . no_grad (): features = F . normalize ( features ) pseudo_labels , mean_logits = self . get_pseudo_labels ( features , idx ) if update : self . update_memory ( features , logits , idx ) return pseudo_labels , mean_logits","title":"NeighborhoodAggregation"},{"location":"layers/neighborhood_aggregation/#pytorch_adapt.layers.neighborhood_aggregation","text":"","title":"neighborhood_aggregation"},{"location":"layers/neighborhood_aggregation/#pytorch_adapt.layers.neighborhood_aggregation.NeighborhoodAggregation","text":"Implementation of the pseudo labeling step in Domain Adaptation with Auxiliary Target Domain-Oriented Classifier .","title":"NeighborhoodAggregation"},{"location":"layers/neighborhood_aggregation/#pytorch_adapt.layers.neighborhood_aggregation.NeighborhoodAggregation.__init__","text":"Parameters: Name Type Description Default dataset_size int The number of samples in the target dataset. required feature_dim int The feature dimensionality, i.e at each iteration the features should be size (N, D) where N is batch size and D is feature_dim . required num_classes int The number of class labels in the target dataset. required k int The number of nearest neighbors used to determine each sample's pseudolabel 5 T float The softmax temperature used when storing predictions in memory. 0.5 Source code in pytorch_adapt\\layers\\neighborhood_aggregation.py def __init__ ( self , dataset_size : int , feature_dim : int , num_classes : int , k : int = 5 , T : float = 0.5 , ): \"\"\" Arguments: dataset_size: The number of samples in the target dataset. feature_dim: The feature dimensionality, i.e at each iteration the features should be size ```(N, D)``` where N is batch size and D is ```feature_dim```. num_classes: The number of class labels in the target dataset. k: The number of nearest neighbors used to determine each sample's pseudolabel T: The softmax temperature used when storing predictions in memory. \"\"\" super () . __init__ () self . register_buffer ( \"feat_memory\" , F . normalize ( torch . rand ( dataset_size , feature_dim )) ) self . register_buffer ( \"pred_memory\" , torch . ones ( dataset_size , num_classes ) / num_classes ) self . k = k self . T = T","title":"__init__()"},{"location":"layers/neighborhood_aggregation/#pytorch_adapt.layers.neighborhood_aggregation.NeighborhoodAggregation.forward","text":"Parameters: Name Type Description Default features Tensor The features to compute pseudolabels for. required logits Tensor The logits from which predictions will be computed and stored in memory. Required if update = True None update bool If True, the current batch of predictions is added to the memory bank. False idx Tensor A tensor containing the dataset indices that produced each row of features . None Source code in pytorch_adapt\\layers\\neighborhood_aggregation.py def forward ( self , features : torch . Tensor , logits : torch . Tensor = None , update : bool = False , idx : torch . Tensor = None , ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Arguments: features: The features to compute pseudolabels for. logits: The logits from which predictions will be computed and stored in memory. Required if ```update = True``` update: If True, the current batch of predictions is added to the memory bank. idx: A tensor containing the dataset indices that produced each row of ```features```. \"\"\" # move to device if necessary self . feat_memory = pml_cf . to_device ( self . feat_memory , features ) self . pred_memory = pml_cf . to_device ( self . pred_memory , features ) with torch . no_grad (): features = F . normalize ( features ) pseudo_labels , mean_logits = self . get_pseudo_labels ( features , idx ) if update : self . update_memory ( features , logits , idx ) return pseudo_labels , mean_logits","title":"forward()"},{"location":"layers/plus_residual/","text":"pytorch_adapt.layers.plus_residual \u00b6 PlusResidual \u00b6 Wraps a layer such that the forward pass returns x + self.layer(x) __init__ ( self , layer ) special \u00b6 Parameters: Name Type Description Default layer Module The layer to be wrapped. required Source code in pytorch_adapt\\layers\\plus_residual.py def __init__ ( self , layer : torch . nn . Module ): \"\"\" Arguments: layer: The layer to be wrapped. \"\"\" super () . __init__ () self . layer = layer","title":"PlusResidual"},{"location":"layers/plus_residual/#pytorch_adapt.layers.plus_residual","text":"","title":"plus_residual"},{"location":"layers/plus_residual/#pytorch_adapt.layers.plus_residual.PlusResidual","text":"Wraps a layer such that the forward pass returns x + self.layer(x)","title":"PlusResidual"},{"location":"layers/plus_residual/#pytorch_adapt.layers.plus_residual.PlusResidual.__init__","text":"Parameters: Name Type Description Default layer Module The layer to be wrapped. required Source code in pytorch_adapt\\layers\\plus_residual.py def __init__ ( self , layer : torch . nn . Module ): \"\"\" Arguments: layer: The layer to be wrapped. \"\"\" super () . __init__ () self . layer = layer","title":"__init__()"},{"location":"layers/randomized_dot_product/","text":"pytorch_adapt.layers.randomized_dot_product \u00b6 RandomizedDotProduct \u00b6 Implementation of randomized multilinear conditioning from Conditional Adversarial Domain Adaptation . __init__ ( self , in_dims , out_dim = 1024 ) special \u00b6 Parameters: Name Type Description Default in_dims List[int] A list of the feature dims. For example, if the input features have shapes (32, 512) and (32, 64) , then in_dims = [512, 64] . required out_dim int The output feature dim. 1024 Source code in pytorch_adapt\\layers\\randomized_dot_product.py def __init__ ( self , in_dims : List [ int ], out_dim : int = 1024 ): \"\"\" Arguments: in_dims: A list of the feature dims. For example, if the input features have shapes ```(32, 512)``` and ```(32, 64)```, then ```in_dims = [512, 64]```. out_dim: The output feature dim. \"\"\" super () . __init__ () self . in_dims = in_dims for i , d in enumerate ( in_dims ): self . register_buffer ( self . rand_mat_name ( i ), torch . randn ( d , out_dim )) self . out_dim = out_dim self . num_mats = len ( in_dims ) self . divisor = math . pow ( float ( self . out_dim ), 1.0 / self . num_mats ) forward ( self , * inputs ) \u00b6 Parameters: Name Type Description Default inputs Tensor The number of inputs must be equal to the length of self.in_dims . () Source code in pytorch_adapt\\layers\\randomized_dot_product.py def forward ( self , * inputs : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: inputs: The number of inputs must be equal to the length of ```self.in_dims```. \"\"\" for i in range ( self . num_mats ): # move to device if necessary curr = inputs [ i ] self . set_rand_mat ( i , pml_cf . to_device ( self . get_rand_mat ( i ), curr , dtype = curr . dtype ) ) return_list = [ torch . mm ( inputs [ i ], self . get_rand_mat ( i )) for i in range ( self . num_mats ) ] return_tensor = return_list [ 0 ] / self . divisor for single in return_list [ 1 :]: return_tensor = return_tensor * single return return_tensor","title":"RandomizedDotProduct"},{"location":"layers/randomized_dot_product/#pytorch_adapt.layers.randomized_dot_product","text":"","title":"randomized_dot_product"},{"location":"layers/randomized_dot_product/#pytorch_adapt.layers.randomized_dot_product.RandomizedDotProduct","text":"Implementation of randomized multilinear conditioning from Conditional Adversarial Domain Adaptation .","title":"RandomizedDotProduct"},{"location":"layers/randomized_dot_product/#pytorch_adapt.layers.randomized_dot_product.RandomizedDotProduct.__init__","text":"Parameters: Name Type Description Default in_dims List[int] A list of the feature dims. For example, if the input features have shapes (32, 512) and (32, 64) , then in_dims = [512, 64] . required out_dim int The output feature dim. 1024 Source code in pytorch_adapt\\layers\\randomized_dot_product.py def __init__ ( self , in_dims : List [ int ], out_dim : int = 1024 ): \"\"\" Arguments: in_dims: A list of the feature dims. For example, if the input features have shapes ```(32, 512)``` and ```(32, 64)```, then ```in_dims = [512, 64]```. out_dim: The output feature dim. \"\"\" super () . __init__ () self . in_dims = in_dims for i , d in enumerate ( in_dims ): self . register_buffer ( self . rand_mat_name ( i ), torch . randn ( d , out_dim )) self . out_dim = out_dim self . num_mats = len ( in_dims ) self . divisor = math . pow ( float ( self . out_dim ), 1.0 / self . num_mats )","title":"__init__()"},{"location":"layers/randomized_dot_product/#pytorch_adapt.layers.randomized_dot_product.RandomizedDotProduct.forward","text":"Parameters: Name Type Description Default inputs Tensor The number of inputs must be equal to the length of self.in_dims . () Source code in pytorch_adapt\\layers\\randomized_dot_product.py def forward ( self , * inputs : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: inputs: The number of inputs must be equal to the length of ```self.in_dims```. \"\"\" for i in range ( self . num_mats ): # move to device if necessary curr = inputs [ i ] self . set_rand_mat ( i , pml_cf . to_device ( self . get_rand_mat ( i ), curr , dtype = curr . dtype ) ) return_list = [ torch . mm ( inputs [ i ], self . get_rand_mat ( i )) for i in range ( self . num_mats ) ] return_tensor = return_list [ 0 ] / self . divisor for single in return_list [ 1 :]: return_tensor = return_tensor * single return return_tensor","title":"forward()"},{"location":"layers/silhouette_score/","text":"pytorch_adapt.layers.silhouette_score \u00b6 SilhouetteScore \u00b6 A PyTorch implementation of the silhouette score","title":"SilhouetteScore"},{"location":"layers/silhouette_score/#pytorch_adapt.layers.silhouette_score","text":"","title":"silhouette_score"},{"location":"layers/silhouette_score/#pytorch_adapt.layers.silhouette_score.SilhouetteScore","text":"A PyTorch implementation of the silhouette score","title":"SilhouetteScore"},{"location":"layers/sliced_wasserstein/","text":"pytorch_adapt.layers.sliced_wasserstein \u00b6 SlicedWasserstein \u00b6 Implementation of the loss used in Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation __init__ ( self , m = 128 ) special \u00b6 Parameters: Name Type Description Default m int The dimensionality to project to. 128 Source code in pytorch_adapt\\layers\\sliced_wasserstein.py def __init__ ( self , m : int = 128 ): \"\"\" Arguments: m: The dimensionality to project to. \"\"\" super () . __init__ () self . m = 128 forward ( self , x , y ) \u00b6 Parameters: Name Type Description Default x Tensor a batch of class predictions required y Tensor the other batch of class predictions required Returns: Type Description Tensor The discrepancy between the two batches of class predictions. Source code in pytorch_adapt\\layers\\sliced_wasserstein.py def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: a batch of class predictions y: the other batch of class predictions Returns: The discrepancy between the two batches of class predictions. \"\"\" d = x . shape [ 1 ] proj = torch . randn ( d , self . m , device = x . device ) proj = torch . nn . functional . normalize ( proj , dim = 0 ) x = torch . matmul ( x , proj ) y = torch . matmul ( y , proj ) x , _ = torch . sort ( x , dim = 0 ) y , _ = torch . sort ( y , dim = 0 ) return torch . mean (( x - y ) ** 2 )","title":"SlicedWasserstein"},{"location":"layers/sliced_wasserstein/#pytorch_adapt.layers.sliced_wasserstein","text":"","title":"sliced_wasserstein"},{"location":"layers/sliced_wasserstein/#pytorch_adapt.layers.sliced_wasserstein.SlicedWasserstein","text":"Implementation of the loss used in Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation","title":"SlicedWasserstein"},{"location":"layers/sliced_wasserstein/#pytorch_adapt.layers.sliced_wasserstein.SlicedWasserstein.__init__","text":"Parameters: Name Type Description Default m int The dimensionality to project to. 128 Source code in pytorch_adapt\\layers\\sliced_wasserstein.py def __init__ ( self , m : int = 128 ): \"\"\" Arguments: m: The dimensionality to project to. \"\"\" super () . __init__ () self . m = 128","title":"__init__()"},{"location":"layers/sliced_wasserstein/#pytorch_adapt.layers.sliced_wasserstein.SlicedWasserstein.forward","text":"Parameters: Name Type Description Default x Tensor a batch of class predictions required y Tensor the other batch of class predictions required Returns: Type Description Tensor The discrepancy between the two batches of class predictions. Source code in pytorch_adapt\\layers\\sliced_wasserstein.py def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: a batch of class predictions y: the other batch of class predictions Returns: The discrepancy between the two batches of class predictions. \"\"\" d = x . shape [ 1 ] proj = torch . randn ( d , self . m , device = x . device ) proj = torch . nn . functional . normalize ( proj , dim = 0 ) x = torch . matmul ( x , proj ) y = torch . matmul ( y , proj ) x , _ = torch . sort ( x , dim = 0 ) y , _ = torch . sort ( y , dim = 0 ) return torch . mean (( x - y ) ** 2 )","title":"forward()"},{"location":"layers/stochastic_linear/","text":"pytorch_adapt.layers.stochastic_linear \u00b6 StochasticLinear \u00b6 Implementation of the stochastic layer from Stochastic Classifiers for Unsupervised Domain Adaptation . In train() mode, it uses random weights and biases that are sampled from a learned normal distribution. In eval() mode, the learned mean is used. __init__ ( self , in_features , out_features , device = None , dtype = None ) special \u00b6 Parameters: Name Type Description Default in_features int size of each input sample required out_features int size of each output sample required Source code in pytorch_adapt\\layers\\stochastic_linear.py def __init__ ( self , in_features : int , out_features : int , device = None , dtype = None ): \"\"\" Arguments: in_features: size of each input sample out_features: size of each output sample \"\"\" factory_kwargs = { \"device\" : device , \"dtype\" : dtype } super () . __init__ () self . in_features = in_features self . out_features = out_features self . weight_mean = torch . nn . Parameter ( torch . rand ( in_features , out_features , ** factory_kwargs ) ) self . weight_sigma = torch . nn . Parameter ( torch . rand ( in_features , out_features , ** factory_kwargs ) ) self . bias_mean = torch . nn . Parameter ( torch . rand ( out_features , ** factory_kwargs )) self . bias_sigma = torch . nn . Parameter ( torch . rand ( out_features , ** factory_kwargs ))","title":"StochasticLinear"},{"location":"layers/stochastic_linear/#pytorch_adapt.layers.stochastic_linear","text":"","title":"stochastic_linear"},{"location":"layers/stochastic_linear/#pytorch_adapt.layers.stochastic_linear.StochasticLinear","text":"Implementation of the stochastic layer from Stochastic Classifiers for Unsupervised Domain Adaptation . In train() mode, it uses random weights and biases that are sampled from a learned normal distribution. In eval() mode, the learned mean is used.","title":"StochasticLinear"},{"location":"layers/stochastic_linear/#pytorch_adapt.layers.stochastic_linear.StochasticLinear.__init__","text":"Parameters: Name Type Description Default in_features int size of each input sample required out_features int size of each output sample required Source code in pytorch_adapt\\layers\\stochastic_linear.py def __init__ ( self , in_features : int , out_features : int , device = None , dtype = None ): \"\"\" Arguments: in_features: size of each input sample out_features: size of each output sample \"\"\" factory_kwargs = { \"device\" : device , \"dtype\" : dtype } super () . __init__ () self . in_features = in_features self . out_features = out_features self . weight_mean = torch . nn . Parameter ( torch . rand ( in_features , out_features , ** factory_kwargs ) ) self . weight_sigma = torch . nn . Parameter ( torch . rand ( in_features , out_features , ** factory_kwargs ) ) self . bias_mean = torch . nn . Parameter ( torch . rand ( out_features , ** factory_kwargs )) self . bias_sigma = torch . nn . Parameter ( torch . rand ( out_features , ** factory_kwargs ))","title":"__init__()"},{"location":"layers/sufficient_accuracy/","text":"pytorch_adapt.layers.sufficient_accuracy \u00b6 SufficientAccuracy \u00b6 Determines if a batch of logits has accuracy greater than some threshold. This can be used to control program flow. Examples: condition_fn = SufficientAccuracy ( threshold = 0.7 ) if condition_fn ( logits , labels ): ... __init__ ( self , threshold , accuracy_func = None , to_probs_func = None ) special \u00b6 Parameters: Name Type Description Default threshold float The accuracy must be greater than this for the forward pass to return True. required accuracy_func Callable[[torch.Tensor, torch.Tensor], torch.Tensor] function that takes in (to_probs_func(logits), labels) and returns accuracy. If None , then classification accuracy is used. None to_probs_func Callable[[torch.Tensor], torch.Tensor] function that processes the logits before they get passed to accuracy_func . If None , then torch.nn.Sigmoid is used None Source code in pytorch_adapt\\layers\\sufficient_accuracy.py def __init__ ( self , threshold : float , accuracy_func : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , to_probs_func : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: threshold: The accuracy must be greater than this for the forward pass to return True. accuracy_func: function that takes in ```(to_probs_func(logits), labels)``` and returns accuracy. If ```None```, then classification accuracy is used. to_probs_func: function that processes the logits before they get passed to ```accuracy_func```. If ```None```, then ```torch.nn.Sigmoid``` is used \"\"\" super () . __init__ () self . threshold = threshold self . accuracy_func = c_f . default ( accuracy_func , accuracy ) self . to_probs_func = c_f . default ( to_probs_func , torch . nn . Sigmoid ()) pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"accuracy\" , \"threshold\" ] ) forward ( self , x , labels ) \u00b6 Parameters: Name Type Description Default x Tensor logits to compute accuracy for required labels Tensor the corresponding labels required Returns: Type Description bool True if the accuracy is greater than self.threshold Source code in pytorch_adapt\\layers\\sufficient_accuracy.py def forward ( self , x : torch . Tensor , labels : torch . Tensor ) -> bool : \"\"\" Arguments: x: logits to compute accuracy for labels: the corresponding labels Returns: ```True``` if the accuracy is greater than ```self.threshold``` \"\"\" with torch . no_grad (): x = self . to_probs_func ( x ) labels = labels . type ( torch . int ) self . accuracy = self . accuracy_func ( x , labels ) . item () return self . accuracy > self . threshold","title":"SufficientAccuracy"},{"location":"layers/sufficient_accuracy/#pytorch_adapt.layers.sufficient_accuracy","text":"","title":"sufficient_accuracy"},{"location":"layers/sufficient_accuracy/#pytorch_adapt.layers.sufficient_accuracy.SufficientAccuracy","text":"Determines if a batch of logits has accuracy greater than some threshold. This can be used to control program flow. Examples: condition_fn = SufficientAccuracy ( threshold = 0.7 ) if condition_fn ( logits , labels ): ...","title":"SufficientAccuracy"},{"location":"layers/sufficient_accuracy/#pytorch_adapt.layers.sufficient_accuracy.SufficientAccuracy.__init__","text":"Parameters: Name Type Description Default threshold float The accuracy must be greater than this for the forward pass to return True. required accuracy_func Callable[[torch.Tensor, torch.Tensor], torch.Tensor] function that takes in (to_probs_func(logits), labels) and returns accuracy. If None , then classification accuracy is used. None to_probs_func Callable[[torch.Tensor], torch.Tensor] function that processes the logits before they get passed to accuracy_func . If None , then torch.nn.Sigmoid is used None Source code in pytorch_adapt\\layers\\sufficient_accuracy.py def __init__ ( self , threshold : float , accuracy_func : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , to_probs_func : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: threshold: The accuracy must be greater than this for the forward pass to return True. accuracy_func: function that takes in ```(to_probs_func(logits), labels)``` and returns accuracy. If ```None```, then classification accuracy is used. to_probs_func: function that processes the logits before they get passed to ```accuracy_func```. If ```None```, then ```torch.nn.Sigmoid``` is used \"\"\" super () . __init__ () self . threshold = threshold self . accuracy_func = c_f . default ( accuracy_func , accuracy ) self . to_probs_func = c_f . default ( to_probs_func , torch . nn . Sigmoid ()) pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"accuracy\" , \"threshold\" ] )","title":"__init__()"},{"location":"layers/sufficient_accuracy/#pytorch_adapt.layers.sufficient_accuracy.SufficientAccuracy.forward","text":"Parameters: Name Type Description Default x Tensor logits to compute accuracy for required labels Tensor the corresponding labels required Returns: Type Description bool True if the accuracy is greater than self.threshold Source code in pytorch_adapt\\layers\\sufficient_accuracy.py def forward ( self , x : torch . Tensor , labels : torch . Tensor ) -> bool : \"\"\" Arguments: x: logits to compute accuracy for labels: the corresponding labels Returns: ```True``` if the accuracy is greater than ```self.threshold``` \"\"\" with torch . no_grad (): x = self . to_probs_func ( x ) labels = labels . type ( torch . int ) self . accuracy = self . accuracy_func ( x , labels ) . item () return self . accuracy > self . threshold","title":"forward()"},{"location":"layers/uniform_distribution_loss/","text":"pytorch_adapt.layers.uniform_distribution_loss \u00b6 UniformDistributionLoss \u00b6 Implementation of the confusion loss from Simultaneous Deep Transfer Across Domains and Tasks .","title":"UniformDistributionLoss"},{"location":"layers/uniform_distribution_loss/#pytorch_adapt.layers.uniform_distribution_loss","text":"","title":"uniform_distribution_loss"},{"location":"layers/uniform_distribution_loss/#pytorch_adapt.layers.uniform_distribution_loss.UniformDistributionLoss","text":"Implementation of the confusion loss from Simultaneous Deep Transfer Across Domains and Tasks .","title":"UniformDistributionLoss"},{"location":"layers/vat_loss/","text":"pytorch_adapt.layers.vat_loss \u00b6 VATLoss \u00b6 Implementation of the loss used in Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning A DIRT-T Approach to Unsupervised Domain Adaptation __init__ ( self , num_power_iterations = 1 , xi = 1e-06 , epsilon = 8.0 ) special \u00b6 Parameters: Name Type Description Default num_power_iterations int The number of iterations for computing the approximation of the adversarial perturbation. 1 xi float The L2 norm of the the generated noise which is used in the process of creating the perturbation. 1e-06 epsilon float The L2 norm of the generated perturbation. 8.0 Source code in pytorch_adapt\\layers\\vat_loss.py def __init__ ( self , num_power_iterations : int = 1 , xi : float = 1e-6 , epsilon : float = 8.0 ): \"\"\" Arguments: num_power_iterations: The number of iterations for computing the approximation of the adversarial perturbation. xi: The L2 norm of the the generated noise which is used in the process of creating the perturbation. epsilon: The L2 norm of the generated perturbation. \"\"\" super () . __init__ () self . num_power_iterations = num_power_iterations self . xi = xi self . epsilon = epsilon self . kl_div = torch . nn . KLDivLoss ( reduction = \"batchmean\" ) pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"num_power_iterations\" , \"xi\" , \"epsilon\" ] ) forward ( self , imgs , logits , model ) \u00b6 Parameters: Name Type Description Default imgs Tensor The input to the model required logits Tensor The model's logits computed from imgs required model Module The aforementioned model required Source code in pytorch_adapt\\layers\\vat_loss.py def forward ( self , imgs : torch . Tensor , logits : torch . Tensor , model : torch . nn . Module ) -> torch . Tensor : \"\"\" Arguments: imgs: The input to the model logits: The model's logits computed from ```imgs``` model: The aforementioned model \"\"\" logits = logits . detach () model . apply ( c_f . set_layers_mode ( \"eval\" , c_f . batchnorm_types ())) perturbation = self . get_perturbation ( imgs , logits , model ) new_logits = model ( imgs + perturbation ) preds = F . softmax ( logits , dim = 1 ) new_preds = F . log_softmax ( new_logits , dim = 1 ) model . apply ( c_f . set_layers_mode ( \"train\" , c_f . batchnorm_types ())) return self . kl_div ( new_preds , preds )","title":"VATLoss"},{"location":"layers/vat_loss/#pytorch_adapt.layers.vat_loss","text":"","title":"vat_loss"},{"location":"layers/vat_loss/#pytorch_adapt.layers.vat_loss.VATLoss","text":"Implementation of the loss used in Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning A DIRT-T Approach to Unsupervised Domain Adaptation","title":"VATLoss"},{"location":"layers/vat_loss/#pytorch_adapt.layers.vat_loss.VATLoss.__init__","text":"Parameters: Name Type Description Default num_power_iterations int The number of iterations for computing the approximation of the adversarial perturbation. 1 xi float The L2 norm of the the generated noise which is used in the process of creating the perturbation. 1e-06 epsilon float The L2 norm of the generated perturbation. 8.0 Source code in pytorch_adapt\\layers\\vat_loss.py def __init__ ( self , num_power_iterations : int = 1 , xi : float = 1e-6 , epsilon : float = 8.0 ): \"\"\" Arguments: num_power_iterations: The number of iterations for computing the approximation of the adversarial perturbation. xi: The L2 norm of the the generated noise which is used in the process of creating the perturbation. epsilon: The L2 norm of the generated perturbation. \"\"\" super () . __init__ () self . num_power_iterations = num_power_iterations self . xi = xi self . epsilon = epsilon self . kl_div = torch . nn . KLDivLoss ( reduction = \"batchmean\" ) pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"num_power_iterations\" , \"xi\" , \"epsilon\" ] )","title":"__init__()"},{"location":"layers/vat_loss/#pytorch_adapt.layers.vat_loss.VATLoss.forward","text":"Parameters: Name Type Description Default imgs Tensor The input to the model required logits Tensor The model's logits computed from imgs required model Module The aforementioned model required Source code in pytorch_adapt\\layers\\vat_loss.py def forward ( self , imgs : torch . Tensor , logits : torch . Tensor , model : torch . nn . Module ) -> torch . Tensor : \"\"\" Arguments: imgs: The input to the model logits: The model's logits computed from ```imgs``` model: The aforementioned model \"\"\" logits = logits . detach () model . apply ( c_f . set_layers_mode ( \"eval\" , c_f . batchnorm_types ())) perturbation = self . get_perturbation ( imgs , logits , model ) new_logits = model ( imgs + perturbation ) preds = F . softmax ( logits , dim = 1 ) new_preds = F . log_softmax ( new_logits , dim = 1 ) model . apply ( c_f . set_layers_mode ( \"train\" , c_f . batchnorm_types ())) return self . kl_div ( new_preds , preds )","title":"forward()"},{"location":"meta_validators/","text":"","title":"Meta Validators"},{"location":"meta_validators/forward_only_validator/","text":"pytorch_adapt.meta_validators.forward_only_validator \u00b6 ForwardOnlyValidator \u00b6 This is basically a pass-through function. It returns the best score and best epoch that is returned by the inner adapter. run ( self , adapter , ** kwargs ) \u00b6 Parameters: Name Type Description Default adapter the framework-wrapped adapter. required **kwargs keyword arguments to be passed into adapter.run() {} Returns: Type Description Tuple[float, int] the best score and best epoch Source code in pytorch_adapt\\meta_validators\\forward_only_validator.py def run ( self , adapter , ** kwargs ) -> Tuple [ float , int ]: \"\"\" Arguments: adapter: the framework-wrapped adapter. **kwargs: keyword arguments to be passed into adapter.run() Returns: the best score and best epoch \"\"\" if \"validator\" not in kwargs : raise KeyError ( \"An adapter validator is required when using ForwardOnlyValidator\" ) return adapter . run ( ** kwargs )","title":"ForwardOnlyValidator"},{"location":"meta_validators/forward_only_validator/#pytorch_adapt.meta_validators.forward_only_validator","text":"","title":"forward_only_validator"},{"location":"meta_validators/forward_only_validator/#pytorch_adapt.meta_validators.forward_only_validator.ForwardOnlyValidator","text":"This is basically a pass-through function. It returns the best score and best epoch that is returned by the inner adapter.","title":"ForwardOnlyValidator"},{"location":"meta_validators/forward_only_validator/#pytorch_adapt.meta_validators.forward_only_validator.ForwardOnlyValidator.run","text":"Parameters: Name Type Description Default adapter the framework-wrapped adapter. required **kwargs keyword arguments to be passed into adapter.run() {} Returns: Type Description Tuple[float, int] the best score and best epoch Source code in pytorch_adapt\\meta_validators\\forward_only_validator.py def run ( self , adapter , ** kwargs ) -> Tuple [ float , int ]: \"\"\" Arguments: adapter: the framework-wrapped adapter. **kwargs: keyword arguments to be passed into adapter.run() Returns: the best score and best epoch \"\"\" if \"validator\" not in kwargs : raise KeyError ( \"An adapter validator is required when using ForwardOnlyValidator\" ) return adapter . run ( ** kwargs )","title":"run()"},{"location":"meta_validators/reverse_validator/","text":"pytorch_adapt.meta_validators.reverse_validator \u00b6 ReverseValidator \u00b6 Reverse validation consists of three steps. Train a model on the labeled source and unlabeled target Use the trained model to create pseudolabels for the target dataset. Train a new model on the labeled target and \"unlabeled\" source. The final score is the accuracy of the model from step 3. run ( self , forward_adapter , reverse_adapter , forward_kwargs , reverse_kwargs , pl_dataloader_creator = None ) \u00b6 Parameters: Name Type Description Default forward_adapter the framework-wrapped adapter for step 1. required reverse_adapter the framework-wrapped adapter for step 3. required forward_kwargs a dict of keyword arguments to be passed to forward_adapter.run() required reverse_kwargs a dict of keyword arguments to be passed to reverse_adapter.run() required pl_dataloader_creator An optional DataloaderCreator for obtaining pseudolabels in step 2. None Returns: Type Description Tuple[float, int] the best score and best epoch of the reverse model Source code in pytorch_adapt\\meta_validators\\reverse_validator.py def run ( self , forward_adapter , reverse_adapter , forward_kwargs , reverse_kwargs , pl_dataloader_creator = None , ) -> Tuple [ float , int ]: \"\"\" Arguments: forward_adapter: the framework-wrapped adapter for step 1. reverse_adapter: the framework-wrapped adapter for step 3. forward_kwargs: a dict of keyword arguments to be passed to forward_adapter.run() reverse_kwargs: a dict of keyword arguments to be passed to reverse_adapter.run() pl_dataloader_creator: An optional DataloaderCreator for obtaining pseudolabels in step 2. Returns: the best score and best epoch of the reverse model \"\"\" if \"datasets\" in reverse_kwargs : raise KeyError ( \"'datasets' should not be in reverse_kwargs because the reverse datasets will be pseudo labeled.\" ) if \"validator\" not in reverse_kwargs : raise KeyError ( \"reverse_kwargs must include 'validator'\" ) forward_adapter . run ( ** forward_kwargs ) if all ( x in forward_kwargs for x in [ \"validator\" , \"saver\" ]): forward_kwargs [ \"saver\" ] . load_adapter ( forward_adapter . adapter , \"best\" ) datasets = forward_kwargs [ \"datasets\" ] pl_dataloader_creator = c_f . default ( pl_dataloader_creator , DataloaderCreator , { \"all_val\" : True } ) d = {} d [ \"src_train\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_train\" , pl_dataloader_creator ) d [ \"src_val\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_val\" , pl_dataloader_creator ) d [ \"target_train\" ] = TargetDataset ( datasets [ \"src_train\" ] . dataset ) d [ \"target_val\" ] = TargetDataset ( datasets [ \"src_val\" ] . dataset ) d [ \"train\" ] = CombinedSourceAndTargetDataset ( d [ \"src_train\" ], d [ \"target_train\" ]) self . pseudo_train = d [ \"src_train\" ] self . pseudo_val = d [ \"src_val\" ] reverse_kwargs [ \"datasets\" ] = d return reverse_adapter . run ( ** reverse_kwargs )","title":"ReverseValidator"},{"location":"meta_validators/reverse_validator/#pytorch_adapt.meta_validators.reverse_validator","text":"","title":"reverse_validator"},{"location":"meta_validators/reverse_validator/#pytorch_adapt.meta_validators.reverse_validator.ReverseValidator","text":"Reverse validation consists of three steps. Train a model on the labeled source and unlabeled target Use the trained model to create pseudolabels for the target dataset. Train a new model on the labeled target and \"unlabeled\" source. The final score is the accuracy of the model from step 3.","title":"ReverseValidator"},{"location":"meta_validators/reverse_validator/#pytorch_adapt.meta_validators.reverse_validator.ReverseValidator.run","text":"Parameters: Name Type Description Default forward_adapter the framework-wrapped adapter for step 1. required reverse_adapter the framework-wrapped adapter for step 3. required forward_kwargs a dict of keyword arguments to be passed to forward_adapter.run() required reverse_kwargs a dict of keyword arguments to be passed to reverse_adapter.run() required pl_dataloader_creator An optional DataloaderCreator for obtaining pseudolabels in step 2. None Returns: Type Description Tuple[float, int] the best score and best epoch of the reverse model Source code in pytorch_adapt\\meta_validators\\reverse_validator.py def run ( self , forward_adapter , reverse_adapter , forward_kwargs , reverse_kwargs , pl_dataloader_creator = None , ) -> Tuple [ float , int ]: \"\"\" Arguments: forward_adapter: the framework-wrapped adapter for step 1. reverse_adapter: the framework-wrapped adapter for step 3. forward_kwargs: a dict of keyword arguments to be passed to forward_adapter.run() reverse_kwargs: a dict of keyword arguments to be passed to reverse_adapter.run() pl_dataloader_creator: An optional DataloaderCreator for obtaining pseudolabels in step 2. Returns: the best score and best epoch of the reverse model \"\"\" if \"datasets\" in reverse_kwargs : raise KeyError ( \"'datasets' should not be in reverse_kwargs because the reverse datasets will be pseudo labeled.\" ) if \"validator\" not in reverse_kwargs : raise KeyError ( \"reverse_kwargs must include 'validator'\" ) forward_adapter . run ( ** forward_kwargs ) if all ( x in forward_kwargs for x in [ \"validator\" , \"saver\" ]): forward_kwargs [ \"saver\" ] . load_adapter ( forward_adapter . adapter , \"best\" ) datasets = forward_kwargs [ \"datasets\" ] pl_dataloader_creator = c_f . default ( pl_dataloader_creator , DataloaderCreator , { \"all_val\" : True } ) d = {} d [ \"src_train\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_train\" , pl_dataloader_creator ) d [ \"src_val\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_val\" , pl_dataloader_creator ) d [ \"target_train\" ] = TargetDataset ( datasets [ \"src_train\" ] . dataset ) d [ \"target_val\" ] = TargetDataset ( datasets [ \"src_val\" ] . dataset ) d [ \"train\" ] = CombinedSourceAndTargetDataset ( d [ \"src_train\" ], d [ \"target_train\" ]) self . pseudo_train = d [ \"src_train\" ] self . pseudo_val = d [ \"src_val\" ] reverse_kwargs [ \"datasets\" ] = d return reverse_adapter . run ( ** reverse_kwargs )","title":"run()"},{"location":"models/","text":"","title":"Models"},{"location":"models/classifier/","text":"pytorch_adapt.models.classifier \u00b6 Classifier \u00b6 forward ( self , x ) \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in pytorch_adapt\\models\\classifier.py def forward ( self , x ): return self . net ( x )","title":"Classifier"},{"location":"models/classifier/#pytorch_adapt.models.classifier","text":"","title":"classifier"},{"location":"models/classifier/#pytorch_adapt.models.classifier.Classifier","text":"","title":"Classifier"},{"location":"models/classifier/#pytorch_adapt.models.classifier.Classifier.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in pytorch_adapt\\models\\classifier.py def forward ( self , x ): return self . net ( x )","title":"forward()"},{"location":"models/discriminator/","text":"pytorch_adapt.models.discriminator \u00b6 Discriminator \u00b6 forward ( self , x ) \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in pytorch_adapt\\models\\discriminator.py def forward ( self , x ): return self . net ( x ) . squeeze ( 1 )","title":"Discriminator"},{"location":"models/discriminator/#pytorch_adapt.models.discriminator","text":"","title":"discriminator"},{"location":"models/discriminator/#pytorch_adapt.models.discriminator.Discriminator","text":"","title":"Discriminator"},{"location":"models/discriminator/#pytorch_adapt.models.discriminator.Discriminator.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in pytorch_adapt\\models\\discriminator.py def forward ( self , x ): return self . net ( x ) . squeeze ( 1 )","title":"forward()"},{"location":"models/mnist/","text":"pytorch_adapt.models.mnist \u00b6 MNISTFeatures \u00b6 forward ( self , x ) \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in pytorch_adapt\\models\\mnist.py def forward ( self , x ): x = self . conv1 ( x ) x = F . relu ( x ) x = F . max_pool2d ( x , kernel_size = 2 , stride = 2 ) x = self . conv2 ( x ) x = F . relu ( x ) x = F . max_pool2d ( x , kernel_size = 2 , stride = 2 ) x = torch . flatten ( x , start_dim = 1 ) return self . fc ( x )","title":"MNISTFeatures"},{"location":"models/mnist/#pytorch_adapt.models.mnist","text":"","title":"mnist"},{"location":"models/mnist/#pytorch_adapt.models.mnist.MNISTFeatures","text":"","title":"MNISTFeatures"},{"location":"models/mnist/#pytorch_adapt.models.mnist.MNISTFeatures.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in pytorch_adapt\\models\\mnist.py def forward ( self , x ): x = self . conv1 ( x ) x = F . relu ( x ) x = F . max_pool2d ( x , kernel_size = 2 , stride = 2 ) x = self . conv2 ( x ) x = F . relu ( x ) x = F . max_pool2d ( x , kernel_size = 2 , stride = 2 ) x = torch . flatten ( x , start_dim = 1 ) return self . fc ( x )","title":"forward()"},{"location":"utils/","text":"","title":"Utils"},{"location":"validators/","text":"","title":"Validators"},{"location":"validators/accuracy_validator/","text":"pytorch_adapt.validators.accuracy_validator \u00b6 AccuracyValidator \u00b6 Returns accuracy using the torchmetrics accuracy function","title":"AccuracyValidator"},{"location":"validators/accuracy_validator/#pytorch_adapt.validators.accuracy_validator","text":"","title":"accuracy_validator"},{"location":"validators/accuracy_validator/#pytorch_adapt.validators.accuracy_validator.AccuracyValidator","text":"Returns accuracy using the torchmetrics accuracy function","title":"AccuracyValidator"},{"location":"validators/base_validator/","text":"pytorch_adapt.validators.base_validator \u00b6","title":"BaseValidator"},{"location":"validators/base_validator/#pytorch_adapt.validators.base_validator","text":"","title":"base_validator"},{"location":"validators/deep_embedded_validator/","text":"pytorch_adapt.validators.deep_embedded_validator \u00b6 DeepEmbeddedValidator \u00b6 Implementation of Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation","title":"DeepEmbeddedValidator"},{"location":"validators/deep_embedded_validator/#pytorch_adapt.validators.deep_embedded_validator","text":"","title":"deep_embedded_validator"},{"location":"validators/deep_embedded_validator/#pytorch_adapt.validators.deep_embedded_validator.DeepEmbeddedValidator","text":"Implementation of Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation","title":"DeepEmbeddedValidator"},{"location":"validators/diversity_validator/","text":"pytorch_adapt.validators.diversity_validator \u00b6 DiversityValidator \u00b6 Returns the negative of the diversity of all logits.","title":"DiversityValidator"},{"location":"validators/diversity_validator/#pytorch_adapt.validators.diversity_validator","text":"","title":"diversity_validator"},{"location":"validators/diversity_validator/#pytorch_adapt.validators.diversity_validator.DiversityValidator","text":"Returns the negative of the diversity of all logits.","title":"DiversityValidator"},{"location":"validators/entropy_validator/","text":"pytorch_adapt.validators.entropy_validator \u00b6 EntropyValidator \u00b6 Returns the negative of the entropy of all logits.","title":"EntropyValidator"},{"location":"validators/entropy_validator/#pytorch_adapt.validators.entropy_validator","text":"","title":"entropy_validator"},{"location":"validators/entropy_validator/#pytorch_adapt.validators.entropy_validator.EntropyValidator","text":"Returns the negative of the entropy of all logits.","title":"EntropyValidator"},{"location":"validators/error_validator/","text":"pytorch_adapt.validators.error_validator \u00b6 ErrorValidator \u00b6 Returns -(1-accuracy)","title":"ErrorValidator"},{"location":"validators/error_validator/#pytorch_adapt.validators.error_validator","text":"","title":"error_validator"},{"location":"validators/error_validator/#pytorch_adapt.validators.error_validator.ErrorValidator","text":"Returns -(1-accuracy)","title":"ErrorValidator"},{"location":"validators/snd_validator/","text":"pytorch_adapt.validators.snd_validator \u00b6 SNDValidator \u00b6 Implementation of Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density","title":"SNDValidator"},{"location":"validators/snd_validator/#pytorch_adapt.validators.snd_validator","text":"","title":"snd_validator"},{"location":"validators/snd_validator/#pytorch_adapt.validators.snd_validator.SNDValidator","text":"Implementation of Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density","title":"SNDValidator"},{"location":"weighters/","text":"Weighters \u00b6 Weighters multiply losses by scalar values, and then reduce the losses to a single value on which you call .backward() . For example: import torch from pytorch_adapt.weighters import MeanWeighter weighter = MeanWeighter ( weights = { \"y\" : 2.3 }) logits = torch . randn ( 32 , 512 ) labels = torch . randint ( 0 , 10 , size = ( 32 ,)) x = torch . nn . functional . cross_entropy ( logits , labels ) y = torch . norm ( logits ) # y will by multiplied by 2.3 # x wasn't given a weight, # so it gets multiplied by the default value of 1. loss , components = weighter ({ \"x\" : x , \"y\" : y }) loss . backward ()","title":"Weighters"},{"location":"weighters/#weighters","text":"Weighters multiply losses by scalar values, and then reduce the losses to a single value on which you call .backward() . For example: import torch from pytorch_adapt.weighters import MeanWeighter weighter = MeanWeighter ( weights = { \"y\" : 2.3 }) logits = torch . randn ( 32 , 512 ) labels = torch . randint ( 0 , 10 , size = ( 32 ,)) x = torch . nn . functional . cross_entropy ( logits , labels ) y = torch . norm ( logits ) # y will by multiplied by 2.3 # x wasn't given a weight, # so it gets multiplied by the default value of 1. loss , components = weighter ({ \"x\" : x , \"y\" : y }) loss . backward ()","title":"Weighters"},{"location":"weighters/base_weighter/","text":"pytorch_adapt.weighters.base_weighter \u00b6 BaseWeighter \u00b6 Multiplies losses by scalar values, and then reduces them to a single value. __call__ ( self , loss_dict ) special \u00b6 Parameters: Name Type Description Default loss_dict Dict[str, torch.Tensor] A mapping from loss names to loss values. required Returns: Type Description Tuple[torch.Tensor, Dict[str, float]] A tuple where tuple[0] is the loss that .backward() can be called on, and tuple[1] is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. Source code in pytorch_adapt\\weighters\\base_weighter.py def __call__ ( self , loss_dict : Dict [ str , torch . Tensor ] ) -> Tuple [ torch . Tensor , Dict [ str , float ]]: \"\"\" Arguments: loss_dict: A mapping from loss names to loss values. Returns: A tuple where ```tuple[0]``` is the loss that ```.backward()``` can be called on, and ```tuple[1]``` is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. \"\"\" return weight_losses ( self . reduction , self . weights , self . scale , loss_dict ) __init__ ( self , reduction , weights = None , scale = 1 ) special \u00b6 Parameters: Name Type Description Default reduction Callable[[List[torch.Tensor]], torch.Tensor] A function that takes in a list of losses and returns a single loss value. required weights Dict[str, float] A mapping from loss names to weight values. If None , weights are assumed to be 1. None scale float A scalar that every loss gets multiplied by. 1 Source code in pytorch_adapt\\weighters\\base_weighter.py def __init__ ( self , reduction : Callable [[ List [ torch . Tensor ]], torch . Tensor ], weights : Dict [ str , float ] = None , scale : float = 1 , ): \"\"\" Arguments: reduction: A function that takes in a list of losses and returns a single loss value. weights: A mapping from loss names to weight values. If ```None```, weights are assumed to be 1. scale: A scalar that every loss gets multiplied by. \"\"\" self . reduction = reduction self . weights = c_f . default ( weights , {}) self . scale = scale pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"weights\" , \"scale\" ])","title":"BaseWeighter"},{"location":"weighters/base_weighter/#pytorch_adapt.weighters.base_weighter","text":"","title":"base_weighter"},{"location":"weighters/base_weighter/#pytorch_adapt.weighters.base_weighter.BaseWeighter","text":"Multiplies losses by scalar values, and then reduces them to a single value.","title":"BaseWeighter"},{"location":"weighters/base_weighter/#pytorch_adapt.weighters.base_weighter.BaseWeighter.__call__","text":"Parameters: Name Type Description Default loss_dict Dict[str, torch.Tensor] A mapping from loss names to loss values. required Returns: Type Description Tuple[torch.Tensor, Dict[str, float]] A tuple where tuple[0] is the loss that .backward() can be called on, and tuple[1] is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. Source code in pytorch_adapt\\weighters\\base_weighter.py def __call__ ( self , loss_dict : Dict [ str , torch . Tensor ] ) -> Tuple [ torch . Tensor , Dict [ str , float ]]: \"\"\" Arguments: loss_dict: A mapping from loss names to loss values. Returns: A tuple where ```tuple[0]``` is the loss that ```.backward()``` can be called on, and ```tuple[1]``` is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. \"\"\" return weight_losses ( self . reduction , self . weights , self . scale , loss_dict )","title":"__call__()"},{"location":"weighters/base_weighter/#pytorch_adapt.weighters.base_weighter.BaseWeighter.__init__","text":"Parameters: Name Type Description Default reduction Callable[[List[torch.Tensor]], torch.Tensor] A function that takes in a list of losses and returns a single loss value. required weights Dict[str, float] A mapping from loss names to weight values. If None , weights are assumed to be 1. None scale float A scalar that every loss gets multiplied by. 1 Source code in pytorch_adapt\\weighters\\base_weighter.py def __init__ ( self , reduction : Callable [[ List [ torch . Tensor ]], torch . Tensor ], weights : Dict [ str , float ] = None , scale : float = 1 , ): \"\"\" Arguments: reduction: A function that takes in a list of losses and returns a single loss value. weights: A mapping from loss names to weight values. If ```None```, weights are assumed to be 1. scale: A scalar that every loss gets multiplied by. \"\"\" self . reduction = reduction self . weights = c_f . default ( weights , {}) self . scale = scale pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"weights\" , \"scale\" ])","title":"__init__()"},{"location":"weighters/mean_weighter/","text":"pytorch_adapt.weighters.mean_weighter \u00b6 MeanWeighter \u00b6 Weights the losses and then returns the mean of the weighted losses.","title":"MeanWeighter"},{"location":"weighters/mean_weighter/#pytorch_adapt.weighters.mean_weighter","text":"","title":"mean_weighter"},{"location":"weighters/mean_weighter/#pytorch_adapt.weighters.mean_weighter.MeanWeighter","text":"Weights the losses and then returns the mean of the weighted losses.","title":"MeanWeighter"},{"location":"weighters/sum_weighter/","text":"pytorch_adapt.weighters.sum_weighter \u00b6 SumWeighter \u00b6 Weights the losses and then returns the sum of the weighted losses.","title":"SumWeighter"},{"location":"weighters/sum_weighter/#pytorch_adapt.weighters.sum_weighter","text":"","title":"sum_weighter"},{"location":"weighters/sum_weighter/#pytorch_adapt.weighters.sum_weighter.SumWeighter","text":"Weights the losses and then returns the sum of the weighted losses.","title":"SumWeighter"}]}