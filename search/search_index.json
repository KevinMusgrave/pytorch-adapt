{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTorch Adapt \u00b6 Google Colab Examples \u00b6 Overview \u00b6 Installation \u00b6 Pip \u00b6 pip install pytorch-adapt To get the latest dev version : pip install pytorch-adapt --pre","title":"Overview"},{"location":"#pytorch-adapt","text":"","title":"PyTorch Adapt"},{"location":"#google-colab-examples","text":"","title":"Google Colab Examples"},{"location":"#overview","text":"","title":"Overview"},{"location":"#installation","text":"","title":"Installation"},{"location":"#pip","text":"pip install pytorch-adapt To get the latest dev version : pip install pytorch-adapt --pre","title":"Pip"},{"location":"SUMMARY/","text":"Overview Datasets DomainNet MNISTM Office31 DataloaderCreator Wrappers Hooks Base Utils Meta Validators Weighters","title":"SUMMARY"},{"location":"adapters/","text":"Adapters \u00b6","title":"Adapters"},{"location":"adapters/#adapters","text":"","title":"Adapters"},{"location":"containers/","text":"Containers \u00b6","title":"Containers"},{"location":"containers/#containers","text":"","title":"Containers"},{"location":"frameworks/","text":"Frameworks \u00b6","title":"Frameworks"},{"location":"frameworks/#frameworks","text":"","title":"Frameworks"},{"location":"layers/","text":"Layers \u00b6","title":"Layers"},{"location":"layers/#layers","text":"","title":"Layers"},{"location":"meta_validators/","text":"Meta Validators \u00b6","title":"Meta Validators"},{"location":"meta_validators/#meta-validators","text":"","title":"Meta Validators"},{"location":"models/","text":"Models \u00b6","title":"Models"},{"location":"models/#models","text":"","title":"Models"},{"location":"utils/","text":"Utils \u00b6","title":"Utils"},{"location":"utils/#utils","text":"","title":"Utils"},{"location":"validators/","text":"Validators \u00b6","title":"Validators"},{"location":"validators/#validators","text":"","title":"Validators"},{"location":"datasets/","text":"Datasets \u00b6","title":"Datasets"},{"location":"datasets/#datasets","text":"","title":"Datasets"},{"location":"datasets/dataloader_creator/","text":"pytorch_adapt.datasets.dataloader_creator \u00b6 DataloaderCreator \u00b6 This is a factory class for creating dataloaders. The __call__ function takes in keyword arguments which are datasets, and outputs a dictionary of dataloaders (one dataloader for each input dataset). __call__ ( self , ** kwargs ) special \u00b6 Parameters: Name Type Description Default **kwargs keyword arguments mapping from dataset names to datasets. {} Returns: Type Description Dict[str, torch.utils.data.dataloader.DataLoader] a dictionary mapping from dataset names to dataloaders. Source code in pytorch_adapt\\datasets\\dataloader_creator.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def __call__ ( self , ** kwargs ) -> Dict [ str , DataLoader ]: \"\"\" Arguments: **kwargs: keyword arguments mapping from dataset names to datasets. Returns: a dictionary mapping from dataset names to dataloaders. \"\"\" output = {} for k , v in kwargs . items (): if self . all_train : dataloader_kwargs = self . train_kwargs elif self . all_val : dataloader_kwargs = self . val_kwargs elif k in self . train_names : dataloader_kwargs = self . train_kwargs elif k in self . val_names : dataloader_kwargs = self . val_kwargs else : raise ValueError ( f \"Dataset split name must be in {self.train_names} or {self.val_names} , or one of self.all_train or self.all_val must be true\" ) output [ k ] = torch . utils . data . DataLoader ( v , ** dataloader_kwargs ) return output __init__ ( self , train_kwargs = None , val_kwargs = None , train_names = None , val_names = None , all_train = False , all_val = False , batch_size = 32 , num_workers = 0 ) special \u00b6 Parameters: Name Type Description Default train_kwargs Dict[str, Any] The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. None val_kwargs Dict[str, Any] The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. None train_names List[str] A list of the dataset names that are used during training. None val_names List[str] A list of the dataset names that are used during validation. None all_train bool If True, then all input datasets are assumed to be for training, regardless of their names. False all_val bool If True, then all input datasets are assumed to be for validation, regardless of their names. False batch_size int The default batch_size used in train_kwargs (if not provided) and val_kwargs (if not provided) 32 num_workers int The default num_workers used in train_kwargs (if not provided) and val_kwargs (if not provided) 0 Source code in pytorch_adapt\\datasets\\dataloader_creator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , train_kwargs : Dict [ str , Any ] = None , val_kwargs : Dict [ str , Any ] = None , train_names : List [ str ] = None , val_names : List [ str ] = None , all_train : bool = False , all_val : bool = False , batch_size : int = 32 , num_workers : int = 0 , ): \"\"\" Arguments: train_kwargs: The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. val_kwargs: The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. train_names: A list of the dataset names that are used during training. val_names: A list of the dataset names that are used during validation. all_train: If True, then all input datasets are assumed to be for training, regardless of their names. all_val: If True, then all input datasets are assumed to be for validation, regardless of their names. batch_size: The default ```batch_size``` used in train_kwargs (if not provided) and val_kwargs (if not provided) num_workers: The default ```num_workers``` used in train_kwargs (if not provided) and val_kwargs (if not provided) \"\"\" self . train_kwargs = c_f . default ( train_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : True , \"drop_last\" : True , }, ) self . val_kwargs = c_f . default ( val_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : False , \"drop_last\" : False , }, ) self . train_names = c_f . default ( train_names , [ \"train\" ]) self . val_names = c_f . default ( val_names , [ \"src_train\" , \"target_train\" , \"src_val\" , \"target_val\" ] ) if not set ( self . train_names ) . isdisjoint ( self . val_names ): raise ValueError ( f \"train_names {self.train_names} must be disjoint from val_names {self.val_names} \" ) if all_train and all_val : raise ValueError ( \"all_train and all_val cannot both be True\" ) self . all_train = all_train self . all_val = all_val","title":"DataloaderCreator"},{"location":"datasets/dataloader_creator/#pytorch_adapt.datasets.dataloader_creator","text":"","title":"dataloader_creator"},{"location":"datasets/dataloader_creator/#pytorch_adapt.datasets.dataloader_creator.DataloaderCreator","text":"This is a factory class for creating dataloaders. The __call__ function takes in keyword arguments which are datasets, and outputs a dictionary of dataloaders (one dataloader for each input dataset).","title":"DataloaderCreator"},{"location":"datasets/dataloader_creator/#pytorch_adapt.datasets.dataloader_creator.DataloaderCreator.__call__","text":"Parameters: Name Type Description Default **kwargs keyword arguments mapping from dataset names to datasets. {} Returns: Type Description Dict[str, torch.utils.data.dataloader.DataLoader] a dictionary mapping from dataset names to dataloaders. Source code in pytorch_adapt\\datasets\\dataloader_creator.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def __call__ ( self , ** kwargs ) -> Dict [ str , DataLoader ]: \"\"\" Arguments: **kwargs: keyword arguments mapping from dataset names to datasets. Returns: a dictionary mapping from dataset names to dataloaders. \"\"\" output = {} for k , v in kwargs . items (): if self . all_train : dataloader_kwargs = self . train_kwargs elif self . all_val : dataloader_kwargs = self . val_kwargs elif k in self . train_names : dataloader_kwargs = self . train_kwargs elif k in self . val_names : dataloader_kwargs = self . val_kwargs else : raise ValueError ( f \"Dataset split name must be in {self.train_names} or {self.val_names} , or one of self.all_train or self.all_val must be true\" ) output [ k ] = torch . utils . data . DataLoader ( v , ** dataloader_kwargs ) return output","title":"__call__()"},{"location":"datasets/dataloader_creator/#pytorch_adapt.datasets.dataloader_creator.DataloaderCreator.__init__","text":"Parameters: Name Type Description Default train_kwargs Dict[str, Any] The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. None val_kwargs Dict[str, Any] The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. None train_names List[str] A list of the dataset names that are used during training. None val_names List[str] A list of the dataset names that are used during validation. None all_train bool If True, then all input datasets are assumed to be for training, regardless of their names. False all_val bool If True, then all input datasets are assumed to be for validation, regardless of their names. False batch_size int The default batch_size used in train_kwargs (if not provided) and val_kwargs (if not provided) 32 num_workers int The default num_workers used in train_kwargs (if not provided) and val_kwargs (if not provided) 0 Source code in pytorch_adapt\\datasets\\dataloader_creator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , train_kwargs : Dict [ str , Any ] = None , val_kwargs : Dict [ str , Any ] = None , train_names : List [ str ] = None , val_names : List [ str ] = None , all_train : bool = False , all_val : bool = False , batch_size : int = 32 , num_workers : int = 0 , ): \"\"\" Arguments: train_kwargs: The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. val_kwargs: The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. train_names: A list of the dataset names that are used during training. val_names: A list of the dataset names that are used during validation. all_train: If True, then all input datasets are assumed to be for training, regardless of their names. all_val: If True, then all input datasets are assumed to be for validation, regardless of their names. batch_size: The default ```batch_size``` used in train_kwargs (if not provided) and val_kwargs (if not provided) num_workers: The default ```num_workers``` used in train_kwargs (if not provided) and val_kwargs (if not provided) \"\"\" self . train_kwargs = c_f . default ( train_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : True , \"drop_last\" : True , }, ) self . val_kwargs = c_f . default ( val_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : False , \"drop_last\" : False , }, ) self . train_names = c_f . default ( train_names , [ \"train\" ]) self . val_names = c_f . default ( val_names , [ \"src_train\" , \"target_train\" , \"src_val\" , \"target_val\" ] ) if not set ( self . train_names ) . isdisjoint ( self . val_names ): raise ValueError ( f \"train_names {self.train_names} must be disjoint from val_names {self.val_names} \" ) if all_train and all_val : raise ValueError ( \"all_train and all_val cannot both be True\" ) self . all_train = all_train self . all_val = all_val","title":"__init__()"},{"location":"datasets/domainnet/","text":"pytorch_adapt.datasets.domainnet \u00b6 DomainNet \u00b6 A large dataset used in \"Moment Matching for Multi-Source Domain Adaptation\". It consists of 345 classes in 6 domains: clipart, infograph, painting, quickdraw, real, sketch __init__ ( self , root , domain , train , transform , ** kwargs ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 6 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 6 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" {domain} _ {name} .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 33525 , \"test\" : 14604 }[ name ], \"infograph\" : { \"train\" : 36023 , \"test\" : 15582 }[ name ], \"painting\" : { \"train\" : 50416 , \"test\" : 21850 }[ name ], \"quickdraw\" : { \"train\" : 120750 , \"test\" : 51750 }[ name ], \"real\" : { \"train\" : 120906 , \"test\" : 52041 }[ name ], \"sketch\" : { \"train\" : 48212 , \"test\" : 20916 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform DomainNet126 \u00b6 A custom train/test split of DomainNet126Full. __init__ ( self , root , domain , train , transform , ** kwargs ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 4 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" {domain} 126_ {name} .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 14962 , \"test\" : 3741 }[ name ], \"painting\" : { \"train\" : 25201 , \"test\" : 6301 }[ name ], \"real\" : { \"train\" : 56286 , \"test\" : 14072 }[ name ], \"sketch\" : { \"train\" : 19665 , \"test\" : 4917 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform DomainNet126Full \u00b6 A subset of DomainNet consisting of 126 classes and 4 domains: clipart, painting, real, sketch __init__ ( self , root , domain , transform , ** kwargs ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 4 domains required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def __init__ ( self , root : str , domain : str , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) filenames = [ f \"labeled_source_images_ {domain} \" , f \"labeled_target_images_ {domain} _1\" , f \"labeled_target_images_ {domain} _3\" , f \"unlabeled_target_images_ {domain} _1\" , f \"unlabeled_target_images_ {domain} _3\" , f \"validation_target_images_ {domain} _3\" , ] filenames = [ os . path . join ( root , \"domainnet\" , f \" {f} .txt\" ) for f in filenames ] img_dir = os . path . join ( root , \"domainnet\" ) content = OrderedDict () for f in filenames : with open ( f ) as fff : for line in fff : path , label = line . rstrip () . split ( \" \" ) content [ path ] = label self . img_paths = [ os . path . join ( img_dir , x ) for x in content . keys ()] check_img_paths ( img_dir , self . img_paths , domain ) self . labels = [ int ( x ) for x in content . values ()] self . transform = transform","title":"DomainNet"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet","text":"","title":"domainnet"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet","text":"A large dataset used in \"Moment Matching for Multi-Source Domain Adaptation\". It consists of 345 classes in 6 domains: clipart, infograph, painting, quickdraw, real, sketch","title":"DomainNet"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 6 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 6 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" {domain} _ {name} .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 33525 , \"test\" : 14604 }[ name ], \"infograph\" : { \"train\" : 36023 , \"test\" : 15582 }[ name ], \"painting\" : { \"train\" : 50416 , \"test\" : 21850 }[ name ], \"quickdraw\" : { \"train\" : 120750 , \"test\" : 51750 }[ name ], \"real\" : { \"train\" : 120906 , \"test\" : 52041 }[ name ], \"sketch\" : { \"train\" : 48212 , \"test\" : 20916 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"__init__()"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet126","text":"A custom train/test split of DomainNet126Full.","title":"DomainNet126"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet126.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 4 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" {domain} 126_ {name} .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 14962 , \"test\" : 3741 }[ name ], \"painting\" : { \"train\" : 25201 , \"test\" : 6301 }[ name ], \"real\" : { \"train\" : 56286 , \"test\" : 14072 }[ name ], \"sketch\" : { \"train\" : 19665 , \"test\" : 4917 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"__init__()"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet126Full","text":"A subset of DomainNet consisting of 126 classes and 4 domains: clipart, painting, real, sketch","title":"DomainNet126Full"},{"location":"datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet126Full.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 4 domains required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def __init__ ( self , root : str , domain : str , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) filenames = [ f \"labeled_source_images_ {domain} \" , f \"labeled_target_images_ {domain} _1\" , f \"labeled_target_images_ {domain} _3\" , f \"unlabeled_target_images_ {domain} _1\" , f \"unlabeled_target_images_ {domain} _3\" , f \"validation_target_images_ {domain} _3\" , ] filenames = [ os . path . join ( root , \"domainnet\" , f \" {f} .txt\" ) for f in filenames ] img_dir = os . path . join ( root , \"domainnet\" ) content = OrderedDict () for f in filenames : with open ( f ) as fff : for line in fff : path , label = line . rstrip () . split ( \" \" ) content [ path ] = label self . img_paths = [ os . path . join ( img_dir , x ) for x in content . keys ()] check_img_paths ( img_dir , self . img_paths , domain ) self . labels = [ int ( x ) for x in content . values ()] self . transform = transform","title":"__init__()"},{"location":"datasets/mnistm/","text":"pytorch_adapt.datasets.mnistm \u00b6 MNISTM \u00b6 The dataset used in \"Domain-Adversarial Training of Neural Networks\". It consists of colored MNIST digits. __init__ ( self , root , train , transform , ** kwargs ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/mnist_m required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\mnistm.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , root : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/mnist_m``` train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = \"MNISTM\" , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ {name} _labels.txt\" ) img_dir = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ {name} \" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_length ( self , { \"train\" : 59001 , \"test\" : 9001 }[ name ]) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"MNISTM"},{"location":"datasets/mnistm/#pytorch_adapt.datasets.mnistm","text":"","title":"mnistm"},{"location":"datasets/mnistm/#pytorch_adapt.datasets.mnistm.MNISTM","text":"The dataset used in \"Domain-Adversarial Training of Neural Networks\". It consists of colored MNIST digits.","title":"MNISTM"},{"location":"datasets/mnistm/#pytorch_adapt.datasets.mnistm.MNISTM.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/mnist_m required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\mnistm.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , root : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/mnist_m``` train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = \"MNISTM\" , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ {name} _labels.txt\" ) img_dir = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ {name} \" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_length ( self , { \"train\" : 59001 , \"test\" : 9001 }[ name ]) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"__init__()"},{"location":"datasets/office31/","text":"pytorch_adapt.datasets.office31 \u00b6 Office31 \u00b6 A custom train/test split of Office31Full. __init__ ( self , root , domain , train , transform , ** kwargs ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/office31 required domain str One of the 3 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\office31.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of the 3 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"office31\" , f \" {domain} _ {name} .txt\" ) img_dir = os . path . join ( root , \"office31\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"amazon\" : { \"train\" : 2253 , \"test\" : 564 }[ name ], \"dslr\" : { \"train\" : 398 , \"test\" : 100 }[ name ], \"webcam\" : { \"train\" : 636 , \"test\" : 159 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform Office31Full \u00b6 A small dataset consisting of 31 classes in 3 domains: amazon, dslr, webcam. __init__ ( self , root , domain , transform ) special \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/office31 required domain str One of the 3 domains required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\office31.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , root : str , domain : str , transform ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of the 3 domains transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain ) self . transform = transform self . dataset = torch_datasets . ImageFolder ( os . path . join ( root , \"office31\" , domain , \"images\" ), transform = self . transform ) check_length ( self , { \"amazon\" : 2817 , \"dslr\" : 498 , \"webcam\" : 795 }[ domain ])","title":"Office31"},{"location":"datasets/office31/#pytorch_adapt.datasets.office31","text":"","title":"office31"},{"location":"datasets/office31/#pytorch_adapt.datasets.office31.Office31","text":"A custom train/test split of Office31Full.","title":"Office31"},{"location":"datasets/office31/#pytorch_adapt.datasets.office31.Office31.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/office31 required domain str One of the 3 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\office31.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of the 3 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"office31\" , f \" {domain} _ {name} .txt\" ) img_dir = os . path . join ( root , \"office31\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"amazon\" : { \"train\" : 2253 , \"test\" : 564 }[ name ], \"dslr\" : { \"train\" : 398 , \"test\" : 100 }[ name ], \"webcam\" : { \"train\" : 636 , \"test\" : 159 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"__init__()"},{"location":"datasets/office31/#pytorch_adapt.datasets.office31.Office31Full","text":"A small dataset consisting of 31 classes in 3 domains: amazon, dslr, webcam.","title":"Office31Full"},{"location":"datasets/office31/#pytorch_adapt.datasets.office31.Office31Full.__init__","text":"Parameters: Name Type Description Default root str The dataset must be located at <root>/office31 required domain str One of the 3 domains required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\office31.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , root : str , domain : str , transform ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of the 3 domains transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain ) self . transform = transform self . dataset = torch_datasets . ImageFolder ( os . path . join ( root , \"office31\" , domain , \"images\" ), transform = self . transform ) check_length ( self , { \"amazon\" : 2817 , \"dslr\" : 498 , \"webcam\" : 795 }[ domain ])","title":"__init__()"},{"location":"datasets/wrappers/","text":"pytorch_adapt.datasets.combined_source_and_target \u00b6 CombinedSourceAndTargetDataset \u00b6 Wraps a source dataset and a target dataset. __getitem__ ( self , idx ) special \u00b6 Parameters: Name Type Description Default idx The index of the target dataset. The source index is picked randomly. required Returns: Type Description Dict[str, Any] A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 33 34 35 36 37 38 39 40 41 42 43 44 def __getitem__ ( self , idx ) -> Dict [ str , Any ]: \"\"\" Arguments: idx: The index of the target dataset. The source index is picked randomly. Returns: A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". \"\"\" target_data = self . target_dataset [ idx ] src_data = self . source_dataset [ self . get_random_src_idx ()] c_f . assert_dicts_are_disjoint ( src_data , target_data ) return { ** src_data , ** target_data } __init__ ( self , source_dataset , target_dataset ) special \u00b6 Parameters: Name Type Description Default source_dataset SourceDataset required target_dataset TargetDataset required Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 16 17 18 19 20 21 22 23 24 def __init__ ( self , source_dataset : SourceDataset , target_dataset : TargetDataset ): \"\"\" Arguments: source_dataset: target_dataset: \"\"\" self . source_dataset = source_dataset self . target_dataset = target_dataset __len__ ( self ) special \u00b6 Returns: Type Description The length of the target dataset. Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 26 27 28 29 30 31 def __len__ ( self ): \"\"\" Returns: The length of the target dataset. \"\"\" return len ( self . target_dataset ) pytorch_adapt.datasets.concat_dataset \u00b6 ConcatDataset \u00b6 Exactly the same as torch.utils.data.ConcatDataset except with a nice __repr__ function. pytorch_adapt.datasets.pseudo_labeled_dataset \u00b6 PseudoLabeledDataset \u00b6 This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) . The label returned by the wrapped dataset is discarded, and the pseudo label is returned instead. __getitem__ ( self , idx ) special \u00b6 Returns: Type Description A dictionary with keys \"src_imgs\" (the data) \"src_domain\" (the integer representing the domain) \"src_labels\" (the pseudo label) \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __getitem__ ( self , idx : int ): \"\"\" Returns: A dictionary with keys: - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the pseudo label) - \"src_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : self . pseudo_labels [ idx ], \"src_sample_idx\" : idx , } __init__ ( self , dataset , pseudo_labels , domain = 0 ) special \u00b6 Parameters: Name Type Description Default dataset Dataset The dataset to wrap required pseudo_labels List[int] The class labels that will be used instead of the labels contained in self.dataset required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py 17 18 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , dataset : Dataset , pseudo_labels : List [ int ], domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap pseudo_labels: The class labels that will be used instead of the labels contained in self.dataset domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain ) if len ( self . dataset ) != len ( pseudo_labels ): raise ValueError ( \"len(dataset) must equal len(pseudo_labels)\" ) self . pseudo_labels = pseudo_labels pytorch_adapt.datasets.source_dataset \u00b6 SourceDataset \u00b6 This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) . __getitem__ ( self , idx ) special \u00b6 Returns: Type Description Dict[str, Any] A dictionary with keys: \"src_imgs\" (the data) \"src_domain\" (the integer representing the domain) \"src_labels\" (the class label) \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\source_dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys: - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the class label) - \"src_sample_idx\" (idx) \"\"\" img , src_labels = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : src_labels , \"src_sample_idx\" : idx , } __init__ ( self , dataset , domain = 0 ) special \u00b6 Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\source_dataset.py 15 16 17 18 19 20 21 def __init__ ( self , dataset : Dataset , domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain ) pytorch_adapt.datasets.target_dataset \u00b6 TargetDataset \u00b6 This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) . __getitem__ ( self , idx ) special \u00b6 Returns: Type Description Dict[str, Any] A dictionary with keys: \"target_imgs\" (the data) \"target_domain\" (the integer representing the domain) \"target_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\target_dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys: - \"target_imgs\" (the data) - \"target_domain\" (the integer representing the domain) - \"target_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"target_imgs\" : img , \"target_domain\" : self . domain , \"target_sample_idx\" : idx , } __init__ ( self , dataset , domain = 1 ) special \u00b6 Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 1 Source code in pytorch_adapt\\datasets\\target_dataset.py 15 16 17 18 19 20 21 def __init__ ( self , dataset : Dataset , domain : int = 1 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain )","title":"Wrappers"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.combined_source_and_target","text":"","title":"combined_source_and_target"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset","text":"Wraps a source dataset and a target dataset.","title":"CombinedSourceAndTargetDataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset.__getitem__","text":"Parameters: Name Type Description Default idx The index of the target dataset. The source index is picked randomly. required Returns: Type Description Dict[str, Any] A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 33 34 35 36 37 38 39 40 41 42 43 44 def __getitem__ ( self , idx ) -> Dict [ str , Any ]: \"\"\" Arguments: idx: The index of the target dataset. The source index is picked randomly. Returns: A dictionary containing both source and target data. The source keys start with \"src\", and the target keys start with \"target\". \"\"\" target_data = self . target_dataset [ idx ] src_data = self . source_dataset [ self . get_random_src_idx ()] c_f . assert_dicts_are_disjoint ( src_data , target_data ) return { ** src_data , ** target_data }","title":"__getitem__()"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset.__init__","text":"Parameters: Name Type Description Default source_dataset SourceDataset required target_dataset TargetDataset required Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 16 17 18 19 20 21 22 23 24 def __init__ ( self , source_dataset : SourceDataset , target_dataset : TargetDataset ): \"\"\" Arguments: source_dataset: target_dataset: \"\"\" self . source_dataset = source_dataset self . target_dataset = target_dataset","title":"__init__()"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset.__len__","text":"Returns: Type Description The length of the target dataset. Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 26 27 28 29 30 31 def __len__ ( self ): \"\"\" Returns: The length of the target dataset. \"\"\" return len ( self . target_dataset )","title":"__len__()"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.concat_dataset","text":"","title":"concat_dataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.concat_dataset.ConcatDataset","text":"Exactly the same as torch.utils.data.ConcatDataset except with a nice __repr__ function.","title":"ConcatDataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.pseudo_labeled_dataset","text":"","title":"pseudo_labeled_dataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.pseudo_labeled_dataset.PseudoLabeledDataset","text":"This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) . The label returned by the wrapped dataset is discarded, and the pseudo label is returned instead.","title":"PseudoLabeledDataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.pseudo_labeled_dataset.PseudoLabeledDataset.__getitem__","text":"Returns: Type Description A dictionary with keys \"src_imgs\" (the data) \"src_domain\" (the integer representing the domain) \"src_labels\" (the pseudo label) \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __getitem__ ( self , idx : int ): \"\"\" Returns: A dictionary with keys: - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the pseudo label) - \"src_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : self . pseudo_labels [ idx ], \"src_sample_idx\" : idx , }","title":"__getitem__()"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.pseudo_labeled_dataset.PseudoLabeledDataset.__init__","text":"Parameters: Name Type Description Default dataset Dataset The dataset to wrap required pseudo_labels List[int] The class labels that will be used instead of the labels contained in self.dataset required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py 17 18 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , dataset : Dataset , pseudo_labels : List [ int ], domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap pseudo_labels: The class labels that will be used instead of the labels contained in self.dataset domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain ) if len ( self . dataset ) != len ( pseudo_labels ): raise ValueError ( \"len(dataset) must equal len(pseudo_labels)\" ) self . pseudo_labels = pseudo_labels","title":"__init__()"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.source_dataset","text":"","title":"source_dataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.source_dataset.SourceDataset","text":"This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) .","title":"SourceDataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.source_dataset.SourceDataset.__getitem__","text":"Returns: Type Description Dict[str, Any] A dictionary with keys: \"src_imgs\" (the data) \"src_domain\" (the integer representing the domain) \"src_labels\" (the class label) \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\source_dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys: - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the class label) - \"src_sample_idx\" (idx) \"\"\" img , src_labels = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : src_labels , \"src_sample_idx\" : idx , }","title":"__getitem__()"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.source_dataset.SourceDataset.__init__","text":"Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\source_dataset.py 15 16 17 18 19 20 21 def __init__ ( self , dataset : Dataset , domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain )","title":"__init__()"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.target_dataset","text":"","title":"target_dataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.target_dataset.TargetDataset","text":"This wrapper returns a dictionary, but it expects the wrapped dataset to return a tuple of (data, label) .","title":"TargetDataset"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.target_dataset.TargetDataset.__getitem__","text":"Returns: Type Description Dict[str, Any] A dictionary with keys: \"target_imgs\" (the data) \"target_domain\" (the integer representing the domain) \"target_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\target_dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys: - \"target_imgs\" (the data) - \"target_domain\" (the integer representing the domain) - \"target_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"target_imgs\" : img , \"target_domain\" : self . domain , \"target_sample_idx\" : idx , }","title":"__getitem__()"},{"location":"datasets/wrappers/#pytorch_adapt.datasets.target_dataset.TargetDataset.__init__","text":"Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 1 Source code in pytorch_adapt\\datasets\\target_dataset.py 15 16 17 18 19 20 21 def __init__ ( self , dataset : Dataset , domain : int = 1 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain )","title":"__init__()"},{"location":"hooks/","text":"Hooks \u00b6 Hooks are the main building block of this library. Every hook is a callable that takes in 2 arguments that represent the current context: A dictionary of previously computed losses. A dictionary of everything else that has been previously computed or passed in. The purpose of the context is to compute data only when necessary. For example, to compute a classification loss, a hook will need logits. If these logits are not available in the context, then they are computed, added to the context, and then used to compute the loss. If they are already in the context, then only the loss is computed.","title":"Hooks"},{"location":"hooks/#hooks","text":"Hooks are the main building block of this library. Every hook is a callable that takes in 2 arguments that represent the current context: A dictionary of previously computed losses. A dictionary of everything else that has been previously computed or passed in. The purpose of the context is to compute data only when necessary. For example, to compute a classification loss, a hook will need logits. If these logits are not available in the context, then they are computed, added to the context, and then used to compute the loss. If they are already in the context, then only the loss is computed.","title":"Hooks"},{"location":"hooks/base/","text":"pytorch_adapt.hooks.base \u00b6 BaseConditionHook \u00b6 The base class for hooks that return a boolean BaseHook \u00b6 All hooks extend BaseHook __init__ ( self , loss_prefix = '' , loss_suffix = '' , out_prefix = '' , out_suffix = '' , key_map = None ) special \u00b6 Parameters: Name Type Description Default loss_prefix str prepended to all new loss keys '' loss_suffix str appended to all new loss keys '' out_prefix str prepended to all new output keys '' out_suffix str appended to all new output keys '' key_map Dict[str, str] a mapping from input_key to new_key . For example, if key_map = {\"A\": \"B\"}, and the input dict to __call__ is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting __call__ , the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. None Source code in pytorch_adapt\\hooks\\base.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , loss_prefix : str = \"\" , loss_suffix : str = \"\" , out_prefix : str = \"\" , out_suffix : str = \"\" , key_map : Dict [ str , str ] = None , ): \"\"\" Arguments: loss_prefix: prepended to all new loss keys loss_suffix: appended to all new loss keys out_prefix: prepended to all new output keys out_suffix: appended to all new output keys key_map: a mapping from ```input_key``` to ```new_key```. For example, if key_map = {\"A\": \"B\"}, and the input dict to ```__call__``` is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting ```__call__```, the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. \"\"\" if any ( not isinstance ( x , str ) for x in [ loss_prefix , loss_suffix , out_prefix , out_suffix ] ): raise TypeError ( \"loss prefix/suffix and out prefix/suffix must be strings\" ) self . loss_prefix = loss_prefix self . loss_suffix = loss_suffix self . out_prefix = out_prefix self . out_suffix = out_suffix self . key_map = c_f . default ( key_map , {}) self . in_keys = [] _loss_keys ( self ) private \u00b6 This must be implemented by the child class Returns: Type Description List[str] The names of the losses that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py 83 84 85 86 87 88 89 90 @abstractmethod def _loss_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the losses that will be added to the context. \"\"\" pass _out_keys ( self ) private \u00b6 This must be implemented by the child class Returns: Type Description List[str] The names of the outputs that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py 98 99 100 101 102 103 104 105 @abstractmethod def _out_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the outputs that will be added to the context. \"\"\" pass call ( self , losses , inputs ) \u00b6 This must be implemented by the child class Parameters: Name Type Description Default losses Dict[str, Any] previously computed losses required inputs Dict[str, Any] holds everything else: tensors, models etc. required Returns: Type Description Union[Tuple[Dict[str, Any], Dict[str, Any]], bool] Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean Source code in pytorch_adapt\\hooks\\base.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @abstractmethod def call ( self , losses : Dict [ str , Any ], inputs : Dict [ str , Any ] ) -> Union [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]], bool ]: \"\"\" This must be implemented by the child class Arguments: losses: previously computed losses inputs: holds everything else: tensors, models etc. Returns: Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean \"\"\" pass BaseWrapperHook \u00b6 A simple wrapper for calling self.hook , which should be defined in the child's __init__ function.","title":"Base"},{"location":"hooks/base/#pytorch_adapt.hooks.base","text":"","title":"base"},{"location":"hooks/base/#pytorch_adapt.hooks.base.BaseConditionHook","text":"The base class for hooks that return a boolean","title":"BaseConditionHook"},{"location":"hooks/base/#pytorch_adapt.hooks.base.BaseHook","text":"All hooks extend BaseHook","title":"BaseHook"},{"location":"hooks/base/#pytorch_adapt.hooks.base.BaseHook.__init__","text":"Parameters: Name Type Description Default loss_prefix str prepended to all new loss keys '' loss_suffix str appended to all new loss keys '' out_prefix str prepended to all new output keys '' out_suffix str appended to all new output keys '' key_map Dict[str, str] a mapping from input_key to new_key . For example, if key_map = {\"A\": \"B\"}, and the input dict to __call__ is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting __call__ , the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. None Source code in pytorch_adapt\\hooks\\base.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , loss_prefix : str = \"\" , loss_suffix : str = \"\" , out_prefix : str = \"\" , out_suffix : str = \"\" , key_map : Dict [ str , str ] = None , ): \"\"\" Arguments: loss_prefix: prepended to all new loss keys loss_suffix: appended to all new loss keys out_prefix: prepended to all new output keys out_suffix: appended to all new output keys key_map: a mapping from ```input_key``` to ```new_key```. For example, if key_map = {\"A\": \"B\"}, and the input dict to ```__call__``` is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting ```__call__```, the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. \"\"\" if any ( not isinstance ( x , str ) for x in [ loss_prefix , loss_suffix , out_prefix , out_suffix ] ): raise TypeError ( \"loss prefix/suffix and out prefix/suffix must be strings\" ) self . loss_prefix = loss_prefix self . loss_suffix = loss_suffix self . out_prefix = out_prefix self . out_suffix = out_suffix self . key_map = c_f . default ( key_map , {}) self . in_keys = []","title":"__init__()"},{"location":"hooks/base/#pytorch_adapt.hooks.base.BaseHook._loss_keys","text":"This must be implemented by the child class Returns: Type Description List[str] The names of the losses that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py 83 84 85 86 87 88 89 90 @abstractmethod def _loss_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the losses that will be added to the context. \"\"\" pass","title":"_loss_keys()"},{"location":"hooks/base/#pytorch_adapt.hooks.base.BaseHook._out_keys","text":"This must be implemented by the child class Returns: Type Description List[str] The names of the outputs that will be added to the context. Source code in pytorch_adapt\\hooks\\base.py 98 99 100 101 102 103 104 105 @abstractmethod def _out_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the outputs that will be added to the context. \"\"\" pass","title":"_out_keys()"},{"location":"hooks/base/#pytorch_adapt.hooks.base.BaseHook.call","text":"This must be implemented by the child class Parameters: Name Type Description Default losses Dict[str, Any] previously computed losses required inputs Dict[str, Any] holds everything else: tensors, models etc. required Returns: Type Description Union[Tuple[Dict[str, Any], Dict[str, Any]], bool] Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean Source code in pytorch_adapt\\hooks\\base.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @abstractmethod def call ( self , losses : Dict [ str , Any ], inputs : Dict [ str , Any ] ) -> Union [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]], bool ]: \"\"\" This must be implemented by the child class Arguments: losses: previously computed losses inputs: holds everything else: tensors, models etc. Returns: Either a tuple of (losses, outputs) that will be merged with the input context, or a boolean \"\"\" pass","title":"call()"},{"location":"hooks/base/#pytorch_adapt.hooks.base.BaseWrapperHook","text":"A simple wrapper for calling self.hook , which should be defined in the child's __init__ function.","title":"BaseWrapperHook"},{"location":"hooks/utils/","text":"pytorch_adapt.hooks.utils \u00b6 ApplyFnHook \u00b6 Applies a function to specific values of the context. __init__ ( self , fn , apply_to , is_loss = False , ** kwargs ) special \u00b6 Parameters: Name Type Description Default fn Callable The function that will be applied to the inputs. required apply_to List[str] fn will be applied to inputs[k] for k in apply_to required is_loss bool If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. False Source code in pytorch_adapt\\hooks\\utils.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def __init__ ( self , fn : Callable , apply_to : List [ str ], is_loss : bool = False , ** kwargs ): \"\"\" Arguments: fn: The function that will be applied to the inputs. apply_to: fn will be applied to ```inputs[k]``` for k in apply_to is_loss: If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. \"\"\" super () . __init__ ( ** kwargs ) self . fn = fn self . apply_to = apply_to self . is_loss = is_loss AssertHook \u00b6 Asserts that the output keys of a hook match a specified regex string __init__ ( self , hook , allowed , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The wrapped hook required allowed str The output dictionary of hook must have keys that match the allowed regex. required Source code in pytorch_adapt\\hooks\\utils.py 306 307 308 309 310 311 312 313 314 315 316 317 def __init__ ( self , hook : BaseHook , allowed : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook allowed: The output dictionary of ```hook``` must have keys that match the ```allowed``` regex. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook if not isinstance ( allowed , str ): raise TypeError ( \"allowed must be a str\" ) self . allowed = allowed ChainHook \u00b6 Calls multiple hooks sequentially. The Nth hook receives the context accumulated through hooks 0 to N-1. __init__ ( self , * hooks , * , conditions = None , alts = None , overwrite = False , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially. () conditions List[pytorch_adapt.hooks.base.BaseConditionHook] an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. None alts List[pytorch_adapt.hooks.base.BaseHook] an optional list of hooks that will be executed when the corresponding condition hook returns False None overwrite Union[bool, List[int]] If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. False Source code in pytorch_adapt\\hooks\\utils.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , * hooks : BaseHook , conditions : List [ BaseConditionHook ] = None , alts : List [ BaseHook ] = None , overwrite : Union [ bool , List [ int ]] = False , ** kwargs , ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially. conditions: an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. alts: an optional list of hooks that will be executed when the corresponding condition hook returns False overwrite: If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . conditions = c_f . default ( conditions , [ TrueHook () for _ in range ( len ( hooks ))] ) self . alts = c_f . default ( alts , [ ZeroLossHook ( h . loss_keys , h . out_keys ) for h in self . hooks ] ) self . check_alt_keys_match_hook_keys () if not isinstance ( overwrite , ( list , bool )): raise TypeError ( \"overwrite must be a list or bool\" ) self . overwrite = overwrite self . in_keys = self . hooks [ 0 ] . in_keys EmptyHook \u00b6 Returns two empty dictionaries. FalseHook \u00b6 Returns False MultiplierHook \u00b6 Multiplies every loss by a scalar __init__ ( self , hook , m , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The losses of this hook will be multiplied by m required m float The scalar required Source code in pytorch_adapt\\hooks\\utils.py 342 343 344 345 346 347 348 349 350 def __init__ ( self , hook : BaseHook , m : float , ** kwargs ): \"\"\" Arguments: hook: The losses of this hook will be multiplied by ```m``` m: The scalar \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . m = m NotHook \u00b6 Returns the boolean negation of the wrapped hook. __init__ ( self , hook , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseConditionHook The condition hook that will be negated. required Source code in pytorch_adapt\\hooks\\utils.py 288 289 290 291 292 293 294 def __init__ ( self , hook : BaseConditionHook , ** kwargs ): \"\"\" Arguments: hook: The condition hook that will be negated. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook OnlyNewOutputsHook \u00b6 Returns only outputs that are not present in the input context. You should use this if you want to change the value of a key passed to self.hook, but not propagate that change to the outside. __init__ ( self , hook , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The hook inside which changes to the context will be allowed. required Source code in pytorch_adapt\\hooks\\utils.py 213 214 215 216 217 218 219 def __init__ ( self , hook : BaseHook , ** kwargs ): \"\"\" Arguments: hook: The hook inside which changes to the context will be allowed. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook ParallelHook \u00b6 Calls multiple hooks while keeping contexts separate. The Nth hook receives the same context as hooks 0 to N-1. All the output contexts are merged at the end. __init__ ( self , * hooks , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. () Source code in pytorch_adapt\\hooks\\utils.py 171 172 173 174 175 176 177 178 179 def __init__ ( self , * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . in_keys = c_f . join_lists ([ h . in_keys for h in self . hooks ]) RepeatHook \u00b6 Executes the wrapped hook n times. __init__ ( self , hook , n , keep_only_last = False , ** kwargs ) special \u00b6 Parameters: Name Type Description Default hook BaseHook The hook that will be executed n times required n int The number of times the hook will be executed. required keep_only_last bool If False , the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If True , then only the (losses, outputs) of the final execution will be kept. False Source code in pytorch_adapt\\hooks\\utils.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def __init__ ( self , hook : BaseHook , n : int , keep_only_last : bool = False , ** kwargs ): \"\"\" Arguments: hook: The hook that will be executed ```n``` times n: The number of times the hook will be executed. keep_only_last: If ```False```, the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If ```True```, then only the (losses, outputs) of the final execution will be kept. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . n = n self . keep_only_last = keep_only_last TrueHook \u00b6 Returns True ZeroLossHook \u00b6 Returns only 0 losses and None outputs. __init__ ( self , loss_names , out_names , ** kwargs ) special \u00b6 Parameters: Name Type Description Default loss_names List[str] The keys of the loss dictionary which will have tensor(0.) as its values. required out_names List[str] The keys of the output dictionary which will have None as its values. required Source code in pytorch_adapt\\hooks\\utils.py 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , loss_names : List [ str ], out_names : List [ str ], ** kwargs ): \"\"\" Arguments: loss_names: The keys of the loss dictionary which will have ```tensor(0.)``` as its values. out_names: The keys of the output dictionary which will have ```None``` as its values. \"\"\" super () . __init__ ( ** kwargs ) self . loss_names = loss_names self . out_names = out_names","title":"Utils"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils","text":"","title":"utils"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ApplyFnHook","text":"Applies a function to specific values of the context.","title":"ApplyFnHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ApplyFnHook.__init__","text":"Parameters: Name Type Description Default fn Callable The function that will be applied to the inputs. required apply_to List[str] fn will be applied to inputs[k] for k in apply_to required is_loss bool If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. False Source code in pytorch_adapt\\hooks\\utils.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def __init__ ( self , fn : Callable , apply_to : List [ str ], is_loss : bool = False , ** kwargs ): \"\"\" Arguments: fn: The function that will be applied to the inputs. apply_to: fn will be applied to ```inputs[k]``` for k in apply_to is_loss: If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. \"\"\" super () . __init__ ( ** kwargs ) self . fn = fn self . apply_to = apply_to self . is_loss = is_loss","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.AssertHook","text":"Asserts that the output keys of a hook match a specified regex string","title":"AssertHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.AssertHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The wrapped hook required allowed str The output dictionary of hook must have keys that match the allowed regex. required Source code in pytorch_adapt\\hooks\\utils.py 306 307 308 309 310 311 312 313 314 315 316 317 def __init__ ( self , hook : BaseHook , allowed : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook allowed: The output dictionary of ```hook``` must have keys that match the ```allowed``` regex. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook if not isinstance ( allowed , str ): raise TypeError ( \"allowed must be a str\" ) self . allowed = allowed","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ChainHook","text":"Calls multiple hooks sequentially. The Nth hook receives the context accumulated through hooks 0 to N-1.","title":"ChainHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ChainHook.__init__","text":"Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially. () conditions List[pytorch_adapt.hooks.base.BaseConditionHook] an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. None alts List[pytorch_adapt.hooks.base.BaseHook] an optional list of hooks that will be executed when the corresponding condition hook returns False None overwrite Union[bool, List[int]] If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. False Source code in pytorch_adapt\\hooks\\utils.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , * hooks : BaseHook , conditions : List [ BaseConditionHook ] = None , alts : List [ BaseHook ] = None , overwrite : Union [ bool , List [ int ]] = False , ** kwargs , ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially. conditions: an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. alts: an optional list of hooks that will be executed when the corresponding condition hook returns False overwrite: If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . conditions = c_f . default ( conditions , [ TrueHook () for _ in range ( len ( hooks ))] ) self . alts = c_f . default ( alts , [ ZeroLossHook ( h . loss_keys , h . out_keys ) for h in self . hooks ] ) self . check_alt_keys_match_hook_keys () if not isinstance ( overwrite , ( list , bool )): raise TypeError ( \"overwrite must be a list or bool\" ) self . overwrite = overwrite self . in_keys = self . hooks [ 0 ] . in_keys","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.EmptyHook","text":"Returns two empty dictionaries.","title":"EmptyHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.FalseHook","text":"Returns False","title":"FalseHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.MultiplierHook","text":"Multiplies every loss by a scalar","title":"MultiplierHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.MultiplierHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The losses of this hook will be multiplied by m required m float The scalar required Source code in pytorch_adapt\\hooks\\utils.py 342 343 344 345 346 347 348 349 350 def __init__ ( self , hook : BaseHook , m : float , ** kwargs ): \"\"\" Arguments: hook: The losses of this hook will be multiplied by ```m``` m: The scalar \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . m = m","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.NotHook","text":"Returns the boolean negation of the wrapped hook.","title":"NotHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.NotHook.__init__","text":"Parameters: Name Type Description Default hook BaseConditionHook The condition hook that will be negated. required Source code in pytorch_adapt\\hooks\\utils.py 288 289 290 291 292 293 294 def __init__ ( self , hook : BaseConditionHook , ** kwargs ): \"\"\" Arguments: hook: The condition hook that will be negated. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.OnlyNewOutputsHook","text":"Returns only outputs that are not present in the input context. You should use this if you want to change the value of a key passed to self.hook, but not propagate that change to the outside.","title":"OnlyNewOutputsHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.OnlyNewOutputsHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The hook inside which changes to the context will be allowed. required Source code in pytorch_adapt\\hooks\\utils.py 213 214 215 216 217 218 219 def __init__ ( self , hook : BaseHook , ** kwargs ): \"\"\" Arguments: hook: The hook inside which changes to the context will be allowed. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ParallelHook","text":"Calls multiple hooks while keeping contexts separate. The Nth hook receives the same context as hooks 0 to N-1. All the output contexts are merged at the end.","title":"ParallelHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ParallelHook.__init__","text":"Parameters: Name Type Description Default hooks BaseHook a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. () Source code in pytorch_adapt\\hooks\\utils.py 171 172 173 174 175 176 177 178 179 def __init__ ( self , * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: hooks: a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . in_keys = c_f . join_lists ([ h . in_keys for h in self . hooks ])","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.RepeatHook","text":"Executes the wrapped hook n times.","title":"RepeatHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.RepeatHook.__init__","text":"Parameters: Name Type Description Default hook BaseHook The hook that will be executed n times required n int The number of times the hook will be executed. required keep_only_last bool If False , the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If True , then only the (losses, outputs) of the final execution will be kept. False Source code in pytorch_adapt\\hooks\\utils.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def __init__ ( self , hook : BaseHook , n : int , keep_only_last : bool = False , ** kwargs ): \"\"\" Arguments: hook: The hook that will be executed ```n``` times n: The number of times the hook will be executed. keep_only_last: If ```False```, the (losses, outputs) from each execution will be accumulated, and the keys will have the iteration number appended. If ```True```, then only the (losses, outputs) of the final execution will be kept. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . n = n self . keep_only_last = keep_only_last","title":"__init__()"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.TrueHook","text":"Returns True","title":"TrueHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ZeroLossHook","text":"Returns only 0 losses and None outputs.","title":"ZeroLossHook"},{"location":"hooks/utils/#pytorch_adapt.hooks.utils.ZeroLossHook.__init__","text":"Parameters: Name Type Description Default loss_names List[str] The keys of the loss dictionary which will have tensor(0.) as its values. required out_names List[str] The keys of the output dictionary which will have None as its values. required Source code in pytorch_adapt\\hooks\\utils.py 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , loss_names : List [ str ], out_names : List [ str ], ** kwargs ): \"\"\" Arguments: loss_names: The keys of the loss dictionary which will have ```tensor(0.)``` as its values. out_names: The keys of the output dictionary which will have ```None``` as its values. \"\"\" super () . __init__ ( ** kwargs ) self . loss_names = loss_names self . out_names = out_names","title":"__init__()"},{"location":"meta_validators/","text":"pytorch_adapt.meta_validators.forward_only_validator \u00b6 ForwardOnlyValidator \u00b6 This is basically a pass-through function. It returns the best score and best epoch that is returned by the inner adapter. run ( self , adapter , ** kwargs ) \u00b6 Parameters: Name Type Description Default adapter the framework-wrapped adapter. required **kwargs keyword arguments to be passed into adapter.run() {} Returns: Type Description Tuple[float, int] the best score and best epoch Source code in pytorch_adapt\\meta_validators\\forward_only_validator.py 11 12 13 14 15 16 17 18 19 20 21 22 23 def run ( self , adapter , ** kwargs ) -> Tuple [ float , int ]: \"\"\" Arguments: adapter: the framework-wrapped adapter. **kwargs: keyword arguments to be passed into adapter.run() Returns: the best score and best epoch \"\"\" if \"validator\" not in kwargs : raise KeyError ( \"An adapter validator is required when using ForwardOnlyValidator\" ) return adapter . run ( ** kwargs ) pytorch_adapt.meta_validators.reverse_validator \u00b6 ReverseValidator \u00b6 Reverse validation consists of three steps. Train a model on the labeled source and unlabeled target Use the trained model to create pseudolabels for the target dataset. Train a new model on the labeled target and \"unlabeled\" source. The final score is the accuracy of the model from step 3. run ( self , forward_adapter , reverse_adapter , forward_kwargs , reverse_kwargs , pl_dataloader_creator = None ) \u00b6 Parameters: Name Type Description Default forward_adapter the framework-wrapped adapter for step 1. required reverse_adapter the framework-wrapped adapter for step 3. required forward_kwargs a dict of keyword arguments to be passed to forward_adapter.run() required reverse_kwargs a dict of keyword arguments to be passed to reverse_adapter.run() required pl_dataloader_creator An optional DataloaderCreator for obtaining pseudolabels in step 2. None Returns: Type Description Tuple[float, int] the best score and best epoch of the reverse model Source code in pytorch_adapt\\meta_validators\\reverse_validator.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def run ( self , forward_adapter , reverse_adapter , forward_kwargs , reverse_kwargs , pl_dataloader_creator = None , ) -> Tuple [ float , int ]: \"\"\" Arguments: forward_adapter: the framework-wrapped adapter for step 1. reverse_adapter: the framework-wrapped adapter for step 3. forward_kwargs: a dict of keyword arguments to be passed to forward_adapter.run() reverse_kwargs: a dict of keyword arguments to be passed to reverse_adapter.run() pl_dataloader_creator: An optional DataloaderCreator for obtaining pseudolabels in step 2. Returns: the best score and best epoch of the reverse model \"\"\" if \"datasets\" in reverse_kwargs : raise KeyError ( \"'datasets' should not be in reverse_kwargs because the reverse datasets will be pseudo labeled.\" ) if \"validator\" not in reverse_kwargs : raise KeyError ( \"reverse_kwargs must include 'validator'\" ) forward_adapter . run ( ** forward_kwargs ) if all ( x in forward_kwargs for x in [ \"validator\" , \"saver\" ]): forward_kwargs [ \"saver\" ] . load_adapter ( forward_adapter . adapter , \"best\" ) datasets = forward_kwargs [ \"datasets\" ] pl_dataloader_creator = c_f . default ( pl_dataloader_creator , DataloaderCreator , { \"all_val\" : True } ) d = {} d [ \"src_train\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_train\" , pl_dataloader_creator ) d [ \"src_val\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_val\" , pl_dataloader_creator ) d [ \"target_train\" ] = TargetDataset ( datasets [ \"src_train\" ] . dataset ) d [ \"target_val\" ] = TargetDataset ( datasets [ \"src_val\" ] . dataset ) d [ \"train\" ] = CombinedSourceAndTargetDataset ( d [ \"src_train\" ], d [ \"target_train\" ]) self . pseudo_train = d [ \"src_train\" ] self . pseudo_val = d [ \"src_val\" ] reverse_kwargs [ \"datasets\" ] = d return reverse_adapter . run ( ** reverse_kwargs )","title":"Meta Validators"},{"location":"meta_validators/#pytorch_adapt.meta_validators.forward_only_validator","text":"","title":"forward_only_validator"},{"location":"meta_validators/#pytorch_adapt.meta_validators.forward_only_validator.ForwardOnlyValidator","text":"This is basically a pass-through function. It returns the best score and best epoch that is returned by the inner adapter.","title":"ForwardOnlyValidator"},{"location":"meta_validators/#pytorch_adapt.meta_validators.forward_only_validator.ForwardOnlyValidator.run","text":"Parameters: Name Type Description Default adapter the framework-wrapped adapter. required **kwargs keyword arguments to be passed into adapter.run() {} Returns: Type Description Tuple[float, int] the best score and best epoch Source code in pytorch_adapt\\meta_validators\\forward_only_validator.py 11 12 13 14 15 16 17 18 19 20 21 22 23 def run ( self , adapter , ** kwargs ) -> Tuple [ float , int ]: \"\"\" Arguments: adapter: the framework-wrapped adapter. **kwargs: keyword arguments to be passed into adapter.run() Returns: the best score and best epoch \"\"\" if \"validator\" not in kwargs : raise KeyError ( \"An adapter validator is required when using ForwardOnlyValidator\" ) return adapter . run ( ** kwargs )","title":"run()"},{"location":"meta_validators/#pytorch_adapt.meta_validators.reverse_validator","text":"","title":"reverse_validator"},{"location":"meta_validators/#pytorch_adapt.meta_validators.reverse_validator.ReverseValidator","text":"Reverse validation consists of three steps. Train a model on the labeled source and unlabeled target Use the trained model to create pseudolabels for the target dataset. Train a new model on the labeled target and \"unlabeled\" source. The final score is the accuracy of the model from step 3.","title":"ReverseValidator"},{"location":"meta_validators/#pytorch_adapt.meta_validators.reverse_validator.ReverseValidator.run","text":"Parameters: Name Type Description Default forward_adapter the framework-wrapped adapter for step 1. required reverse_adapter the framework-wrapped adapter for step 3. required forward_kwargs a dict of keyword arguments to be passed to forward_adapter.run() required reverse_kwargs a dict of keyword arguments to be passed to reverse_adapter.run() required pl_dataloader_creator An optional DataloaderCreator for obtaining pseudolabels in step 2. None Returns: Type Description Tuple[float, int] the best score and best epoch of the reverse model Source code in pytorch_adapt\\meta_validators\\reverse_validator.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def run ( self , forward_adapter , reverse_adapter , forward_kwargs , reverse_kwargs , pl_dataloader_creator = None , ) -> Tuple [ float , int ]: \"\"\" Arguments: forward_adapter: the framework-wrapped adapter for step 1. reverse_adapter: the framework-wrapped adapter for step 3. forward_kwargs: a dict of keyword arguments to be passed to forward_adapter.run() reverse_kwargs: a dict of keyword arguments to be passed to reverse_adapter.run() pl_dataloader_creator: An optional DataloaderCreator for obtaining pseudolabels in step 2. Returns: the best score and best epoch of the reverse model \"\"\" if \"datasets\" in reverse_kwargs : raise KeyError ( \"'datasets' should not be in reverse_kwargs because the reverse datasets will be pseudo labeled.\" ) if \"validator\" not in reverse_kwargs : raise KeyError ( \"reverse_kwargs must include 'validator'\" ) forward_adapter . run ( ** forward_kwargs ) if all ( x in forward_kwargs for x in [ \"validator\" , \"saver\" ]): forward_kwargs [ \"saver\" ] . load_adapter ( forward_adapter . adapter , \"best\" ) datasets = forward_kwargs [ \"datasets\" ] pl_dataloader_creator = c_f . default ( pl_dataloader_creator , DataloaderCreator , { \"all_val\" : True } ) d = {} d [ \"src_train\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_train\" , pl_dataloader_creator ) d [ \"src_val\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_val\" , pl_dataloader_creator ) d [ \"target_train\" ] = TargetDataset ( datasets [ \"src_train\" ] . dataset ) d [ \"target_val\" ] = TargetDataset ( datasets [ \"src_val\" ] . dataset ) d [ \"train\" ] = CombinedSourceAndTargetDataset ( d [ \"src_train\" ], d [ \"target_train\" ]) self . pseudo_train = d [ \"src_train\" ] self . pseudo_val = d [ \"src_val\" ] reverse_kwargs [ \"datasets\" ] = d return reverse_adapter . run ( ** reverse_kwargs )","title":"run()"},{"location":"weighters/","text":"Weighters \u00b6 Weighters multiply losses by scalar values, and then reduce the losses to a single value on which you call .backward() . For example: import torch from pytorch_adapt.weighters import MeanWeighter weighter = MeanWeighter ( weights = { \"y\" : 2.3 }) logits = torch . randn ( 32 , 512 ) labels = torch . randint ( 0 , 10 , size = ( 32 ,)) x = torch . nn . functional . cross_entropy ( logits , labels ) y = torch . norm ( logits ) # y will by multiplied by 2.3 # x wasn't given a weight, # so it gets multiplied by the default value of 1. loss , components = weighter ({ \"x\" : x , \"y\" : y }) loss . backward () pytorch_adapt.weighters.base_weighter \u00b6 BaseWeighter \u00b6 Multiplies losses by scalar values, and then reduces them to a single value. __call__ ( self , loss_dict ) special \u00b6 Parameters: Name Type Description Default loss_dict Dict[str, torch.Tensor] A mapping from loss names to loss values. required Returns: Type Description Tuple[torch.Tensor, Dict[str, float]] A tuple where tuple[0] is the loss that .backward() can be called on, and tuple[1] is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. Source code in pytorch_adapt\\weighters\\base_weighter.py 52 53 54 55 56 57 58 59 60 61 62 63 def __call__ ( self , loss_dict : Dict [ str , torch . Tensor ] ) -> Tuple [ torch . Tensor , Dict [ str , float ]]: \"\"\" Arguments: loss_dict: A mapping from loss names to loss values. Returns: A tuple where ```tuple[0]``` is the loss that ```.backward()``` can be called on, and ```tuple[1]``` is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. \"\"\" return weight_losses ( self . reduction , self . weights , self . scale , loss_dict ) __init__ ( self , reduction , weights = None , scale = 1 ) special \u00b6 Parameters: Name Type Description Default reduction Callable[[List[torch.Tensor]], torch.Tensor] A function that takes in a list of losses and returns a single loss value. required weights Dict[str, float] A mapping from loss names to weight values. If None , weights are assumed to be 1. None scale float A scalar that every loss gets multiplied by. 1 Source code in pytorch_adapt\\weighters\\base_weighter.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , reduction : Callable [[ List [ torch . Tensor ]], torch . Tensor ], weights : Dict [ str , float ] = None , scale : float = 1 , ): \"\"\" Arguments: reduction: A function that takes in a list of losses and returns a single loss value. weights: A mapping from loss names to weight values. If ```None```, weights are assumed to be 1. scale: A scalar that every loss gets multiplied by. \"\"\" self . reduction = reduction self . weights = c_f . default ( weights , {}) self . scale = scale pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"weights\" , \"scale\" ]) pytorch_adapt.weighters.mean_weighter \u00b6 MeanWeighter \u00b6 Weights the losses and then returns the mean of the weighted losses. pytorch_adapt.weighters.sum_weighter \u00b6 SumWeighter \u00b6 Weights the losses and then returns the sum of the weighted losses.","title":"Weighters"},{"location":"weighters/#weighters","text":"Weighters multiply losses by scalar values, and then reduce the losses to a single value on which you call .backward() . For example: import torch from pytorch_adapt.weighters import MeanWeighter weighter = MeanWeighter ( weights = { \"y\" : 2.3 }) logits = torch . randn ( 32 , 512 ) labels = torch . randint ( 0 , 10 , size = ( 32 ,)) x = torch . nn . functional . cross_entropy ( logits , labels ) y = torch . norm ( logits ) # y will by multiplied by 2.3 # x wasn't given a weight, # so it gets multiplied by the default value of 1. loss , components = weighter ({ \"x\" : x , \"y\" : y }) loss . backward ()","title":"Weighters"},{"location":"weighters/#pytorch_adapt.weighters.base_weighter","text":"","title":"base_weighter"},{"location":"weighters/#pytorch_adapt.weighters.base_weighter.BaseWeighter","text":"Multiplies losses by scalar values, and then reduces them to a single value.","title":"BaseWeighter"},{"location":"weighters/#pytorch_adapt.weighters.base_weighter.BaseWeighter.__call__","text":"Parameters: Name Type Description Default loss_dict Dict[str, torch.Tensor] A mapping from loss names to loss values. required Returns: Type Description Tuple[torch.Tensor, Dict[str, float]] A tuple where tuple[0] is the loss that .backward() can be called on, and tuple[1] is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. Source code in pytorch_adapt\\weighters\\base_weighter.py 52 53 54 55 56 57 58 59 60 61 62 63 def __call__ ( self , loss_dict : Dict [ str , torch . Tensor ] ) -> Tuple [ torch . Tensor , Dict [ str , float ]]: \"\"\" Arguments: loss_dict: A mapping from loss names to loss values. Returns: A tuple where ```tuple[0]``` is the loss that ```.backward()``` can be called on, and ```tuple[1]``` is a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. \"\"\" return weight_losses ( self . reduction , self . weights , self . scale , loss_dict )","title":"__call__()"},{"location":"weighters/#pytorch_adapt.weighters.base_weighter.BaseWeighter.__init__","text":"Parameters: Name Type Description Default reduction Callable[[List[torch.Tensor]], torch.Tensor] A function that takes in a list of losses and returns a single loss value. required weights Dict[str, float] A mapping from loss names to weight values. If None , weights are assumed to be 1. None scale float A scalar that every loss gets multiplied by. 1 Source code in pytorch_adapt\\weighters\\base_weighter.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , reduction : Callable [[ List [ torch . Tensor ]], torch . Tensor ], weights : Dict [ str , float ] = None , scale : float = 1 , ): \"\"\" Arguments: reduction: A function that takes in a list of losses and returns a single loss value. weights: A mapping from loss names to weight values. If ```None```, weights are assumed to be 1. scale: A scalar that every loss gets multiplied by. \"\"\" self . reduction = reduction self . weights = c_f . default ( weights , {}) self . scale = scale pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"weights\" , \"scale\" ])","title":"__init__()"},{"location":"weighters/#pytorch_adapt.weighters.mean_weighter","text":"","title":"mean_weighter"},{"location":"weighters/#pytorch_adapt.weighters.mean_weighter.MeanWeighter","text":"Weights the losses and then returns the mean of the weighted losses.","title":"MeanWeighter"},{"location":"weighters/#pytorch_adapt.weighters.sum_weighter","text":"","title":"sum_weighter"},{"location":"weighters/#pytorch_adapt.weighters.sum_weighter.SumWeighter","text":"Weights the losses and then returns the sum of the weighted losses.","title":"SumWeighter"}]}