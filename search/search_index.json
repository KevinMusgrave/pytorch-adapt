{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTorch Adapt \u00b6 Examples \u00b6 See the examples folder for notebooks you can download or run on Google Colab. How to... \u00b6 Use in vanilla PyTorch \u00b6 from pytorch_adapt.hooks import DANNHook from pytorch_adapt.utils.common_functions import batch_to_device # Assuming that models, optimizers, and dataloader are already created. hook = DANNHook ( optimizers ) for data in tqdm ( dataloader ): data = batch_to_device ( data , device ) # Optimization is done inside the hook. # The returned loss is for logging. loss , _ = hook ({}, { ** models , ** data }) Build complex algorithms \u00b6 Let's customize DANNHook with: minimum class confusion virtual adversarial training from pytorch_adapt.hooks import MCCHook , VATHook # G and C are the Generator and Classifier models G , C = models [ \"G\" ], models [ \"C\" ] misc = { \"combined_model\" : torch . nn . Sequential ( G , C )} hook = DANNHook ( optimizers , post_g = [ MCCHook (), VATHook ()]) for data in tqdm ( dataloader ): data = batch_to_device ( data , device ) loss , _ = hook ({}, { ** models , ** data , ** misc }) Wrap with your favorite PyTorch framework \u00b6 First, set up the adapter and dataloaders: from pytorch_adapt.adapters import DANN from pytorch_adapt.containers import Models from pytorch_adapt.datasets import DataloaderCreator models_cont = Models ( models ) adapter = DANN ( models = models_cont ) dc = DataloaderCreator ( num_workers = 2 ) dataloaders = dc ( ** datasets ) Then use a framework wrapper: PyTorch Lightning \u00b6 import pytorch_lightning as pl from pytorch_adapt.frameworks.lightning import Lightning L_adapter = Lightning ( adapter ) trainer = pl . Trainer ( gpus = 1 , max_epochs = 1 ) trainer . fit ( L_adapter , dataloaders [ \"train\" ]) PyTorch Ignite \u00b6 trainer = Ignite ( adapter ) trainer . run ( datasets , dataloader_creator = dc ) Check your model's performance \u00b6 You can do this in vanilla PyTorch: from pytorch_adapt.validators import SNDValidator # Assuming predictions have been collected target_train = { \"preds\" : preds } validator = SNDValidator () score = validator ( target_train = target_train ) You can also do this during training with a framework wrapper: PyTorch Lightning \u00b6 from pytorch_adapt.frameworks.utils import filter_datasets validator = SNDValidator () dataloaders = dc ( ** filter_datasets ( datasets , validator )) train_loader = dataloaders . pop ( \"train\" ) L_adapter = Lightning ( adapter , validator = validator ) trainer = pl . Trainer ( gpus = 1 , max_epochs = 1 ) trainer . fit ( L_adapter , train_loader , list ( dataloaders . values ())) Pytorch Ignite \u00b6 from pytorch_adapt.validators import ScoreHistory validator = ScoreHistory ( SNDValidator ()) trainer = Ignite ( adapter , validator = validator ) trainer . run ( datasets , dataloader_creator = dc ) Run the above examples \u00b6 See this notebook and the examples page for other notebooks. Installation \u00b6 Pip \u00b6 pip install pytorch-adapt To get the latest dev version : pip install pytorch-adapt --pre To use pytorch_adapt.frameworks.lightning : pip install pytorch-adapt[lightning] To use pytorch_adapt.frameworks.ignite : pip install pytorch-adapt[ignite] Conda \u00b6 Coming soon... Dependencies \u00b6 See setup.py","title":"Overview"},{"location":"#pytorch-adapt","text":"","title":"PyTorch Adapt"},{"location":"#examples","text":"See the examples folder for notebooks you can download or run on Google Colab.","title":"Examples"},{"location":"#how-to","text":"","title":"How to..."},{"location":"#installation","text":"","title":"Installation"},{"location":"getting_started/","text":"Getting started \u00b6 Examples \u00b6 Currently the best place to start is the example jupyter notebooks . You can download them or run them on Google Colab. Please open an issue on GitHub if you have any questions. Overview \u00b6 This library consists of 12 modules: Module Description Adapters Wrappers for training and inference steps Containers Dictionaries for simplifying object creation Datasets Commonly used datasets and tools for domain adaptation Frameworks Wrappers for training/testing pipelines Hooks Modular building blocks for domain adaptation algorithms Inference Algorithm-specific functions used during inference Layers Loss functions and helper layers Meta Validators Post-processing of metrics, for hyperparameter optimization Models Architectures used for benchmarking and in examples Utils Various tools Validators Metrics for determining and estimating accuracy Weighters Functions for weighting losses Adapters \u00b6 Adapters contain an algorithm's training step and inference step. The training step is defined in the wrapped hook . View this notebook for examples . Containers \u00b6 Containers are Python dictionaries with extra functions that simplify object creation. View this notebook for examples . Datasets \u00b6 The datasets module consists of wrapper classes that output data in a format compatible with hooks . It also contains some common domain-adaptation datasets like MNISTM , Office31 , and OfficeHome . View this notebook for examples . Frameworks \u00b6 This library works with plain PyTorch, but what about PyTorch frameworks like Ignite , Lightning , and Catalyst ? Of course, anything that works in PyTorch will also work in PyTorch frameworks. However, there is some setup involved, like registering event handlers in Ignite, or writing class definitions in Lightning and Catalyst. The purpose of the frameworks module is to eliminate that setup. Check out these notebooks for examples: DANN with PyTorch Lightning DANN with PyTorch Ignite DANN with PyTorch Ignite + Visualizations Hooks \u00b6 Hooks are the building blocks of the algorithms in this library. Check out these notebooks for examples: Customizing Algorithms Hooks explained in depth Layers \u00b6 The layers module contains loss functions and wrapper models. These are used in combination with hooks to create domain adaptation algorithms. Validators \u00b6 Validators compute or estimate the accuracy of a model. Weighters \u00b6 Weighters multiply losses by scalar values, and then reduce the losses to a single value on which you call .backward() . View this notebook for examples .","title":"Getting started"},{"location":"getting_started/#getting-started","text":"","title":"Getting started"},{"location":"getting_started/#examples","text":"Currently the best place to start is the example jupyter notebooks . You can download them or run them on Google Colab. Please open an issue on GitHub if you have any questions.","title":"Examples"},{"location":"getting_started/#overview","text":"This library consists of 12 modules: Module Description Adapters Wrappers for training and inference steps Containers Dictionaries for simplifying object creation Datasets Commonly used datasets and tools for domain adaptation Frameworks Wrappers for training/testing pipelines Hooks Modular building blocks for domain adaptation algorithms Inference Algorithm-specific functions used during inference Layers Loss functions and helper layers Meta Validators Post-processing of metrics, for hyperparameter optimization Models Architectures used for benchmarking and in examples Utils Various tools Validators Metrics for determining and estimating accuracy Weighters Functions for weighting losses","title":"Overview"},{"location":"getting_started/#adapters","text":"Adapters contain an algorithm's training step and inference step. The training step is defined in the wrapped hook . View this notebook for examples .","title":"Adapters"},{"location":"getting_started/#containers","text":"Containers are Python dictionaries with extra functions that simplify object creation. View this notebook for examples .","title":"Containers"},{"location":"getting_started/#datasets","text":"The datasets module consists of wrapper classes that output data in a format compatible with hooks . It also contains some common domain-adaptation datasets like MNISTM , Office31 , and OfficeHome . View this notebook for examples .","title":"Datasets"},{"location":"getting_started/#frameworks","text":"This library works with plain PyTorch, but what about PyTorch frameworks like Ignite , Lightning , and Catalyst ? Of course, anything that works in PyTorch will also work in PyTorch frameworks. However, there is some setup involved, like registering event handlers in Ignite, or writing class definitions in Lightning and Catalyst. The purpose of the frameworks module is to eliminate that setup. Check out these notebooks for examples: DANN with PyTorch Lightning DANN with PyTorch Ignite DANN with PyTorch Ignite + Visualizations","title":"Frameworks"},{"location":"getting_started/#hooks","text":"Hooks are the building blocks of the algorithms in this library. Check out these notebooks for examples: Customizing Algorithms Hooks explained in depth","title":"Hooks"},{"location":"getting_started/#layers","text":"The layers module contains loss functions and wrapper models. These are used in combination with hooks to create domain adaptation algorithms.","title":"Layers"},{"location":"getting_started/#validators","text":"Validators compute or estimate the accuracy of a model.","title":"Validators"},{"location":"getting_started/#weighters","text":"Weighters multiply losses by scalar values, and then reduce the losses to a single value on which you call .backward() . View this notebook for examples .","title":"Weighters"},{"location":"algorithms/uda/","text":"Unsupervised Domain Adaptation \u00b6 Notebooks \u00b6 These notebooks contain runnable paper implementations: Paper Implementations (as hooks) Papers \u00b6 AdaBN \u00b6 Revisiting Batch Normalization For Practical Domain Adaptation Docs coming soon ADDA \u00b6 Adversarial Discriminative Domain Adaptation adapters.ADDA hooks.ADDAHook hooks.StrongDHook AFN \u00b6 Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation layers.AdaptiveFeatureNorm layers.L2PreservedDropout ATDOC \u00b6 Domain Adaptation with Auxiliary Target Domain-Oriented Classifier hooks.ATDOCHook layers.NeighborhoodAggregation layers.ConfidenceWeights BNM \u00b6 Towards Discriminability and Diversity: Batch Nuclear-norm Maximization under Label Insufficient Situations layers.BNMLoss BSP \u00b6 Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation layers.BatchSpectralLoss CDAN \u00b6 Conditional Adversarial Domain Adaptation adapters.CDAN hooks.CDANHook hooks.EntropyReducer layers.EntropyWeights layers.RandomizedDotProduct CORAL \u00b6 Deep CORAL: Correlation Alignment for Deep Domain Adaptation adapters.Aligner hooks.AlignerPlusCHook layers.CORALLoss DANN \u00b6 Domain-Adversarial Training of Neural Networks adapters.DANN hooks.DANNHook layers.GradientReversal Domain Confusion \u00b6 Simultaneous Deep Transfer Across Domains and Tasks adapters.DomainConfusion hooks.DomainConfusionHook layers.UniformDistributionLoss GAN \u00b6 adapters.GAN hooks.GANHook GVB \u00b6 Gradually Vanishing Bridge for Adversarial Domain Adaptation adapters.GVB hooks.GVBHook layers.ModelWithBridge IM \u00b6 layers.EntropyLoss layers.DiversityLoss ITL \u00b6 Information-Theoretical Learning of Discriminative Clusters for Unsupervised Domain Adaptation Docs coming soon JMMD \u00b6 Deep Transfer Learning with Joint Adaptation Networks adapters.Aligner hooks.AlignerPlusCHook hooks.JointAlignerHook layers.MMDLoss MCC \u00b6 Minimum Class Confusion for Versatile Domain Adaptation layers.MCCLoss MCD \u00b6 Maximum Classifier Discrepancy for Unsupervised Domain Adaptation adapters.MCD hooks.MCDHook layers.MCDLoss MMD \u00b6 Learning Transferable Features with Deep Adaptation Networks adapters.Aligner hooks.AlignerPlusCHook layers.MMDLoss RTN \u00b6 Unsupervised Domain Adaptation with Residual Transfer Networks adapters.RTN hooks.RTNHook layers.PlusResidual STAR \u00b6 Stochastic Classifiers for Unsupervised Domain Adaptation adapters.MCD hooks.MCDHook layers.StochasticLinear SWD \u00b6 Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation adapters.MCD hooks.MCDHook layers.SlicedWasserstein SymNets \u00b6 Domain-Symmetric Networks for Adversarial Domain Adaptation adapters.SymNets hooks.SymNets layers.ConcatSoftmax VADA \u00b6 A DIRT-T Approach to Unsupervised Domain Adaptation adapters.VADA hooks.VADAHook hooks.VATHook layers.VATLoss layers.EntropyLoss","title":"Unsupervised Domain Adaptation"},{"location":"algorithms/uda/#unsupervised-domain-adaptation","text":"","title":"Unsupervised Domain Adaptation"},{"location":"algorithms/uda/#notebooks","text":"These notebooks contain runnable paper implementations: Paper Implementations (as hooks)","title":"Notebooks"},{"location":"algorithms/uda/#papers","text":"","title":"Papers"},{"location":"algorithms/validators/","text":"Validators \u00b6 Deep Embedded Validation \u00b6 Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation validators.DeepEmbeddedValidator Information Maximization Validator \u00b6 validators.IMValidator Soft Neighborhood Density \u00b6 Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density validators.SNDValidator","title":"Validators"},{"location":"algorithms/validators/#validators","text":"","title":"Validators"},{"location":"docs/SUMMARY/","text":"adapters adabn adda aligner base_adapter classifier dann gan mcd symnets utils containers base_container key_enforcer lr_schedulers misc models multiple_containers optimizers datasets base_dataset combined_source_and_target concat_dataset dataloader_creator domainnet mnistm office31 officehome pseudo_labeled_dataset source_dataset target_dataset frameworks ignite loggers ignite_record_keeper_logger checkpoint_utils ignite lightning lightning hooks adabn adda aligners atdoc base cdan classification conditions dann domain domain_confusion features gan gvb mcd optimizer reducers rtn symnets utils vada validate inference inference layers abs_loss adaptive_feature_norm batch_spectral_loss bnm_loss concat_softmax confidence_weights coral_loss diversity_loss do_nothing_optimizer entropy_loss entropy_weights gradient_reversal ist_loss mcc_loss mcd_loss mmd_loss model_with_bridge multiple_models neighborhood_aggregation nll_loss plus_residual randomized_dot_product silhouette_score sliced_wasserstein stochastic_linear sufficient_accuracy uniform_distribution_loss vat_loss meta_validators forward_only_validator reverse_validator models classifier discriminator mnist pretrained utils validators accuracy_validator base_validator deep_embedded_validator diversity_validator entropy_validator error_validator im_validator multiple_validators score_history snd_validator weighters base_weighter mean_weighter sum_weighter","title":"SUMMARY"},{"location":"docs/adapters/","text":"The following can be imported like this (using ADDA as an example): from pytorch_adapt.adapters import ADDA Direct module members \u00b6 ADDA AdaBN Aligner BaseAdapter CDAN CDANE CDANNE Classifier DANN DANNE DomainConfusion Finetuner GAN GANE GVB GVBE MCD RTN SymNets VADA","title":"adapters"},{"location":"docs/adapters/#direct-module-members","text":"ADDA AdaBN Aligner BaseAdapter CDAN CDANE CDANNE Classifier DANN DANNE DomainConfusion Finetuner GAN GANE GVB GVBE MCD RTN SymNets VADA","title":"Direct module members"},{"location":"docs/adapters/adabn/","text":"AdaBN \u00b6 Bases: BaseAdapter Wraps AdaBNHook . Container Required keys models [\"G\", \"C\"] Source code in pytorch_adapt\\adapters\\adabn.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class AdaBN ( BaseAdapter ): \"\"\" Wraps [AdaBNHook][pytorch_adapt.hooks.AdaBNHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\"]```| \"\"\" hook_cls = AdaBNHook def __init__ ( self , * args , inference_fn = None , ** kwargs ): \"\"\" Arguments: inference_fn: Default is [adabn_fn][pytorch_adapt.inference.adabn_fn] \"\"\" inference_fn = c_f . default ( inference_fn , adabn_fn ) super () . __init__ ( * args , inference_fn = inference_fn , ** kwargs ) def init_hook ( self , hook_kwargs ): self . hook = self . hook_cls ( ** hook_kwargs ) def get_key_enforcer ( self ) -> KeyEnforcer : return KeyEnforcer ( models = [ \"G\" , \"C\" ], optimizers = []) def get_default_containers ( self ) -> MultipleContainers : return MultipleContainers () __init__ ( * args , inference_fn = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default inference_fn Default is adabn_fn None Source code in pytorch_adapt\\adapters\\adabn.py 19 20 21 22 23 24 25 def __init__ ( self , * args , inference_fn = None , ** kwargs ): \"\"\" Arguments: inference_fn: Default is [adabn_fn][pytorch_adapt.inference.adabn_fn] \"\"\" inference_fn = c_f . default ( inference_fn , adabn_fn ) super () . __init__ ( * args , inference_fn = inference_fn , ** kwargs )","title":"adabn"},{"location":"docs/adapters/adabn/#pytorch_adapt.adapters.adabn.AdaBN","text":"Bases: BaseAdapter Wraps AdaBNHook . Container Required keys models [\"G\", \"C\"] Source code in pytorch_adapt\\adapters\\adabn.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class AdaBN ( BaseAdapter ): \"\"\" Wraps [AdaBNHook][pytorch_adapt.hooks.AdaBNHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\"]```| \"\"\" hook_cls = AdaBNHook def __init__ ( self , * args , inference_fn = None , ** kwargs ): \"\"\" Arguments: inference_fn: Default is [adabn_fn][pytorch_adapt.inference.adabn_fn] \"\"\" inference_fn = c_f . default ( inference_fn , adabn_fn ) super () . __init__ ( * args , inference_fn = inference_fn , ** kwargs ) def init_hook ( self , hook_kwargs ): self . hook = self . hook_cls ( ** hook_kwargs ) def get_key_enforcer ( self ) -> KeyEnforcer : return KeyEnforcer ( models = [ \"G\" , \"C\" ], optimizers = []) def get_default_containers ( self ) -> MultipleContainers : return MultipleContainers ()","title":"AdaBN"},{"location":"docs/adapters/adda/","text":"ADDA \u00b6 Bases: BaseAdapter Wraps ADDAHook . Container Required keys models [\"G\", \"C\", \"D\"] optimizers [\"D\", \"T\"] The target model (\"T\") is created during initialization by deep-copying the G model. Source code in pytorch_adapt\\adapters\\adda.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class ADDA ( BaseAdapter ): \"\"\" Wraps [ADDAHook][pytorch_adapt.hooks.ADDAHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"D\"]```| |optimizers|```[\"D\", \"T\"]```| The target model (\"T\") is created during initialization by deep-copying the G model. \"\"\" hook_cls = ADDAHook def __init__ ( self , * args , inference_fn = None , ** kwargs ): \"\"\" Arguments: inference_fn: Default is [adda_fn][pytorch_adapt.inference.adda_fn] \"\"\" inference_fn = c_f . default ( inference_fn , adda_fn ) super () . __init__ ( * args , inference_fn = inference_fn , ** kwargs ) def get_default_containers ( self ) -> MultipleContainers : \"\"\" Returns: The default set of containers. This will use the [default optimizer][pytorch_adapt.adapters.utils.default_optimizer_tuple] for the T and D models. \"\"\" optimizers = Optimizers ( default_optimizer_tuple (), keys = [ \"T\" , \"D\" ]) return MultipleContainers ( optimizers = optimizers ) def get_key_enforcer ( self ) -> KeyEnforcer : return KeyEnforcer ( models = [ \"G\" , \"C\" , \"D\" , \"T\" ], optimizers = [ \"D\" , \"T\" ], ) def init_hook ( self , hook_kwargs ): self . hook = self . hook_cls ( d_opts = with_opt ([ \"D\" ]), g_opts = with_opt ([ \"T\" ]), ** hook_kwargs ) def init_containers_and_check_keys ( self , containers ): containers [ \"models\" ][ \"T\" ] = copy . deepcopy ( containers [ \"models\" ][ \"G\" ]) super () . init_containers_and_check_keys ( containers ) __init__ ( * args , inference_fn = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default inference_fn Default is adda_fn None Source code in pytorch_adapt\\adapters\\adda.py 25 26 27 28 29 30 31 def __init__ ( self , * args , inference_fn = None , ** kwargs ): \"\"\" Arguments: inference_fn: Default is [adda_fn][pytorch_adapt.inference.adda_fn] \"\"\" inference_fn = c_f . default ( inference_fn , adda_fn ) super () . __init__ ( * args , inference_fn = inference_fn , ** kwargs ) get_default_containers () \u00b6 Returns: Type Description MultipleContainers The default set of containers. This MultipleContainers will use the MultipleContainers default optimizer MultipleContainers for the T and D models. Source code in pytorch_adapt\\adapters\\adda.py 33 34 35 36 37 38 39 40 41 42 def get_default_containers ( self ) -> MultipleContainers : \"\"\" Returns: The default set of containers. This will use the [default optimizer][pytorch_adapt.adapters.utils.default_optimizer_tuple] for the T and D models. \"\"\" optimizers = Optimizers ( default_optimizer_tuple (), keys = [ \"T\" , \"D\" ]) return MultipleContainers ( optimizers = optimizers )","title":"adda"},{"location":"docs/adapters/adda/#pytorch_adapt.adapters.adda.ADDA","text":"Bases: BaseAdapter Wraps ADDAHook . Container Required keys models [\"G\", \"C\", \"D\"] optimizers [\"D\", \"T\"] The target model (\"T\") is created during initialization by deep-copying the G model. Source code in pytorch_adapt\\adapters\\adda.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class ADDA ( BaseAdapter ): \"\"\" Wraps [ADDAHook][pytorch_adapt.hooks.ADDAHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"D\"]```| |optimizers|```[\"D\", \"T\"]```| The target model (\"T\") is created during initialization by deep-copying the G model. \"\"\" hook_cls = ADDAHook def __init__ ( self , * args , inference_fn = None , ** kwargs ): \"\"\" Arguments: inference_fn: Default is [adda_fn][pytorch_adapt.inference.adda_fn] \"\"\" inference_fn = c_f . default ( inference_fn , adda_fn ) super () . __init__ ( * args , inference_fn = inference_fn , ** kwargs ) def get_default_containers ( self ) -> MultipleContainers : \"\"\" Returns: The default set of containers. This will use the [default optimizer][pytorch_adapt.adapters.utils.default_optimizer_tuple] for the T and D models. \"\"\" optimizers = Optimizers ( default_optimizer_tuple (), keys = [ \"T\" , \"D\" ]) return MultipleContainers ( optimizers = optimizers ) def get_key_enforcer ( self ) -> KeyEnforcer : return KeyEnforcer ( models = [ \"G\" , \"C\" , \"D\" , \"T\" ], optimizers = [ \"D\" , \"T\" ], ) def init_hook ( self , hook_kwargs ): self . hook = self . hook_cls ( d_opts = with_opt ([ \"D\" ]), g_opts = with_opt ([ \"T\" ]), ** hook_kwargs ) def init_containers_and_check_keys ( self , containers ): containers [ \"models\" ][ \"T\" ] = copy . deepcopy ( containers [ \"models\" ][ \"G\" ]) super () . init_containers_and_check_keys ( containers )","title":"ADDA"},{"location":"docs/adapters/aligner/","text":"Aligner \u00b6 Bases: BaseGCAdapter Wraps AlignerPlusCHook . Container Required keys models [\"G\", \"C\"] optimizers [\"G\", \"C\"] Source code in pytorch_adapt\\adapters\\aligner.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Aligner ( BaseGCAdapter ): \"\"\" Wraps [AlignerPlusCHook][pytorch_adapt.hooks.AlignerPlusCHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\"]```| |optimizers|```[\"G\", \"C\"]```| \"\"\" hook_cls = AlignerPlusCHook def init_hook ( self , hook_kwargs ): opts = with_opt ( list ( self . optimizers . keys ())) self . hook = self . hook_cls ( opts , ** hook_kwargs ) RTN \u00b6 Bases: Aligner Wraps RTNHook . Container Required keys models [\"G\", \"C\", \"residual_model\"] optimizers [\"G\", \"C\", \"residual_model\"] misc [\"feature_combiner\"] Source code in pytorch_adapt\\adapters\\aligner.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class RTN ( Aligner ): \"\"\" Wraps [RTNHook][pytorch_adapt.hooks.RTNHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"residual_model\"]```| |optimizers|```[\"G\", \"C\", \"residual_model\"]```| |misc|```[\"feature_combiner\"]```| \"\"\" hook_cls = RTNHook def __init__ ( self , * args , inference_fn = None , ** kwargs ): \"\"\" Arguments: inference_fn: Default is [rtn_fn][pytorch_adapt.inference.rtn_fn] \"\"\" inference_fn = c_f . default ( inference_fn , rtn_fn ) super () . __init__ ( * args , inference_fn = inference_fn , ** kwargs ) def get_key_enforcer ( self ) -> KeyEnforcer : ke = super () . get_key_enforcer () ke . requirements [ \"models\" ] . append ( \"residual_model\" ) ke . requirements [ \"optimizers\" ] . append ( \"residual_model\" ) ke . requirements [ \"misc\" ] = [ \"feature_combiner\" ] return ke __init__ ( * args , inference_fn = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default inference_fn Default is rtn_fn None Source code in pytorch_adapt\\adapters\\aligner.py 39 40 41 42 43 44 45 def __init__ ( self , * args , inference_fn = None , ** kwargs ): \"\"\" Arguments: inference_fn: Default is [rtn_fn][pytorch_adapt.inference.rtn_fn] \"\"\" inference_fn = c_f . default ( inference_fn , rtn_fn ) super () . __init__ ( * args , inference_fn = inference_fn , ** kwargs )","title":"aligner"},{"location":"docs/adapters/aligner/#pytorch_adapt.adapters.aligner.Aligner","text":"Bases: BaseGCAdapter Wraps AlignerPlusCHook . Container Required keys models [\"G\", \"C\"] optimizers [\"G\", \"C\"] Source code in pytorch_adapt\\adapters\\aligner.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Aligner ( BaseGCAdapter ): \"\"\" Wraps [AlignerPlusCHook][pytorch_adapt.hooks.AlignerPlusCHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\"]```| |optimizers|```[\"G\", \"C\"]```| \"\"\" hook_cls = AlignerPlusCHook def init_hook ( self , hook_kwargs ): opts = with_opt ( list ( self . optimizers . keys ())) self . hook = self . hook_cls ( opts , ** hook_kwargs )","title":"Aligner"},{"location":"docs/adapters/aligner/#pytorch_adapt.adapters.aligner.RTN","text":"Bases: Aligner Wraps RTNHook . Container Required keys models [\"G\", \"C\", \"residual_model\"] optimizers [\"G\", \"C\", \"residual_model\"] misc [\"feature_combiner\"] Source code in pytorch_adapt\\adapters\\aligner.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class RTN ( Aligner ): \"\"\" Wraps [RTNHook][pytorch_adapt.hooks.RTNHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"residual_model\"]```| |optimizers|```[\"G\", \"C\", \"residual_model\"]```| |misc|```[\"feature_combiner\"]```| \"\"\" hook_cls = RTNHook def __init__ ( self , * args , inference_fn = None , ** kwargs ): \"\"\" Arguments: inference_fn: Default is [rtn_fn][pytorch_adapt.inference.rtn_fn] \"\"\" inference_fn = c_f . default ( inference_fn , rtn_fn ) super () . __init__ ( * args , inference_fn = inference_fn , ** kwargs ) def get_key_enforcer ( self ) -> KeyEnforcer : ke = super () . get_key_enforcer () ke . requirements [ \"models\" ] . append ( \"residual_model\" ) ke . requirements [ \"optimizers\" ] . append ( \"residual_model\" ) ke . requirements [ \"misc\" ] = [ \"feature_combiner\" ] return ke","title":"RTN"},{"location":"docs/adapters/base_adapter/","text":"BaseAdapter \u00b6 Bases: ABC Parent class of all adapters. Source code in pytorch_adapt\\adapters\\base_adapter.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class BaseAdapter ( ABC ): \"\"\" Parent class of all adapters. \"\"\" def __init__ ( self , models : Models = None , optimizers : Optimizers = None , lr_schedulers : LRSchedulers = None , misc : Misc = None , default_containers : MultipleContainers = None , key_enforcer : KeyEnforcer = None , inference_fn = None , before_training_starts = None , hook_kwargs : Dict [ str , Any ] = None , ): \"\"\" Arguments: models: A [```Models```][pytorch_adapt.containers.Models] container. The models will be passed to the wrapped hook at each training iteration. optimizers: An [```Optimizers```][pytorch_adapt.containers.Optimizers] container. The optimizers will be passed to the wrapped hook at each training iteration. lr_schedulers: An [```LRSchedulers```][pytorch_adapt.containers.LRSchedulers] container. The lr schedulers are called automatically by the [```framework```](../frameworks/index.md) that wrap this adapter. misc: A [```Misc```][pytorch_adapt.containers.Misc] container for models that don't require optimizers, and other miscellaneous objects. These are passed into the wrapped hook at each training iteration. default_containers: The default set of containers to use, wrapped in a [```MultipleContainers```][pytorch_adapt.containers.MultipleContainers] object. If ```None``` then the default containers are defined in [```self.get_default_containers```][pytorch_adapt.adapters.BaseAdapter.get_default_containers] key_enforcer: A [```KeyEnforcer```][pytorch_adapt.containers.KeyEnforcer] object. If ```None```, then [```self.get_key_enforcer```][pytorch_adapt.adapters.BaseAdapter.get_key_enforcer] is used. inference_fn: A function that takes in: - ```x```: the input to the model - ```domain```: an integer representing the domain of the data - ```models```: a dictionary of models, i.e. ```self.models``` - ```misc```: a dictionary of misc objects, i.e. ```self.misc``` before_training_starts: A function that takes in this adapter and returns another function that is optionally called by a framework wrapper before training starts. hook_kwargs: A dictionary of keyword arguments that will be passed into the wrapped hook during initialization. \"\"\" containers = c_f . default ( default_containers , self . get_default_containers , {}) self . key_enforcer = c_f . default ( key_enforcer , self . get_key_enforcer , {}) self . before_training_starts = c_f . class_default ( self , before_training_starts , self . before_training_starts_default ) containers . merge ( models = models , optimizers = optimizers , lr_schedulers = lr_schedulers , misc = misc , ) hook_kwargs = c_f . default ( hook_kwargs , {}) self . init_containers_and_check_keys ( containers ) self . init_hook ( hook_kwargs ) self . inference_fn = c_f . default ( inference_fn , default_fn ) def training_step ( self , batch : Dict [ str , Any ], ** kwargs ) -> Dict [ str , Dict [ str , float ]]: \"\"\" Calls the wrapped hook at each iteration during training. Arguments: batch: A dictionary containing training data. **kwargs: Any other data that will be passed into the hook. Returns: A two-level dictionary - the outer level is associated with a particular optimization step (relevant for GAN architectures), - the inner level contains the loss components. \"\"\" combined = c_f . assert_dicts_are_disjoint ( self . models , self . misc , with_opt ( self . optimizers ), batch , kwargs ) _ , losses = self . hook ( combined ) return losses def inference ( self , x : torch . Tensor , domain : int = None ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Arguments: x: The input to the model domain: An optional integer indicating the domain. Returns: Features and logits \"\"\" return self . inference_fn ( x = x , domain = domain , models = self . models , misc = self . misc , ) def get_default_containers ( self ) -> MultipleContainers : \"\"\" Returns: The default set of containers. Consists of an [Optimizers][pytorch_adapt.containers.Optimizers] container using the [default][pytorch_adapt.adapters.utils.default_optimizer_tuple] of Adam with lr 0.0001. \"\"\" optimizers = Optimizers ( default_optimizer_tuple ()) return MultipleContainers ( optimizers = optimizers ) @abstractmethod def get_key_enforcer ( self ) -> KeyEnforcer : \"\"\" Returns: The default KeyEnforcer. \"\"\" pass @abstractmethod def init_hook ( self ): \"\"\" ```self.hook``` is initialized here. \"\"\" pass def init_containers_and_check_keys ( self , containers ): \"\"\" Called in ```__init__``` before [```init_hook```][pytorch_adapt.adapters.BaseAdapter.init_hook]. \"\"\" containers . create () self . key_enforcer . check ( containers ) for k , v in containers . items (): setattr ( self , k , v ) def before_training_starts_default ( self , framework ): c_f . LOGGER . debug ( f \"models \\n { self . models } \" ) c_f . LOGGER . debug ( f \"optimizers \\n { self . optimizers } \" ) c_f . LOGGER . debug ( f \"lr_schedulers \\n { self . lr_schedulers } \" ) c_f . LOGGER . debug ( f \"misc \\n { self . misc } \" ) c_f . LOGGER . debug ( f \"hook \\n { self . hook } \" ) __init__ ( models = None , optimizers = None , lr_schedulers = None , misc = None , default_containers = None , key_enforcer = None , inference_fn = None , before_training_starts = None , hook_kwargs = None ) \u00b6 Parameters: Name Type Description Default models Models A Models container. The models will be passed to the wrapped hook at each training iteration. None optimizers Optimizers An Optimizers container. The optimizers will be passed to the wrapped hook at each training iteration. None lr_schedulers LRSchedulers An LRSchedulers container. The lr schedulers are called automatically by the framework that wrap this adapter. None misc Misc A Misc container for models that don't require optimizers, and other miscellaneous objects. These are passed into the wrapped hook at each training iteration. None default_containers MultipleContainers The default set of containers to use, wrapped in a MultipleContainers object. If None then the default containers are defined in self.get_default_containers None key_enforcer KeyEnforcer A KeyEnforcer object. If None , then self.get_key_enforcer is used. None inference_fn A function that takes in: x : the input to the model domain : an integer representing the domain of the data models : a dictionary of models, i.e. self.models misc : a dictionary of misc objects, i.e. self.misc None before_training_starts A function that takes in this adapter and returns another function that is optionally called by a framework wrapper before training starts. None hook_kwargs Dict [ str , Any ] A dictionary of keyword arguments that will be passed into the wrapped hook during initialization. None Source code in pytorch_adapt\\adapters\\base_adapter.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , models : Models = None , optimizers : Optimizers = None , lr_schedulers : LRSchedulers = None , misc : Misc = None , default_containers : MultipleContainers = None , key_enforcer : KeyEnforcer = None , inference_fn = None , before_training_starts = None , hook_kwargs : Dict [ str , Any ] = None , ): \"\"\" Arguments: models: A [```Models```][pytorch_adapt.containers.Models] container. The models will be passed to the wrapped hook at each training iteration. optimizers: An [```Optimizers```][pytorch_adapt.containers.Optimizers] container. The optimizers will be passed to the wrapped hook at each training iteration. lr_schedulers: An [```LRSchedulers```][pytorch_adapt.containers.LRSchedulers] container. The lr schedulers are called automatically by the [```framework```](../frameworks/index.md) that wrap this adapter. misc: A [```Misc```][pytorch_adapt.containers.Misc] container for models that don't require optimizers, and other miscellaneous objects. These are passed into the wrapped hook at each training iteration. default_containers: The default set of containers to use, wrapped in a [```MultipleContainers```][pytorch_adapt.containers.MultipleContainers] object. If ```None``` then the default containers are defined in [```self.get_default_containers```][pytorch_adapt.adapters.BaseAdapter.get_default_containers] key_enforcer: A [```KeyEnforcer```][pytorch_adapt.containers.KeyEnforcer] object. If ```None```, then [```self.get_key_enforcer```][pytorch_adapt.adapters.BaseAdapter.get_key_enforcer] is used. inference_fn: A function that takes in: - ```x```: the input to the model - ```domain```: an integer representing the domain of the data - ```models```: a dictionary of models, i.e. ```self.models``` - ```misc```: a dictionary of misc objects, i.e. ```self.misc``` before_training_starts: A function that takes in this adapter and returns another function that is optionally called by a framework wrapper before training starts. hook_kwargs: A dictionary of keyword arguments that will be passed into the wrapped hook during initialization. \"\"\" containers = c_f . default ( default_containers , self . get_default_containers , {}) self . key_enforcer = c_f . default ( key_enforcer , self . get_key_enforcer , {}) self . before_training_starts = c_f . class_default ( self , before_training_starts , self . before_training_starts_default ) containers . merge ( models = models , optimizers = optimizers , lr_schedulers = lr_schedulers , misc = misc , ) hook_kwargs = c_f . default ( hook_kwargs , {}) self . init_containers_and_check_keys ( containers ) self . init_hook ( hook_kwargs ) self . inference_fn = c_f . default ( inference_fn , default_fn ) get_default_containers () \u00b6 Returns: Type Description MultipleContainers The default set of containers. Consists of an MultipleContainers Optimizers MultipleContainers container using the default MultipleContainers of Adam with lr 0.0001. Source code in pytorch_adapt\\adapters\\base_adapter.py 127 128 129 130 131 132 133 134 135 136 def get_default_containers ( self ) -> MultipleContainers : \"\"\" Returns: The default set of containers. Consists of an [Optimizers][pytorch_adapt.containers.Optimizers] container using the [default][pytorch_adapt.adapters.utils.default_optimizer_tuple] of Adam with lr 0.0001. \"\"\" optimizers = Optimizers ( default_optimizer_tuple ()) return MultipleContainers ( optimizers = optimizers ) get_key_enforcer () abstractmethod \u00b6 Returns: Type Description KeyEnforcer The default KeyEnforcer. Source code in pytorch_adapt\\adapters\\base_adapter.py 138 139 140 141 142 143 144 @abstractmethod def get_key_enforcer ( self ) -> KeyEnforcer : \"\"\" Returns: The default KeyEnforcer. \"\"\" pass inference ( x , domain = None ) \u00b6 Parameters: Name Type Description Default x torch . Tensor The input to the model required domain int An optional integer indicating the domain. None Returns: Type Description Tuple [ torch . Tensor , torch . Tensor ] Features and logits Source code in pytorch_adapt\\adapters\\base_adapter.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def inference ( self , x : torch . Tensor , domain : int = None ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Arguments: x: The input to the model domain: An optional integer indicating the domain. Returns: Features and logits \"\"\" return self . inference_fn ( x = x , domain = domain , models = self . models , misc = self . misc , ) init_containers_and_check_keys ( containers ) \u00b6 Called in __init__ before init_hook . Source code in pytorch_adapt\\adapters\\base_adapter.py 153 154 155 156 157 158 159 160 161 def init_containers_and_check_keys ( self , containers ): \"\"\" Called in ```__init__``` before [```init_hook```][pytorch_adapt.adapters.BaseAdapter.init_hook]. \"\"\" containers . create () self . key_enforcer . check ( containers ) for k , v in containers . items (): setattr ( self , k , v ) init_hook () abstractmethod \u00b6 self.hook is initialized here. Source code in pytorch_adapt\\adapters\\base_adapter.py 146 147 148 149 150 151 @abstractmethod def init_hook ( self ): \"\"\" ```self.hook``` is initialized here. \"\"\" pass training_step ( batch , ** kwargs ) \u00b6 Calls the wrapped hook at each iteration during training. Parameters: Name Type Description Default batch Dict [ str , Any ] A dictionary containing training data. required **kwargs Any other data that will be passed into the hook. {} Returns: Type Description Dict [ str , Dict [ str , float ]] A two-level dictionary Dict [ str , Dict [ str , float ]] the outer level is associated with a particular optimization step (relevant for GAN architectures), Dict [ str , Dict [ str , float ]] the inner level contains the loss components. Source code in pytorch_adapt\\adapters\\base_adapter.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def training_step ( self , batch : Dict [ str , Any ], ** kwargs ) -> Dict [ str , Dict [ str , float ]]: \"\"\" Calls the wrapped hook at each iteration during training. Arguments: batch: A dictionary containing training data. **kwargs: Any other data that will be passed into the hook. Returns: A two-level dictionary - the outer level is associated with a particular optimization step (relevant for GAN architectures), - the inner level contains the loss components. \"\"\" combined = c_f . assert_dicts_are_disjoint ( self . models , self . misc , with_opt ( self . optimizers ), batch , kwargs ) _ , losses = self . hook ( combined ) return losses BaseGCAdapter \u00b6 Bases: BaseAdapter Base class for adapters that use a Generator and Classifier. Source code in pytorch_adapt\\adapters\\base_adapter.py 183 184 185 186 187 188 189 190 191 192 class BaseGCAdapter ( BaseAdapter ): \"\"\" Base class for adapters that use a Generator and Classifier. \"\"\" def get_key_enforcer ( self ) -> KeyEnforcer : return KeyEnforcer ( models = [ \"G\" , \"C\" ], optimizers = [ \"G\" , \"C\" ], ) BaseGCDAdapter \u00b6 Bases: BaseAdapter Base class for adapters that use a Generator, Classifier, and Discriminator. Source code in pytorch_adapt\\adapters\\base_adapter.py 171 172 173 174 175 176 177 178 179 180 class BaseGCDAdapter ( BaseAdapter ): \"\"\" Base class for adapters that use a Generator, Classifier, and Discriminator. \"\"\" def get_key_enforcer ( self ) -> KeyEnforcer : return KeyEnforcer ( models = [ \"G\" , \"C\" , \"D\" ], optimizers = [ \"G\" , \"C\" , \"D\" ], )","title":"base_adapter"},{"location":"docs/adapters/base_adapter/#pytorch_adapt.adapters.base_adapter.BaseAdapter","text":"Bases: ABC Parent class of all adapters. Source code in pytorch_adapt\\adapters\\base_adapter.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class BaseAdapter ( ABC ): \"\"\" Parent class of all adapters. \"\"\" def __init__ ( self , models : Models = None , optimizers : Optimizers = None , lr_schedulers : LRSchedulers = None , misc : Misc = None , default_containers : MultipleContainers = None , key_enforcer : KeyEnforcer = None , inference_fn = None , before_training_starts = None , hook_kwargs : Dict [ str , Any ] = None , ): \"\"\" Arguments: models: A [```Models```][pytorch_adapt.containers.Models] container. The models will be passed to the wrapped hook at each training iteration. optimizers: An [```Optimizers```][pytorch_adapt.containers.Optimizers] container. The optimizers will be passed to the wrapped hook at each training iteration. lr_schedulers: An [```LRSchedulers```][pytorch_adapt.containers.LRSchedulers] container. The lr schedulers are called automatically by the [```framework```](../frameworks/index.md) that wrap this adapter. misc: A [```Misc```][pytorch_adapt.containers.Misc] container for models that don't require optimizers, and other miscellaneous objects. These are passed into the wrapped hook at each training iteration. default_containers: The default set of containers to use, wrapped in a [```MultipleContainers```][pytorch_adapt.containers.MultipleContainers] object. If ```None``` then the default containers are defined in [```self.get_default_containers```][pytorch_adapt.adapters.BaseAdapter.get_default_containers] key_enforcer: A [```KeyEnforcer```][pytorch_adapt.containers.KeyEnforcer] object. If ```None```, then [```self.get_key_enforcer```][pytorch_adapt.adapters.BaseAdapter.get_key_enforcer] is used. inference_fn: A function that takes in: - ```x```: the input to the model - ```domain```: an integer representing the domain of the data - ```models```: a dictionary of models, i.e. ```self.models``` - ```misc```: a dictionary of misc objects, i.e. ```self.misc``` before_training_starts: A function that takes in this adapter and returns another function that is optionally called by a framework wrapper before training starts. hook_kwargs: A dictionary of keyword arguments that will be passed into the wrapped hook during initialization. \"\"\" containers = c_f . default ( default_containers , self . get_default_containers , {}) self . key_enforcer = c_f . default ( key_enforcer , self . get_key_enforcer , {}) self . before_training_starts = c_f . class_default ( self , before_training_starts , self . before_training_starts_default ) containers . merge ( models = models , optimizers = optimizers , lr_schedulers = lr_schedulers , misc = misc , ) hook_kwargs = c_f . default ( hook_kwargs , {}) self . init_containers_and_check_keys ( containers ) self . init_hook ( hook_kwargs ) self . inference_fn = c_f . default ( inference_fn , default_fn ) def training_step ( self , batch : Dict [ str , Any ], ** kwargs ) -> Dict [ str , Dict [ str , float ]]: \"\"\" Calls the wrapped hook at each iteration during training. Arguments: batch: A dictionary containing training data. **kwargs: Any other data that will be passed into the hook. Returns: A two-level dictionary - the outer level is associated with a particular optimization step (relevant for GAN architectures), - the inner level contains the loss components. \"\"\" combined = c_f . assert_dicts_are_disjoint ( self . models , self . misc , with_opt ( self . optimizers ), batch , kwargs ) _ , losses = self . hook ( combined ) return losses def inference ( self , x : torch . Tensor , domain : int = None ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Arguments: x: The input to the model domain: An optional integer indicating the domain. Returns: Features and logits \"\"\" return self . inference_fn ( x = x , domain = domain , models = self . models , misc = self . misc , ) def get_default_containers ( self ) -> MultipleContainers : \"\"\" Returns: The default set of containers. Consists of an [Optimizers][pytorch_adapt.containers.Optimizers] container using the [default][pytorch_adapt.adapters.utils.default_optimizer_tuple] of Adam with lr 0.0001. \"\"\" optimizers = Optimizers ( default_optimizer_tuple ()) return MultipleContainers ( optimizers = optimizers ) @abstractmethod def get_key_enforcer ( self ) -> KeyEnforcer : \"\"\" Returns: The default KeyEnforcer. \"\"\" pass @abstractmethod def init_hook ( self ): \"\"\" ```self.hook``` is initialized here. \"\"\" pass def init_containers_and_check_keys ( self , containers ): \"\"\" Called in ```__init__``` before [```init_hook```][pytorch_adapt.adapters.BaseAdapter.init_hook]. \"\"\" containers . create () self . key_enforcer . check ( containers ) for k , v in containers . items (): setattr ( self , k , v ) def before_training_starts_default ( self , framework ): c_f . LOGGER . debug ( f \"models \\n { self . models } \" ) c_f . LOGGER . debug ( f \"optimizers \\n { self . optimizers } \" ) c_f . LOGGER . debug ( f \"lr_schedulers \\n { self . lr_schedulers } \" ) c_f . LOGGER . debug ( f \"misc \\n { self . misc } \" ) c_f . LOGGER . debug ( f \"hook \\n { self . hook } \" )","title":"BaseAdapter"},{"location":"docs/adapters/base_adapter/#pytorch_adapt.adapters.base_adapter.BaseGCAdapter","text":"Bases: BaseAdapter Base class for adapters that use a Generator and Classifier. Source code in pytorch_adapt\\adapters\\base_adapter.py 183 184 185 186 187 188 189 190 191 192 class BaseGCAdapter ( BaseAdapter ): \"\"\" Base class for adapters that use a Generator and Classifier. \"\"\" def get_key_enforcer ( self ) -> KeyEnforcer : return KeyEnforcer ( models = [ \"G\" , \"C\" ], optimizers = [ \"G\" , \"C\" ], )","title":"BaseGCAdapter"},{"location":"docs/adapters/base_adapter/#pytorch_adapt.adapters.base_adapter.BaseGCDAdapter","text":"Bases: BaseAdapter Base class for adapters that use a Generator, Classifier, and Discriminator. Source code in pytorch_adapt\\adapters\\base_adapter.py 171 172 173 174 175 176 177 178 179 180 class BaseGCDAdapter ( BaseAdapter ): \"\"\" Base class for adapters that use a Generator, Classifier, and Discriminator. \"\"\" def get_key_enforcer ( self ) -> KeyEnforcer : return KeyEnforcer ( models = [ \"G\" , \"C\" , \"D\" ], optimizers = [ \"G\" , \"C\" , \"D\" ], )","title":"BaseGCDAdapter"},{"location":"docs/adapters/classifier/","text":"Classifier \u00b6 Bases: BaseGCAdapter Wraps ClassifierHook . Container Required keys models [\"G\", \"C\"] optimizers [\"G\", \"C\"] Source code in pytorch_adapt\\adapters\\classifier.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Classifier ( BaseGCAdapter ): \"\"\" Wraps [ClassifierHook][pytorch_adapt.hooks.ClassifierHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\"]```| |optimizers|```[\"G\", \"C\"]```| \"\"\" hook_cls = ClassifierHook def init_hook ( self , hook_kwargs ): opts = with_opt ( list ( self . optimizers . keys ())) self . hook = self . hook_cls ( opts = opts , ** hook_kwargs ) Finetuner \u00b6 Bases: Classifier Wraps FinetunerHook . Container Required keys models [\"G\", \"C\"] optimizers [\"C\"] Source code in pytorch_adapt\\adapters\\classifier.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class Finetuner ( Classifier ): \"\"\" Wraps [FinetunerHook][pytorch_adapt.hooks.FinetunerHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\"]```| |optimizers|```[\"C\"]```| \"\"\" hook_cls = FinetunerHook def get_default_containers ( self ) -> MultipleContainers : optimizers = Optimizers ( default_optimizer_tuple (), keys = [ \"C\" ]) return MultipleContainers ( optimizers = optimizers ) def get_key_enforcer ( self ) -> KeyEnforcer : ke = super () . get_key_enforcer () ke . requirements [ \"optimizers\" ] . remove ( \"G\" ) return ke","title":"classifier"},{"location":"docs/adapters/classifier/#pytorch_adapt.adapters.classifier.Classifier","text":"Bases: BaseGCAdapter Wraps ClassifierHook . Container Required keys models [\"G\", \"C\"] optimizers [\"G\", \"C\"] Source code in pytorch_adapt\\adapters\\classifier.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Classifier ( BaseGCAdapter ): \"\"\" Wraps [ClassifierHook][pytorch_adapt.hooks.ClassifierHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\"]```| |optimizers|```[\"G\", \"C\"]```| \"\"\" hook_cls = ClassifierHook def init_hook ( self , hook_kwargs ): opts = with_opt ( list ( self . optimizers . keys ())) self . hook = self . hook_cls ( opts = opts , ** hook_kwargs )","title":"Classifier"},{"location":"docs/adapters/classifier/#pytorch_adapt.adapters.classifier.Finetuner","text":"Bases: Classifier Wraps FinetunerHook . Container Required keys models [\"G\", \"C\"] optimizers [\"C\"] Source code in pytorch_adapt\\adapters\\classifier.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class Finetuner ( Classifier ): \"\"\" Wraps [FinetunerHook][pytorch_adapt.hooks.FinetunerHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\"]```| |optimizers|```[\"C\"]```| \"\"\" hook_cls = FinetunerHook def get_default_containers ( self ) -> MultipleContainers : optimizers = Optimizers ( default_optimizer_tuple (), keys = [ \"C\" ]) return MultipleContainers ( optimizers = optimizers ) def get_key_enforcer ( self ) -> KeyEnforcer : ke = super () . get_key_enforcer () ke . requirements [ \"optimizers\" ] . remove ( \"G\" ) return ke","title":"Finetuner"},{"location":"docs/adapters/dann/","text":"DANN \u00b6 Bases: BaseGCDAdapter Wraps DANNHook . Container Required keys models [\"G\", \"C\", \"D\"] optimizers [\"G\", \"C\", \"D\"] Source code in pytorch_adapt\\adapters\\dann.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class DANN ( BaseGCDAdapter ): \"\"\" Wraps [DANNHook][pytorch_adapt.hooks.DANNHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"D\"]```| |optimizers|```[\"G\", \"C\", \"D\"]```| \"\"\" hook_cls = DANNHook def init_hook ( self , hook_kwargs ): opts = with_opt ( list ( self . optimizers . keys ())) self . hook = self . hook_cls ( opts = opts , ** hook_kwargs ) GVB \u00b6 Bases: DANN Wraps GVBHook . Container Required keys models [\"G\", \"C\", \"D\"] optimizers [\"G\", \"C\", \"D\"] Models D and C must be of type ModelWithBridge . If not, they will be converted into instances of ModelWithBridge , with each bridge being a re-initialized copy of each model. Source code in pytorch_adapt\\adapters\\dann.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class GVB ( DANN ): \"\"\" Wraps [GVBHook][pytorch_adapt.hooks.GVBHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"D\"]```| |optimizers|```[\"G\", \"C\", \"D\"]```| Models D and C must be of type [```ModelWithBridge```][pytorch_adapt.layers.ModelWithBridge]. If not, they will be converted into instances of ```ModelWithBridge```, with each bridge being a re-initialized copy of each model. \"\"\" hook_cls = GVBHook def init_containers_and_check_keys ( self , containers ): models = containers [ \"models\" ] for k in [ \"D\" , \"C\" ]: if not isinstance ( models [ k ], ModelWithBridge ): models [ k ] = ModelWithBridge ( models [ k ]) super () . init_containers_and_check_keys ( containers )","title":"dann"},{"location":"docs/adapters/dann/#pytorch_adapt.adapters.dann.DANN","text":"Bases: BaseGCDAdapter Wraps DANNHook . Container Required keys models [\"G\", \"C\", \"D\"] optimizers [\"G\", \"C\", \"D\"] Source code in pytorch_adapt\\adapters\\dann.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class DANN ( BaseGCDAdapter ): \"\"\" Wraps [DANNHook][pytorch_adapt.hooks.DANNHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"D\"]```| |optimizers|```[\"G\", \"C\", \"D\"]```| \"\"\" hook_cls = DANNHook def init_hook ( self , hook_kwargs ): opts = with_opt ( list ( self . optimizers . keys ())) self . hook = self . hook_cls ( opts = opts , ** hook_kwargs )","title":"DANN"},{"location":"docs/adapters/dann/#pytorch_adapt.adapters.dann.GVB","text":"Bases: DANN Wraps GVBHook . Container Required keys models [\"G\", \"C\", \"D\"] optimizers [\"G\", \"C\", \"D\"] Models D and C must be of type ModelWithBridge . If not, they will be converted into instances of ModelWithBridge , with each bridge being a re-initialized copy of each model. Source code in pytorch_adapt\\adapters\\dann.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class GVB ( DANN ): \"\"\" Wraps [GVBHook][pytorch_adapt.hooks.GVBHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"D\"]```| |optimizers|```[\"G\", \"C\", \"D\"]```| Models D and C must be of type [```ModelWithBridge```][pytorch_adapt.layers.ModelWithBridge]. If not, they will be converted into instances of ```ModelWithBridge```, with each bridge being a re-initialized copy of each model. \"\"\" hook_cls = GVBHook def init_containers_and_check_keys ( self , containers ): models = containers [ \"models\" ] for k in [ \"D\" , \"C\" ]: if not isinstance ( models [ k ], ModelWithBridge ): models [ k ] = ModelWithBridge ( models [ k ]) super () . init_containers_and_check_keys ( containers )","title":"GVB"},{"location":"docs/adapters/gan/","text":"CDAN \u00b6 Bases: GAN Wraps CDANHook . Container Required keys models [\"G\", \"C\", \"D\"] optimizers [\"G\", \"C\", \"D\"] misc [\"feature_combiner\"] Source code in pytorch_adapt\\adapters\\gan.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class CDAN ( GAN ): \"\"\" Wraps [CDANHook][pytorch_adapt.hooks.CDANHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"D\"]```| |optimizers|```[\"G\", \"C\", \"D\"]```| |misc|```[\"feature_combiner\"]```| \"\"\" hook_cls = CDANHook def get_key_enforcer ( self ) -> KeyEnforcer : ke = super () . get_key_enforcer () ke . requirements [ \"misc\" ] = [ \"feature_combiner\" ] return ke DomainConfusion \u00b6 Bases: GAN Wraps DomainConfusionHook . Container Required keys models [\"G\", \"C\", \"D\"] optimizers [\"G\", \"C\", \"D\"] Source code in pytorch_adapt\\adapters\\gan.py 61 62 63 64 65 66 67 68 69 70 71 class DomainConfusion ( GAN ): \"\"\" Wraps [DomainConfusionHook][pytorch_adapt.hooks.DomainConfusionHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"D\"]```| |optimizers|```[\"G\", \"C\", \"D\"]```| \"\"\" hook_cls = DomainConfusionHook GAN \u00b6 Bases: BaseGCDAdapter Wraps GANHook . Container Required keys models [\"G\", \"C\", \"D\"] optimizers [\"G\", \"C\", \"D\"] Source code in pytorch_adapt\\adapters\\gan.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class GAN ( BaseGCDAdapter ): \"\"\" Wraps [GANHook][pytorch_adapt.hooks.GANHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"D\"]```| |optimizers|```[\"G\", \"C\", \"D\"]```| \"\"\" hook_cls = GANHook def init_hook ( self , hook_kwargs ): g_opts = with_opt ([ \"G\" , \"C\" ]) d_opts = with_opt ([ \"D\" ]) self . hook = self . hook_cls ( d_opts = d_opts , g_opts = g_opts , ** hook_kwargs ) VADA \u00b6 Bases: GAN Wraps VADAHook . Container Required keys models [\"G\", \"C\", \"D\"] optimizers [\"G\", \"C\", \"D\"] misc [\"combined_model\"] The \"combined_model\" key does not need to be passed in. It is simply torch.nn.Sequential(G, C) , and is set automatically during initialization. Source code in pytorch_adapt\\adapters\\gan.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class VADA ( GAN ): \"\"\" Wraps [VADAHook][pytorch_adapt.hooks.VADAHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"D\"]```| |optimizers|```[\"G\", \"C\", \"D\"]```| |misc|```[\"combined_model\"]```| The ```\"combined_model\"``` key does not need to be passed in. It is simply ```torch.nn.Sequential(G, C)```, and is set automatically during initialization. \"\"\" hook_cls = VADAHook def init_containers_and_check_keys ( self , containers ): models = containers [ \"models\" ] misc = containers [ \"misc\" ] misc [ \"combined_model\" ] = torch . nn . Sequential ( models [ \"G\" ], models [ \"C\" ]) super () . init_containers_and_check_keys ( containers ) def get_key_enforcer ( self ) -> KeyEnforcer : ke = super () . get_key_enforcer () ke . requirements [ \"misc\" ] = [ \"combined_model\" ] return ke","title":"gan"},{"location":"docs/adapters/gan/#pytorch_adapt.adapters.gan.CDAN","text":"Bases: GAN Wraps CDANHook . Container Required keys models [\"G\", \"C\", \"D\"] optimizers [\"G\", \"C\", \"D\"] misc [\"feature_combiner\"] Source code in pytorch_adapt\\adapters\\gan.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class CDAN ( GAN ): \"\"\" Wraps [CDANHook][pytorch_adapt.hooks.CDANHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"D\"]```| |optimizers|```[\"G\", \"C\", \"D\"]```| |misc|```[\"feature_combiner\"]```| \"\"\" hook_cls = CDANHook def get_key_enforcer ( self ) -> KeyEnforcer : ke = super () . get_key_enforcer () ke . requirements [ \"misc\" ] = [ \"feature_combiner\" ] return ke","title":"CDAN"},{"location":"docs/adapters/gan/#pytorch_adapt.adapters.gan.DomainConfusion","text":"Bases: GAN Wraps DomainConfusionHook . Container Required keys models [\"G\", \"C\", \"D\"] optimizers [\"G\", \"C\", \"D\"] Source code in pytorch_adapt\\adapters\\gan.py 61 62 63 64 65 66 67 68 69 70 71 class DomainConfusion ( GAN ): \"\"\" Wraps [DomainConfusionHook][pytorch_adapt.hooks.DomainConfusionHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"D\"]```| |optimizers|```[\"G\", \"C\", \"D\"]```| \"\"\" hook_cls = DomainConfusionHook","title":"DomainConfusion"},{"location":"docs/adapters/gan/#pytorch_adapt.adapters.gan.GAN","text":"Bases: BaseGCDAdapter Wraps GANHook . Container Required keys models [\"G\", \"C\", \"D\"] optimizers [\"G\", \"C\", \"D\"] Source code in pytorch_adapt\\adapters\\gan.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class GAN ( BaseGCDAdapter ): \"\"\" Wraps [GANHook][pytorch_adapt.hooks.GANHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"D\"]```| |optimizers|```[\"G\", \"C\", \"D\"]```| \"\"\" hook_cls = GANHook def init_hook ( self , hook_kwargs ): g_opts = with_opt ([ \"G\" , \"C\" ]) d_opts = with_opt ([ \"D\" ]) self . hook = self . hook_cls ( d_opts = d_opts , g_opts = g_opts , ** hook_kwargs )","title":"GAN"},{"location":"docs/adapters/gan/#pytorch_adapt.adapters.gan.VADA","text":"Bases: GAN Wraps VADAHook . Container Required keys models [\"G\", \"C\", \"D\"] optimizers [\"G\", \"C\", \"D\"] misc [\"combined_model\"] The \"combined_model\" key does not need to be passed in. It is simply torch.nn.Sequential(G, C) , and is set automatically during initialization. Source code in pytorch_adapt\\adapters\\gan.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class VADA ( GAN ): \"\"\" Wraps [VADAHook][pytorch_adapt.hooks.VADAHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\", \"D\"]```| |optimizers|```[\"G\", \"C\", \"D\"]```| |misc|```[\"combined_model\"]```| The ```\"combined_model\"``` key does not need to be passed in. It is simply ```torch.nn.Sequential(G, C)```, and is set automatically during initialization. \"\"\" hook_cls = VADAHook def init_containers_and_check_keys ( self , containers ): models = containers [ \"models\" ] misc = containers [ \"misc\" ] misc [ \"combined_model\" ] = torch . nn . Sequential ( models [ \"G\" ], models [ \"C\" ]) super () . init_containers_and_check_keys ( containers ) def get_key_enforcer ( self ) -> KeyEnforcer : ke = super () . get_key_enforcer () ke . requirements [ \"misc\" ] = [ \"combined_model\" ] return ke","title":"VADA"},{"location":"docs/adapters/mcd/","text":"MCD \u00b6 Bases: BaseGCAdapter Wraps MCDHook . Container Required keys models [\"G\", \"C\"] optimizers [\"G\", \"C\"] The C model must output a list of logits, where each list element corresponds with a separate classifier. Usually the number of classifiers is 2, so C should output [logits1, logits2] . Source code in pytorch_adapt\\adapters\\mcd.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class MCD ( BaseGCAdapter ): \"\"\" Wraps [MCDHook][pytorch_adapt.hooks.MCDHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\"]```| |optimizers|```[\"G\", \"C\"]```| The C model must output a list of logits, where each list element corresponds with a separate classifier. Usually the number of classifiers is 2, so C should output ```[logits1, logits2]```. \"\"\" hook_cls = MCDHook def __init__ ( self , * args , inference_fn = None , ** kwargs ): inference_fn = c_f . default ( inference_fn , mcd_fn ) super () . __init__ ( * args , inference_fn = inference_fn , ** kwargs ) def init_hook ( self , hook_kwargs ): self . hook = self . hook_cls ( g_opts = with_opt ([ \"G\" ]), c_opts = with_opt ([ \"C\" ]), ** hook_kwargs )","title":"mcd"},{"location":"docs/adapters/mcd/#pytorch_adapt.adapters.mcd.MCD","text":"Bases: BaseGCAdapter Wraps MCDHook . Container Required keys models [\"G\", \"C\"] optimizers [\"G\", \"C\"] The C model must output a list of logits, where each list element corresponds with a separate classifier. Usually the number of classifiers is 2, so C should output [logits1, logits2] . Source code in pytorch_adapt\\adapters\\mcd.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class MCD ( BaseGCAdapter ): \"\"\" Wraps [MCDHook][pytorch_adapt.hooks.MCDHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\"]```| |optimizers|```[\"G\", \"C\"]```| The C model must output a list of logits, where each list element corresponds with a separate classifier. Usually the number of classifiers is 2, so C should output ```[logits1, logits2]```. \"\"\" hook_cls = MCDHook def __init__ ( self , * args , inference_fn = None , ** kwargs ): inference_fn = c_f . default ( inference_fn , mcd_fn ) super () . __init__ ( * args , inference_fn = inference_fn , ** kwargs ) def init_hook ( self , hook_kwargs ): self . hook = self . hook_cls ( g_opts = with_opt ([ \"G\" ]), c_opts = with_opt ([ \"C\" ]), ** hook_kwargs )","title":"MCD"},{"location":"docs/adapters/symnets/","text":"SymNets \u00b6 Bases: BaseGCAdapter Wraps SymNetsHook . Container Required keys models [\"G\", \"C\"] optimizers [\"G\", \"C\"] The C model must output a list of logits: [logits1, logits2] . Source code in pytorch_adapt\\adapters\\symnets.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class SymNets ( BaseGCAdapter ): \"\"\" Wraps [SymNetsHook][pytorch_adapt.hooks.SymNetsHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\"]```| |optimizers|```[\"G\", \"C\"]```| The C model must output a list of logits: ```[logits1, logits2]```. \"\"\" hook_cls = SymNetsHook def __init__ ( self , * args , inference_fn = None , ** kwargs ): inference_fn = c_f . default ( inference_fn , symnets_fn ) super () . __init__ ( * args , inference_fn = inference_fn , ** kwargs ) def init_hook ( self , hook_kwargs ): self . hook = self . hook_cls ( g_opts = with_opt ([ \"G\" ]), c_opts = with_opt ([ \"C\" ]), ** hook_kwargs )","title":"symnets"},{"location":"docs/adapters/symnets/#pytorch_adapt.adapters.symnets.SymNets","text":"Bases: BaseGCAdapter Wraps SymNetsHook . Container Required keys models [\"G\", \"C\"] optimizers [\"G\", \"C\"] The C model must output a list of logits: [logits1, logits2] . Source code in pytorch_adapt\\adapters\\symnets.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class SymNets ( BaseGCAdapter ): \"\"\" Wraps [SymNetsHook][pytorch_adapt.hooks.SymNetsHook]. |Container|Required keys| |---|---| |models|```[\"G\", \"C\"]```| |optimizers|```[\"G\", \"C\"]```| The C model must output a list of logits: ```[logits1, logits2]```. \"\"\" hook_cls = SymNetsHook def __init__ ( self , * args , inference_fn = None , ** kwargs ): inference_fn = c_f . default ( inference_fn , symnets_fn ) super () . __init__ ( * args , inference_fn = inference_fn , ** kwargs ) def init_hook ( self , hook_kwargs ): self . hook = self . hook_cls ( g_opts = with_opt ([ \"G\" ]), c_opts = with_opt ([ \"C\" ]), ** hook_kwargs )","title":"SymNets"},{"location":"docs/adapters/utils/","text":"default_optimizer_tuple () \u00b6 Returns: Type Description torch . optim . Optimizer A tuple to be passed into an Optimizers Dict [ str , Any ] container. The tuple specifies an Adam optimizer with lr 0.0001. Source code in pytorch_adapt\\adapters\\utils.py 7 8 9 10 11 12 13 def default_optimizer_tuple () -> Tuple [ torch . optim . Optimizer , Dict [ str , Any ]]: \"\"\" Returns: A tuple to be passed into an [Optimizers][pytorch_adapt.containers.Optimizers] container. The tuple specifies an Adam optimizer with lr 0.0001. \"\"\" return ( torch . optim . Adam , { \"lr\" : 0.0001 })","title":"utils"},{"location":"docs/adapters/utils/#pytorch_adapt.adapters.utils.default_optimizer_tuple","text":"Returns: Type Description torch . optim . Optimizer A tuple to be passed into an Optimizers Dict [ str , Any ] container. The tuple specifies an Adam optimizer with lr 0.0001. Source code in pytorch_adapt\\adapters\\utils.py 7 8 9 10 11 12 13 def default_optimizer_tuple () -> Tuple [ torch . optim . Optimizer , Dict [ str , Any ]]: \"\"\" Returns: A tuple to be passed into an [Optimizers][pytorch_adapt.containers.Optimizers] container. The tuple specifies an Adam optimizer with lr 0.0001. \"\"\" return ( torch . optim . Adam , { \"lr\" : 0.0001 })","title":"default_optimizer_tuple()"},{"location":"docs/containers/","text":"The following can be imported like this (using BaseContainer as an example): from pytorch_adapt.containers import BaseContainer Direct module members \u00b6 BaseContainer DeleteKey KeyEnforcer LRSchedulers Misc Models MultipleContainers Optimizers","title":"containers"},{"location":"docs/containers/#direct-module-members","text":"BaseContainer DeleteKey KeyEnforcer LRSchedulers Misc Models MultipleContainers Optimizers","title":"Direct module members"},{"location":"docs/containers/base_container/","text":"BaseContainer \u00b6 Bases: MutableMapping The parent class of all containers. Containers are dictionaries with extra functionality that simplify object creation. Source code in pytorch_adapt\\containers\\base_container.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 class BaseContainer ( MutableMapping ): \"\"\" The parent class of all containers. Containers are dictionaries with extra functionality that simplify object creation. \"\"\" def __init__ ( self , store , other = None , keys = None ): \"\"\" Arguments: store: A tuple or dictionary - A tuple consists of ```(<class_ref>, <init kwargs>)```. For example, ```(torch.optim.Adam, {\"lr\": 0.1})``` - A dictionary maps from object name to either a tuple or a fully constructed object. other: Another container which is used in the process of creating objects in this container, (e.g optimizers require model parameters). keys: Converts ```store``` from tuple to dict format, where each dict value is the tuple. This only works if ```store``` is passed in as a tuple. \"\"\" if not isinstance ( store , ( tuple , dict )): raise TypeError ( \"BaseContainer input must be a tuple or dict\" ) if isinstance ( store , tuple ): self . store_as_tuple = store self . store = {} else : self . store_as_tuple = None self . store = store if keys is not None : self . duplicate ( keys ) if other is not None : self . create_with ( other ) def __getitem__ ( self , key ): return self . store [ self . _keytransform ( key )] def __setitem__ ( self , key , value ): self . store [ self . _keytransform ( key )] = value def __delitem__ ( self , key ): del self . store [ self . _keytransform ( key )] def __iter__ ( self ): return iter ( self . store ) def __len__ ( self ): return len ( self . store ) def _keytransform ( self , key ): return key def __repr__ ( self ): if isinstance ( self . store , dict ): output = \"\" for k , v in self . items (): output += f \" { k } : { v } \\n \" return output return str ( self . store ) def merge ( self , other : \"BaseContainer\" ): \"\"\" Merges another container into this one. Arguments: other: The container that will be merged into this container. \"\"\" if not isinstance ( other , BaseContainer ): raise TypeError ( \"merge can only be done with another container\" ) if other . store_as_tuple : if len ( self ) > 0 : for k , v in self . items (): self [ k ] = other . store_as_tuple else : self . store_as_tuple = other . store_as_tuple else : for k , v in other . items (): self [ k ] = v def create ( self ): \"\"\" Initializes objects by converting all tuples in the store into objects. \"\"\" for k , v in self . items (): if isinstance ( v , tuple ): if len ( v ) == 2 : class_ref , kwargs = v self [ k ] = class_ref ( ** kwargs ) elif len ( v ) == 1 : self [ k ] = v [ 0 ] else : raise ValueError ( f \"The tuple { v } has length= { len ( v ) } , but it must be of length 1 or 2\" ) self . delete_unwanted_keys () def create_with ( self , other ): \"\"\" Initializes objects conditioned on the input container. \"\"\" self . store_as_tuple = self . type_check ( self . store_as_tuple , other ) self . store = self . type_check ( self . store , other ) self . store_as_tuple . update ( self . store ) self . store = self . store_as_tuple self . store_as_tuple = None self . delete_unwanted_keys () self . _create_with ( other ) def _create_with ( self , other ): pass def type_check ( self , store , other ): if isinstance ( store , tuple ): return { k : store for k in other . keys ()} elif isinstance ( store , dict ): return store elif store is None : return {} def duplicate ( self , keys ): if isinstance ( self . store_as_tuple , tuple ): self . store = { k : self . store_as_tuple for k in keys } self . store_as_tuple = None else : raise TypeError ( \"If keys are specified, store must be a tuple.\" ) def apply ( self , function , keys = None ): if keys is None : keys = list ( self . keys ()) for k in keys : self [ k ] = function ( self [ k ]) def delete_unwanted_keys ( self ): del_list = [] for k , v in self . items (): if isinstance ( v , DeleteKey ) or ( isinstance ( v , tuple ) and v [ 0 ] == DeleteKey ): del_list . append ( k ) for k in del_list : del self [ k ] def state_dict ( self ): return { k : v . state_dict () for k , v in self . items () if hasattr ( v , \"state_dict\" )} def load_state_dict ( self , state_dict ): c_f . assert_state_dict_keys ( state_dict , self . keys ()) for k , v in state_dict . items (): self [ k ] . load_state_dict ( v ) __init__ ( store , other = None , keys = None ) \u00b6 Parameters: Name Type Description Default store A tuple or dictionary A tuple consists of (<class_ref>, <init kwargs>) . For example, (torch.optim.Adam, {\"lr\": 0.1}) A dictionary maps from object name to either a tuple or a fully constructed object. required other Another container which is used in the process of creating objects in this container, (e.g optimizers require model parameters). None keys Converts store from tuple to dict format, where each dict value is the tuple. This only works if store is passed in as a tuple. None Source code in pytorch_adapt\\containers\\base_container.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , store , other = None , keys = None ): \"\"\" Arguments: store: A tuple or dictionary - A tuple consists of ```(<class_ref>, <init kwargs>)```. For example, ```(torch.optim.Adam, {\"lr\": 0.1})``` - A dictionary maps from object name to either a tuple or a fully constructed object. other: Another container which is used in the process of creating objects in this container, (e.g optimizers require model parameters). keys: Converts ```store``` from tuple to dict format, where each dict value is the tuple. This only works if ```store``` is passed in as a tuple. \"\"\" if not isinstance ( store , ( tuple , dict )): raise TypeError ( \"BaseContainer input must be a tuple or dict\" ) if isinstance ( store , tuple ): self . store_as_tuple = store self . store = {} else : self . store_as_tuple = None self . store = store if keys is not None : self . duplicate ( keys ) if other is not None : self . create_with ( other ) create () \u00b6 Initializes objects by converting all tuples in the store into objects. Source code in pytorch_adapt\\containers\\base_container.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def create ( self ): \"\"\" Initializes objects by converting all tuples in the store into objects. \"\"\" for k , v in self . items (): if isinstance ( v , tuple ): if len ( v ) == 2 : class_ref , kwargs = v self [ k ] = class_ref ( ** kwargs ) elif len ( v ) == 1 : self [ k ] = v [ 0 ] else : raise ValueError ( f \"The tuple { v } has length= { len ( v ) } , but it must be of length 1 or 2\" ) self . delete_unwanted_keys () create_with ( other ) \u00b6 Initializes objects conditioned on the input container. Source code in pytorch_adapt\\containers\\base_container.py 108 109 110 111 112 113 114 115 116 117 118 def create_with ( self , other ): \"\"\" Initializes objects conditioned on the input container. \"\"\" self . store_as_tuple = self . type_check ( self . store_as_tuple , other ) self . store = self . type_check ( self . store , other ) self . store_as_tuple . update ( self . store ) self . store = self . store_as_tuple self . store_as_tuple = None self . delete_unwanted_keys () self . _create_with ( other ) merge ( other ) \u00b6 Merges another container into this one. Parameters: Name Type Description Default other 'BaseContainer' The container that will be merged into this container. required Source code in pytorch_adapt\\containers\\base_container.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def merge ( self , other : \"BaseContainer\" ): \"\"\" Merges another container into this one. Arguments: other: The container that will be merged into this container. \"\"\" if not isinstance ( other , BaseContainer ): raise TypeError ( \"merge can only be done with another container\" ) if other . store_as_tuple : if len ( self ) > 0 : for k , v in self . items (): self [ k ] = other . store_as_tuple else : self . store_as_tuple = other . store_as_tuple else : for k , v in other . items (): self [ k ] = v","title":"base_container"},{"location":"docs/containers/base_container/#pytorch_adapt.containers.base_container.BaseContainer","text":"Bases: MutableMapping The parent class of all containers. Containers are dictionaries with extra functionality that simplify object creation. Source code in pytorch_adapt\\containers\\base_container.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 class BaseContainer ( MutableMapping ): \"\"\" The parent class of all containers. Containers are dictionaries with extra functionality that simplify object creation. \"\"\" def __init__ ( self , store , other = None , keys = None ): \"\"\" Arguments: store: A tuple or dictionary - A tuple consists of ```(<class_ref>, <init kwargs>)```. For example, ```(torch.optim.Adam, {\"lr\": 0.1})``` - A dictionary maps from object name to either a tuple or a fully constructed object. other: Another container which is used in the process of creating objects in this container, (e.g optimizers require model parameters). keys: Converts ```store``` from tuple to dict format, where each dict value is the tuple. This only works if ```store``` is passed in as a tuple. \"\"\" if not isinstance ( store , ( tuple , dict )): raise TypeError ( \"BaseContainer input must be a tuple or dict\" ) if isinstance ( store , tuple ): self . store_as_tuple = store self . store = {} else : self . store_as_tuple = None self . store = store if keys is not None : self . duplicate ( keys ) if other is not None : self . create_with ( other ) def __getitem__ ( self , key ): return self . store [ self . _keytransform ( key )] def __setitem__ ( self , key , value ): self . store [ self . _keytransform ( key )] = value def __delitem__ ( self , key ): del self . store [ self . _keytransform ( key )] def __iter__ ( self ): return iter ( self . store ) def __len__ ( self ): return len ( self . store ) def _keytransform ( self , key ): return key def __repr__ ( self ): if isinstance ( self . store , dict ): output = \"\" for k , v in self . items (): output += f \" { k } : { v } \\n \" return output return str ( self . store ) def merge ( self , other : \"BaseContainer\" ): \"\"\" Merges another container into this one. Arguments: other: The container that will be merged into this container. \"\"\" if not isinstance ( other , BaseContainer ): raise TypeError ( \"merge can only be done with another container\" ) if other . store_as_tuple : if len ( self ) > 0 : for k , v in self . items (): self [ k ] = other . store_as_tuple else : self . store_as_tuple = other . store_as_tuple else : for k , v in other . items (): self [ k ] = v def create ( self ): \"\"\" Initializes objects by converting all tuples in the store into objects. \"\"\" for k , v in self . items (): if isinstance ( v , tuple ): if len ( v ) == 2 : class_ref , kwargs = v self [ k ] = class_ref ( ** kwargs ) elif len ( v ) == 1 : self [ k ] = v [ 0 ] else : raise ValueError ( f \"The tuple { v } has length= { len ( v ) } , but it must be of length 1 or 2\" ) self . delete_unwanted_keys () def create_with ( self , other ): \"\"\" Initializes objects conditioned on the input container. \"\"\" self . store_as_tuple = self . type_check ( self . store_as_tuple , other ) self . store = self . type_check ( self . store , other ) self . store_as_tuple . update ( self . store ) self . store = self . store_as_tuple self . store_as_tuple = None self . delete_unwanted_keys () self . _create_with ( other ) def _create_with ( self , other ): pass def type_check ( self , store , other ): if isinstance ( store , tuple ): return { k : store for k in other . keys ()} elif isinstance ( store , dict ): return store elif store is None : return {} def duplicate ( self , keys ): if isinstance ( self . store_as_tuple , tuple ): self . store = { k : self . store_as_tuple for k in keys } self . store_as_tuple = None else : raise TypeError ( \"If keys are specified, store must be a tuple.\" ) def apply ( self , function , keys = None ): if keys is None : keys = list ( self . keys ()) for k in keys : self [ k ] = function ( self [ k ]) def delete_unwanted_keys ( self ): del_list = [] for k , v in self . items (): if isinstance ( v , DeleteKey ) or ( isinstance ( v , tuple ) and v [ 0 ] == DeleteKey ): del_list . append ( k ) for k in del_list : del self [ k ] def state_dict ( self ): return { k : v . state_dict () for k , v in self . items () if hasattr ( v , \"state_dict\" )} def load_state_dict ( self , state_dict ): c_f . assert_state_dict_keys ( state_dict , self . keys ()) for k , v in state_dict . items (): self [ k ] . load_state_dict ( v )","title":"BaseContainer"},{"location":"docs/containers/key_enforcer/","text":"KeyEnforcer \u00b6 Makes sure containers have the specified keys. Source code in pytorch_adapt\\containers\\key_enforcer.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class KeyEnforcer : \"\"\" Makes sure containers have the specified keys. \"\"\" def __init__ ( self , ** kwargs : List [ str ]): \"\"\" Arguments: **kwargs: A mapping from container name to a list of required keys for that container. \"\"\" self . requirements = kwargs def check ( self , containers : MultipleContainers ): \"\"\" Compares the input containers' keys to ```self.requirements```. Raises ```KeyError``` if there is a mismatch. Arguments: containers: The containers to check. \"\"\" for k , required_keys in self . requirements . items (): container_keys = list ( containers [ k ] . keys ()) r_c_diff = c_f . list_diff ( required_keys , container_keys ) c_r_diff = c_f . list_diff ( container_keys , required_keys ) error_msg = \"\" if len ( r_c_diff ) > 0 : error_msg += ( f \"The { k } container is missing the following keys: { r_c_diff } . \" ) if len ( c_r_diff ) > 0 : error_msg += ( f \"The { k } container has the following unallowed keys: { c_r_diff } .\" ) if error_msg != \"\" : raise KeyError ( error_msg ) __init__ ( ** kwargs ) \u00b6 Parameters: Name Type Description Default **kwargs List [ str ] A mapping from container name to a list of required keys for that container. {} Source code in pytorch_adapt\\containers\\key_enforcer.py 12 13 14 15 16 17 18 def __init__ ( self , ** kwargs : List [ str ]): \"\"\" Arguments: **kwargs: A mapping from container name to a list of required keys for that container. \"\"\" self . requirements = kwargs check ( containers ) \u00b6 Compares the input containers' keys to self.requirements . Raises KeyError if there is a mismatch. Parameters: Name Type Description Default containers MultipleContainers The containers to check. required Source code in pytorch_adapt\\containers\\key_enforcer.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def check ( self , containers : MultipleContainers ): \"\"\" Compares the input containers' keys to ```self.requirements```. Raises ```KeyError``` if there is a mismatch. Arguments: containers: The containers to check. \"\"\" for k , required_keys in self . requirements . items (): container_keys = list ( containers [ k ] . keys ()) r_c_diff = c_f . list_diff ( required_keys , container_keys ) c_r_diff = c_f . list_diff ( container_keys , required_keys ) error_msg = \"\" if len ( r_c_diff ) > 0 : error_msg += ( f \"The { k } container is missing the following keys: { r_c_diff } . \" ) if len ( c_r_diff ) > 0 : error_msg += ( f \"The { k } container has the following unallowed keys: { c_r_diff } .\" ) if error_msg != \"\" : raise KeyError ( error_msg )","title":"key_enforcer"},{"location":"docs/containers/key_enforcer/#pytorch_adapt.containers.key_enforcer.KeyEnforcer","text":"Makes sure containers have the specified keys. Source code in pytorch_adapt\\containers\\key_enforcer.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class KeyEnforcer : \"\"\" Makes sure containers have the specified keys. \"\"\" def __init__ ( self , ** kwargs : List [ str ]): \"\"\" Arguments: **kwargs: A mapping from container name to a list of required keys for that container. \"\"\" self . requirements = kwargs def check ( self , containers : MultipleContainers ): \"\"\" Compares the input containers' keys to ```self.requirements```. Raises ```KeyError``` if there is a mismatch. Arguments: containers: The containers to check. \"\"\" for k , required_keys in self . requirements . items (): container_keys = list ( containers [ k ] . keys ()) r_c_diff = c_f . list_diff ( required_keys , container_keys ) c_r_diff = c_f . list_diff ( container_keys , required_keys ) error_msg = \"\" if len ( r_c_diff ) > 0 : error_msg += ( f \"The { k } container is missing the following keys: { r_c_diff } . \" ) if len ( c_r_diff ) > 0 : error_msg += ( f \"The { k } container has the following unallowed keys: { c_r_diff } .\" ) if error_msg != \"\" : raise KeyError ( error_msg )","title":"KeyEnforcer"},{"location":"docs/containers/lr_schedulers/","text":"LRSchedulers \u00b6 Bases: BaseContainer A container for optimizer learning rate schedulers. Source code in pytorch_adapt\\containers\\lr_schedulers.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class LRSchedulers ( BaseContainer ): \"\"\" A container for optimizer learning rate schedulers. \"\"\" def __init__ ( self , store , scheduler_types = None , ** kwargs ): \"\"\" Arguments: store: See [```BaseContainer```][pytorch_adapt.containers.BaseContainer] scheduler_types: A dictionary mapping from scheduler type (```\"per_step\"``` or ```\"per_epoch\"```) to a list of object names. If ```None```, then all schedulers are assumed to be ```\"per_step\"``` **kwargs: [```BaseContainer```][pytorch_adapt.containers.BaseContainer] keyword arguments. \"\"\" self . scheduler_types = scheduler_types super () . __init__ ( store , ** kwargs ) def _create_with ( self , other ): to_be_deleted = [] for k , v in self . items (): try : class_ref , kwargs = v except TypeError : continue optimizer = other [ k ] if not c_f . is_optimizer ( optimizer ): to_be_deleted . append ( k ) else : self [ k ] = class_ref ( optimizer , ** kwargs ) for k in to_be_deleted : del self [ k ] def step ( self , scheduler_type : str ): \"\"\" Step the lr schedulers of the specified type. Arguments: scheduler_type: ```\"per_step\"``` or ```\"per_epoch\"``` \"\"\" for v in self . filter_by_scheduler_type ( scheduler_type ): v . step () def filter_by_scheduler_type ( self , x ): if self . scheduler_types is not None : return [ v for k , v in self . items () if k in self . scheduler_types [ x ]] elif x == \"per_step\" : return self . values () elif x == \"per_epoch\" : return [] else : raise ValueError ( f \"scheduler types are 'per_step' or 'per_epoch', but input is ' { x } '\" ) def merge ( self , other ): super () . merge ( other ) if other . scheduler_types is not None : if self . scheduler_types is not None : for k , v in other . scheduler_types . items (): curr_list = self . scheduler_types [ k ] curr_list . extend ( v ) self . scheduler_types [ k ] = list ( set ( curr_list )) else : self . scheduler_types = other . scheduler_types __init__ ( store , scheduler_types = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default store See BaseContainer required scheduler_types A dictionary mapping from scheduler type ( \"per_step\" or \"per_epoch\" ) to a list of object names. If None , then all schedulers are assumed to be \"per_step\" None **kwargs BaseContainer keyword arguments. {} Source code in pytorch_adapt\\containers\\lr_schedulers.py 10 11 12 13 14 15 16 17 18 19 20 21 22 def __init__ ( self , store , scheduler_types = None , ** kwargs ): \"\"\" Arguments: store: See [```BaseContainer```][pytorch_adapt.containers.BaseContainer] scheduler_types: A dictionary mapping from scheduler type (```\"per_step\"``` or ```\"per_epoch\"```) to a list of object names. If ```None```, then all schedulers are assumed to be ```\"per_step\"``` **kwargs: [```BaseContainer```][pytorch_adapt.containers.BaseContainer] keyword arguments. \"\"\" self . scheduler_types = scheduler_types super () . __init__ ( store , ** kwargs ) step ( scheduler_type ) \u00b6 Step the lr schedulers of the specified type. Parameters: Name Type Description Default scheduler_type str \"per_step\" or \"per_epoch\" required Source code in pytorch_adapt\\containers\\lr_schedulers.py 40 41 42 43 44 45 46 47 def step ( self , scheduler_type : str ): \"\"\" Step the lr schedulers of the specified type. Arguments: scheduler_type: ```\"per_step\"``` or ```\"per_epoch\"``` \"\"\" for v in self . filter_by_scheduler_type ( scheduler_type ): v . step ()","title":"lr_schedulers"},{"location":"docs/containers/lr_schedulers/#pytorch_adapt.containers.lr_schedulers.LRSchedulers","text":"Bases: BaseContainer A container for optimizer learning rate schedulers. Source code in pytorch_adapt\\containers\\lr_schedulers.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class LRSchedulers ( BaseContainer ): \"\"\" A container for optimizer learning rate schedulers. \"\"\" def __init__ ( self , store , scheduler_types = None , ** kwargs ): \"\"\" Arguments: store: See [```BaseContainer```][pytorch_adapt.containers.BaseContainer] scheduler_types: A dictionary mapping from scheduler type (```\"per_step\"``` or ```\"per_epoch\"```) to a list of object names. If ```None```, then all schedulers are assumed to be ```\"per_step\"``` **kwargs: [```BaseContainer```][pytorch_adapt.containers.BaseContainer] keyword arguments. \"\"\" self . scheduler_types = scheduler_types super () . __init__ ( store , ** kwargs ) def _create_with ( self , other ): to_be_deleted = [] for k , v in self . items (): try : class_ref , kwargs = v except TypeError : continue optimizer = other [ k ] if not c_f . is_optimizer ( optimizer ): to_be_deleted . append ( k ) else : self [ k ] = class_ref ( optimizer , ** kwargs ) for k in to_be_deleted : del self [ k ] def step ( self , scheduler_type : str ): \"\"\" Step the lr schedulers of the specified type. Arguments: scheduler_type: ```\"per_step\"``` or ```\"per_epoch\"``` \"\"\" for v in self . filter_by_scheduler_type ( scheduler_type ): v . step () def filter_by_scheduler_type ( self , x ): if self . scheduler_types is not None : return [ v for k , v in self . items () if k in self . scheduler_types [ x ]] elif x == \"per_step\" : return self . values () elif x == \"per_epoch\" : return [] else : raise ValueError ( f \"scheduler types are 'per_step' or 'per_epoch', but input is ' { x } '\" ) def merge ( self , other ): super () . merge ( other ) if other . scheduler_types is not None : if self . scheduler_types is not None : for k , v in other . scheduler_types . items (): curr_list = self . scheduler_types [ k ] curr_list . extend ( v ) self . scheduler_types [ k ] = list ( set ( curr_list )) else : self . scheduler_types = other . scheduler_types","title":"LRSchedulers"},{"location":"docs/containers/misc/","text":"Misc \u00b6 Bases: BaseContainer This is used for modules that don't have any optimizable parameters, and other miscellaneous objects. Source code in pytorch_adapt\\containers\\misc.py 4 5 6 7 8 9 10 11 class Misc ( BaseContainer ): \"\"\" This is used for modules that don't have any optimizable parameters, and other miscellaneous objects. \"\"\" pass","title":"misc"},{"location":"docs/containers/misc/#pytorch_adapt.containers.misc.Misc","text":"Bases: BaseContainer This is used for modules that don't have any optimizable parameters, and other miscellaneous objects. Source code in pytorch_adapt\\containers\\misc.py 4 5 6 7 8 9 10 11 class Misc ( BaseContainer ): \"\"\" This is used for modules that don't have any optimizable parameters, and other miscellaneous objects. \"\"\" pass","title":"Misc"},{"location":"docs/containers/models/","text":"Models \u00b6 Bases: BaseContainer A container with some functions specific to models that have optimizable parameters. Source code in pytorch_adapt\\containers\\models.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Models ( BaseContainer ): \"\"\" A container with some functions specific to models that have optimizable parameters. \"\"\" def train ( self ): \"\"\" Sets all models to train mode. \"\"\" for v in self . values (): v . train () def eval ( self ): \"\"\" Sets all models to eval mode. \"\"\" for v in self . values (): v . eval () def zero_grad ( self ): \"\"\" Zeros the gradients in all models. \"\"\" for v in self . values (): v . zero_grad () def to ( self , device ): \"\"\" Moves all models to ```device```. \"\"\" for v in self . values (): v . to ( device ) eval () \u00b6 Sets all models to eval mode. Source code in pytorch_adapt\\containers\\models.py 17 18 19 20 21 22 def eval ( self ): \"\"\" Sets all models to eval mode. \"\"\" for v in self . values (): v . eval () to ( device ) \u00b6 Moves all models to device . Source code in pytorch_adapt\\containers\\models.py 31 32 33 34 35 36 def to ( self , device ): \"\"\" Moves all models to ```device```. \"\"\" for v in self . values (): v . to ( device ) train () \u00b6 Sets all models to train mode. Source code in pytorch_adapt\\containers\\models.py 10 11 12 13 14 15 def train ( self ): \"\"\" Sets all models to train mode. \"\"\" for v in self . values (): v . train () zero_grad () \u00b6 Zeros the gradients in all models. Source code in pytorch_adapt\\containers\\models.py 24 25 26 27 28 29 def zero_grad ( self ): \"\"\" Zeros the gradients in all models. \"\"\" for v in self . values (): v . zero_grad ()","title":"models"},{"location":"docs/containers/models/#pytorch_adapt.containers.models.Models","text":"Bases: BaseContainer A container with some functions specific to models that have optimizable parameters. Source code in pytorch_adapt\\containers\\models.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Models ( BaseContainer ): \"\"\" A container with some functions specific to models that have optimizable parameters. \"\"\" def train ( self ): \"\"\" Sets all models to train mode. \"\"\" for v in self . values (): v . train () def eval ( self ): \"\"\" Sets all models to eval mode. \"\"\" for v in self . values (): v . eval () def zero_grad ( self ): \"\"\" Zeros the gradients in all models. \"\"\" for v in self . values (): v . zero_grad () def to ( self , device ): \"\"\" Moves all models to ```device```. \"\"\" for v in self . values (): v . to ( device )","title":"Models"},{"location":"docs/containers/multiple_containers/","text":"MultipleContainers \u00b6 Bases: BaseContainer Contains other containers and initializes them. Source code in pytorch_adapt\\containers\\multiple_containers.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class MultipleContainers ( BaseContainer ): \"\"\" Contains other containers and initializes them. \"\"\" def __init__ ( self , ** kwargs ): self . store = kwargs def merge ( self , ** kwargs : Union [ BaseContainer , None ]): \"\"\" Merges the input containers into any existing sub-containers. \"\"\" for k , v in kwargs . items (): if isinstance ( v , BaseContainer ): if k in self : self [ k ] . merge ( v ) else : self [ k ] = v elif v is None : if k not in self : self [ k ] = get_container ( k ) else : raise TypeError ( f \"Input to { c_f . cls_name ( self ) } .merge must be BaseContainer or None\" ) def create ( self ): \"\"\" Calls [```.create()```][pytorch_adapt.containers.BaseContainer.create] or [```.create_with()```][pytorch_adapt.containers.BaseContainer.create_with] on sub-containers. - Optimizers are created with models as input. - LR schedulers are created with optimizers as input. \"\"\" self [ \"models\" ] . create () self [ \"optimizers\" ] . create_with ( self [ \"models\" ]) self [ \"lr_schedulers\" ] . create_with ( self [ \"optimizers\" ]) self [ \"misc\" ] . create () create () \u00b6 Calls .create() or .create_with() on sub-containers. Optimizers are created with models as input. LR schedulers are created with optimizers as input. Source code in pytorch_adapt\\containers\\multiple_containers.py 50 51 52 53 54 55 56 57 58 59 60 61 62 def create ( self ): \"\"\" Calls [```.create()```][pytorch_adapt.containers.BaseContainer.create] or [```.create_with()```][pytorch_adapt.containers.BaseContainer.create_with] on sub-containers. - Optimizers are created with models as input. - LR schedulers are created with optimizers as input. \"\"\" self [ \"models\" ] . create () self [ \"optimizers\" ] . create_with ( self [ \"models\" ]) self [ \"lr_schedulers\" ] . create_with ( self [ \"optimizers\" ]) self [ \"misc\" ] . create () merge ( ** kwargs ) \u00b6 Merges the input containers into any existing sub-containers. Source code in pytorch_adapt\\containers\\multiple_containers.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def merge ( self , ** kwargs : Union [ BaseContainer , None ]): \"\"\" Merges the input containers into any existing sub-containers. \"\"\" for k , v in kwargs . items (): if isinstance ( v , BaseContainer ): if k in self : self [ k ] . merge ( v ) else : self [ k ] = v elif v is None : if k not in self : self [ k ] = get_container ( k ) else : raise TypeError ( f \"Input to { c_f . cls_name ( self ) } .merge must be BaseContainer or None\" )","title":"multiple_containers"},{"location":"docs/containers/multiple_containers/#pytorch_adapt.containers.multiple_containers.MultipleContainers","text":"Bases: BaseContainer Contains other containers and initializes them. Source code in pytorch_adapt\\containers\\multiple_containers.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class MultipleContainers ( BaseContainer ): \"\"\" Contains other containers and initializes them. \"\"\" def __init__ ( self , ** kwargs ): self . store = kwargs def merge ( self , ** kwargs : Union [ BaseContainer , None ]): \"\"\" Merges the input containers into any existing sub-containers. \"\"\" for k , v in kwargs . items (): if isinstance ( v , BaseContainer ): if k in self : self [ k ] . merge ( v ) else : self [ k ] = v elif v is None : if k not in self : self [ k ] = get_container ( k ) else : raise TypeError ( f \"Input to { c_f . cls_name ( self ) } .merge must be BaseContainer or None\" ) def create ( self ): \"\"\" Calls [```.create()```][pytorch_adapt.containers.BaseContainer.create] or [```.create_with()```][pytorch_adapt.containers.BaseContainer.create_with] on sub-containers. - Optimizers are created with models as input. - LR schedulers are created with optimizers as input. \"\"\" self [ \"models\" ] . create () self [ \"optimizers\" ] . create_with ( self [ \"models\" ]) self [ \"lr_schedulers\" ] . create_with ( self [ \"optimizers\" ]) self [ \"misc\" ] . create ()","title":"MultipleContainers"},{"location":"docs/containers/optimizers/","text":"Optimizers \u00b6 Bases: BaseContainer A container for model optimizers. Source code in pytorch_adapt\\containers\\optimizers.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class Optimizers ( BaseContainer ): \"\"\" A container for model optimizers. \"\"\" def __init__ ( self , * args , multipliers = None , ** kwargs ): \"\"\" Arguments: *args: [```BaseContainer```][pytorch_adapt.containers.BaseContainer] arguments. multipliers: A dictionary mapping from optimizer name to lr multiplier. Each optimizer will have ```lr = lr * multiplier``` upon initialization. If ```None```, then multiplier is 1. **kwargs: [```BaseContainer```][pytorch_adapt.containers.BaseContainer] keyword arguments. \"\"\" self . multipliers = c_f . default ( multipliers , {}) super () . __init__ ( * args , ** kwargs ) def _create_with ( self , other ): c_f . assert_keys_are_present_cls ( self , \"multipliers\" , self ) for k , v in self . items (): if c_f . is_optimizer ( v ): continue class_ref , kwargs = v model = other [ k ] if c_f . has_no_parameters ( model ): self [ k ] = DoNothingOptimizer () else : kwargs = copy . deepcopy ( kwargs ) kwargs [ \"lr\" ] *= self . multipliers . get ( k , 1 ) self [ k ] = class_ref ( model . parameters (), ** kwargs ) def step ( self ): \"\"\" Calls ```.step()``` on all optimizers. \"\"\" for v in self . values (): v . step () def zero_grad ( self ): \"\"\" Calls ```.zero_grad()``` on all optimizers. \"\"\" for v in self . values (): v . zero_grad () def merge ( self , other ): super () . merge ( other ) self . multipliers . update ( other . multipliers ) def zero_back_step ( self , loss , keys : List [ str ] = None ): \"\"\" Zeros gradients, computes gradients, and updates model weights. Arguments: loss: The loss on which ```.backward()``` is called. keys: The subset of optimizers on which to call ```.zero_grad()``` and ```.step()```. If ```None```, then all optimizers are used. \"\"\" keys = c_f . default ( keys , self . keys ()) optimizers = [ self [ k ] for k in keys ] c_f . zero_back_step ( loss , optimizers ) __init__ ( * args , multipliers = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default *args BaseContainer arguments. () multipliers A dictionary mapping from optimizer name to lr multiplier. Each optimizer will have lr = lr * multiplier upon initialization. If None , then multiplier is 1. None **kwargs BaseContainer keyword arguments. {} Source code in pytorch_adapt\\containers\\optimizers.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , * args , multipliers = None , ** kwargs ): \"\"\" Arguments: *args: [```BaseContainer```][pytorch_adapt.containers.BaseContainer] arguments. multipliers: A dictionary mapping from optimizer name to lr multiplier. Each optimizer will have ```lr = lr * multiplier``` upon initialization. If ```None```, then multiplier is 1. **kwargs: [```BaseContainer```][pytorch_adapt.containers.BaseContainer] keyword arguments. \"\"\" self . multipliers = c_f . default ( multipliers , {}) super () . __init__ ( * args , ** kwargs ) step () \u00b6 Calls .step() on all optimizers. Source code in pytorch_adapt\\containers\\optimizers.py 43 44 45 46 47 48 def step ( self ): \"\"\" Calls ```.step()``` on all optimizers. \"\"\" for v in self . values (): v . step () zero_back_step ( loss , keys = None ) \u00b6 Zeros gradients, computes gradients, and updates model weights. Parameters: Name Type Description Default loss The loss on which .backward() is called. required keys List [ str ] The subset of optimizers on which to call .zero_grad() and .step() . If None , then all optimizers are used. None Source code in pytorch_adapt\\containers\\optimizers.py 61 62 63 64 65 66 67 68 69 70 71 72 def zero_back_step ( self , loss , keys : List [ str ] = None ): \"\"\" Zeros gradients, computes gradients, and updates model weights. Arguments: loss: The loss on which ```.backward()``` is called. keys: The subset of optimizers on which to call ```.zero_grad()``` and ```.step()```. If ```None```, then all optimizers are used. \"\"\" keys = c_f . default ( keys , self . keys ()) optimizers = [ self [ k ] for k in keys ] c_f . zero_back_step ( loss , optimizers ) zero_grad () \u00b6 Calls .zero_grad() on all optimizers. Source code in pytorch_adapt\\containers\\optimizers.py 50 51 52 53 54 55 def zero_grad ( self ): \"\"\" Calls ```.zero_grad()``` on all optimizers. \"\"\" for v in self . values (): v . zero_grad ()","title":"optimizers"},{"location":"docs/containers/optimizers/#pytorch_adapt.containers.optimizers.Optimizers","text":"Bases: BaseContainer A container for model optimizers. Source code in pytorch_adapt\\containers\\optimizers.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class Optimizers ( BaseContainer ): \"\"\" A container for model optimizers. \"\"\" def __init__ ( self , * args , multipliers = None , ** kwargs ): \"\"\" Arguments: *args: [```BaseContainer```][pytorch_adapt.containers.BaseContainer] arguments. multipliers: A dictionary mapping from optimizer name to lr multiplier. Each optimizer will have ```lr = lr * multiplier``` upon initialization. If ```None```, then multiplier is 1. **kwargs: [```BaseContainer```][pytorch_adapt.containers.BaseContainer] keyword arguments. \"\"\" self . multipliers = c_f . default ( multipliers , {}) super () . __init__ ( * args , ** kwargs ) def _create_with ( self , other ): c_f . assert_keys_are_present_cls ( self , \"multipliers\" , self ) for k , v in self . items (): if c_f . is_optimizer ( v ): continue class_ref , kwargs = v model = other [ k ] if c_f . has_no_parameters ( model ): self [ k ] = DoNothingOptimizer () else : kwargs = copy . deepcopy ( kwargs ) kwargs [ \"lr\" ] *= self . multipliers . get ( k , 1 ) self [ k ] = class_ref ( model . parameters (), ** kwargs ) def step ( self ): \"\"\" Calls ```.step()``` on all optimizers. \"\"\" for v in self . values (): v . step () def zero_grad ( self ): \"\"\" Calls ```.zero_grad()``` on all optimizers. \"\"\" for v in self . values (): v . zero_grad () def merge ( self , other ): super () . merge ( other ) self . multipliers . update ( other . multipliers ) def zero_back_step ( self , loss , keys : List [ str ] = None ): \"\"\" Zeros gradients, computes gradients, and updates model weights. Arguments: loss: The loss on which ```.backward()``` is called. keys: The subset of optimizers on which to call ```.zero_grad()``` and ```.step()```. If ```None```, then all optimizers are used. \"\"\" keys = c_f . default ( keys , self . keys ()) optimizers = [ self [ k ] for k in keys ] c_f . zero_back_step ( loss , optimizers )","title":"Optimizers"},{"location":"docs/datasets/","text":"The following can be imported like this (using BaseDataset as an example): from pytorch_adapt.datasets import BaseDataset Direct module members \u00b6 BaseDataset BaseDownloadableDataset CombinedSourceAndTargetDataset ConcatDataset DataloaderCreator DomainNet DomainNet126 DomainNet126Full MNISTM Office31 Office31Full OfficeHome OfficeHomeFull PseudoLabeledDataset SourceDataset TargetDataset get_mnist_mnistm get_office31 get_officehome","title":"datasets"},{"location":"docs/datasets/#direct-module-members","text":"BaseDataset BaseDownloadableDataset CombinedSourceAndTargetDataset ConcatDataset DataloaderCreator DomainNet DomainNet126 DomainNet126Full MNISTM Office31 Office31Full OfficeHome OfficeHomeFull PseudoLabeledDataset SourceDataset TargetDataset get_mnist_mnistm get_office31 get_officehome","title":"Direct module members"},{"location":"docs/datasets/base_dataset/","text":"BaseDataset \u00b6 Bases: torch . utils . data . Dataset Base dataset class Source code in pytorch_adapt\\datasets\\base_dataset.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class BaseDataset ( torch . utils . data . Dataset ): \"\"\" Base dataset class \"\"\" def __init__ ( self , domain : str ): super () . __init__ () self . domain = domain def __len__ ( self ): return len ( self . img_paths ) def __getitem__ ( self , idx ): label = self . labels [ idx ] img = Image . open ( self . img_paths [ idx ]) . convert ( \"RGB\" ) if self . transform is not None : img = self . transform ( img ) return img , label def __repr__ ( self ): extra_repr = f \"domain= { self . domain } \\n len= { str ( self . __len__ ()) } \" return c_f . nice_repr ( self , extra_repr , { \"transform\" : self . transform }) BaseDownloadableDataset \u00b6 Bases: BaseDataset Allows automatic downloading of datasets. Source code in pytorch_adapt\\datasets\\base_dataset.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class BaseDownloadableDataset ( BaseDataset ): \"\"\" Allows automatic downloading of datasets. \"\"\" def __init__ ( self , root : str , download : bool = False , ** kwargs ): \"\"\" Arguments: root: Folder where dataset will be downloaded to. download: If True, will download the dataset if it hasn't already been downloaded. \"\"\" super () . __init__ ( ** kwargs ) if download : try : self . set_paths_and_labels ( root ) except ( FileNotFoundError , ValueError ): self . download_dataset ( root ) self . set_paths_and_labels ( root ) else : self . set_paths_and_labels ( root ) def set_paths_and_labels ( self , root ): raise NotImplementedError def download_dataset ( self , root ): download_url ( self . url , root , filename = self . filename , md5 = self . md5 ) with tarfile . open ( os . path . join ( root , self . filename ), \"r:gz\" ) as tar : tar . extractall ( path = root , members = c_f . extract_progress ( tar )) __init__ ( root , download = False , ** kwargs ) \u00b6 Parameters: Name Type Description Default root str Folder where dataset will be downloaded to. required download bool If True, will download the dataset if it hasn't already been downloaded. False Source code in pytorch_adapt\\datasets\\base_dataset.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , root : str , download : bool = False , ** kwargs ): \"\"\" Arguments: root: Folder where dataset will be downloaded to. download: If True, will download the dataset if it hasn't already been downloaded. \"\"\" super () . __init__ ( ** kwargs ) if download : try : self . set_paths_and_labels ( root ) except ( FileNotFoundError , ValueError ): self . download_dataset ( root ) self . set_paths_and_labels ( root ) else : self . set_paths_and_labels ( root )","title":"base_dataset"},{"location":"docs/datasets/base_dataset/#pytorch_adapt.datasets.base_dataset.BaseDataset","text":"Bases: torch . utils . data . Dataset Base dataset class Source code in pytorch_adapt\\datasets\\base_dataset.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class BaseDataset ( torch . utils . data . Dataset ): \"\"\" Base dataset class \"\"\" def __init__ ( self , domain : str ): super () . __init__ () self . domain = domain def __len__ ( self ): return len ( self . img_paths ) def __getitem__ ( self , idx ): label = self . labels [ idx ] img = Image . open ( self . img_paths [ idx ]) . convert ( \"RGB\" ) if self . transform is not None : img = self . transform ( img ) return img , label def __repr__ ( self ): extra_repr = f \"domain= { self . domain } \\n len= { str ( self . __len__ ()) } \" return c_f . nice_repr ( self , extra_repr , { \"transform\" : self . transform })","title":"BaseDataset"},{"location":"docs/datasets/base_dataset/#pytorch_adapt.datasets.base_dataset.BaseDownloadableDataset","text":"Bases: BaseDataset Allows automatic downloading of datasets. Source code in pytorch_adapt\\datasets\\base_dataset.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class BaseDownloadableDataset ( BaseDataset ): \"\"\" Allows automatic downloading of datasets. \"\"\" def __init__ ( self , root : str , download : bool = False , ** kwargs ): \"\"\" Arguments: root: Folder where dataset will be downloaded to. download: If True, will download the dataset if it hasn't already been downloaded. \"\"\" super () . __init__ ( ** kwargs ) if download : try : self . set_paths_and_labels ( root ) except ( FileNotFoundError , ValueError ): self . download_dataset ( root ) self . set_paths_and_labels ( root ) else : self . set_paths_and_labels ( root ) def set_paths_and_labels ( self , root ): raise NotImplementedError def download_dataset ( self , root ): download_url ( self . url , root , filename = self . filename , md5 = self . md5 ) with tarfile . open ( os . path . join ( root , self . filename ), \"r:gz\" ) as tar : tar . extractall ( path = root , members = c_f . extract_progress ( tar ))","title":"BaseDownloadableDataset"},{"location":"docs/datasets/combined_source_and_target/","text":"CombinedSourceAndTargetDataset \u00b6 Bases: torch . utils . data . Dataset Wraps a source dataset and a target dataset. Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class CombinedSourceAndTargetDataset ( torch . utils . data . Dataset ): \"\"\" Wraps a source dataset and a target dataset. \"\"\" def __init__ ( self , source_dataset : SourceDataset , target_dataset : TargetDataset ): \"\"\" Arguments: source_dataset: target_dataset: \"\"\" self . source_dataset = source_dataset self . target_dataset = target_dataset def __len__ ( self ) -> int : \"\"\" Returns: The length of the target dataset. \"\"\" return len ( self . target_dataset ) def __getitem__ ( self , idx ) -> Dict [ str , Any ]: \"\"\" Arguments: idx: The index of the target dataset. The source index is picked randomly. Returns: A dictionary containing both source and target data. The source keys start with ```\"src_\"```, and the target keys start with ```\"target_\"```. See [```SourceDataset.__getitem__```][pytorch_adapt.datasets.SourceDataset.__getitem__] and [```TargetDataset.__getitem__```][pytorch_adapt.datasets.TargetDataset.__getitem__] for details. \"\"\" target_data = self . target_dataset [ idx ] src_data = self . source_dataset [ self . get_random_src_idx ()] return c_f . assert_dicts_are_disjoint ( src_data , target_data ) def get_random_src_idx ( self ): return np . random . choice ( len ( self . source_dataset )) def __repr__ ( self ): return c_f . nice_repr ( self , \"\" , { \"source_dataset\" : self . source_dataset , \"target_dataset\" : self . target_dataset , }, ) __getitem__ ( idx ) \u00b6 Parameters: Name Type Description Default idx The index of the target dataset. The source index is picked randomly. required Returns: Type Description Dict [ str , Any ] A dictionary containing both source and target data. Dict [ str , Any ] The source keys start with \"src_\" , and the target keys start with \"target_\" . Dict [ str , Any ] See SourceDataset.__getitem__ and Dict [ str , Any ] TargetDataset.__getitem__ Dict [ str , Any ] for details. Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __getitem__ ( self , idx ) -> Dict [ str , Any ]: \"\"\" Arguments: idx: The index of the target dataset. The source index is picked randomly. Returns: A dictionary containing both source and target data. The source keys start with ```\"src_\"```, and the target keys start with ```\"target_\"```. See [```SourceDataset.__getitem__```][pytorch_adapt.datasets.SourceDataset.__getitem__] and [```TargetDataset.__getitem__```][pytorch_adapt.datasets.TargetDataset.__getitem__] for details. \"\"\" target_data = self . target_dataset [ idx ] src_data = self . source_dataset [ self . get_random_src_idx ()] return c_f . assert_dicts_are_disjoint ( src_data , target_data ) __init__ ( source_dataset , target_dataset ) \u00b6 Parameters: Name Type Description Default source_dataset SourceDataset required target_dataset TargetDataset required Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 16 17 18 19 20 21 22 23 24 def __init__ ( self , source_dataset : SourceDataset , target_dataset : TargetDataset ): \"\"\" Arguments: source_dataset: target_dataset: \"\"\" self . source_dataset = source_dataset self . target_dataset = target_dataset __len__ () \u00b6 Returns: Type Description int The length of the target dataset. Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 26 27 28 29 30 31 def __len__ ( self ) -> int : \"\"\" Returns: The length of the target dataset. \"\"\" return len ( self . target_dataset )","title":"combined_source_and_target"},{"location":"docs/datasets/combined_source_and_target/#pytorch_adapt.datasets.combined_source_and_target.CombinedSourceAndTargetDataset","text":"Bases: torch . utils . data . Dataset Wraps a source dataset and a target dataset. Source code in pytorch_adapt\\datasets\\combined_source_and_target.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class CombinedSourceAndTargetDataset ( torch . utils . data . Dataset ): \"\"\" Wraps a source dataset and a target dataset. \"\"\" def __init__ ( self , source_dataset : SourceDataset , target_dataset : TargetDataset ): \"\"\" Arguments: source_dataset: target_dataset: \"\"\" self . source_dataset = source_dataset self . target_dataset = target_dataset def __len__ ( self ) -> int : \"\"\" Returns: The length of the target dataset. \"\"\" return len ( self . target_dataset ) def __getitem__ ( self , idx ) -> Dict [ str , Any ]: \"\"\" Arguments: idx: The index of the target dataset. The source index is picked randomly. Returns: A dictionary containing both source and target data. The source keys start with ```\"src_\"```, and the target keys start with ```\"target_\"```. See [```SourceDataset.__getitem__```][pytorch_adapt.datasets.SourceDataset.__getitem__] and [```TargetDataset.__getitem__```][pytorch_adapt.datasets.TargetDataset.__getitem__] for details. \"\"\" target_data = self . target_dataset [ idx ] src_data = self . source_dataset [ self . get_random_src_idx ()] return c_f . assert_dicts_are_disjoint ( src_data , target_data ) def get_random_src_idx ( self ): return np . random . choice ( len ( self . source_dataset )) def __repr__ ( self ): return c_f . nice_repr ( self , \"\" , { \"source_dataset\" : self . source_dataset , \"target_dataset\" : self . target_dataset , }, )","title":"CombinedSourceAndTargetDataset"},{"location":"docs/datasets/concat_dataset/","text":"ConcatDataset \u00b6 Bases: torch . utils . data . ConcatDataset Exactly the same as torch.utils.data.ConcatDataset except with a nice __repr__ function. Source code in pytorch_adapt\\datasets\\concat_dataset.py 6 7 8 9 10 11 12 13 14 class ConcatDataset ( torch . utils . data . ConcatDataset ): \"\"\" Exactly the same as ```torch.utils.data.ConcatDataset``` except with a nice ```__repr__``` function. \"\"\" def __repr__ ( self ): extra_repr = f \"len= { str ( self . __len__ ()) } \" return c_f . nice_repr ( self , extra_repr , { \"datasets\" : self . datasets })","title":"concat_dataset"},{"location":"docs/datasets/concat_dataset/#pytorch_adapt.datasets.concat_dataset.ConcatDataset","text":"Bases: torch . utils . data . ConcatDataset Exactly the same as torch.utils.data.ConcatDataset except with a nice __repr__ function. Source code in pytorch_adapt\\datasets\\concat_dataset.py 6 7 8 9 10 11 12 13 14 class ConcatDataset ( torch . utils . data . ConcatDataset ): \"\"\" Exactly the same as ```torch.utils.data.ConcatDataset``` except with a nice ```__repr__``` function. \"\"\" def __repr__ ( self ): extra_repr = f \"len= { str ( self . __len__ ()) } \" return c_f . nice_repr ( self , extra_repr , { \"datasets\" : self . datasets })","title":"ConcatDataset"},{"location":"docs/datasets/dataloader_creator/","text":"DataloaderCreator \u00b6 This is a factory class for creating dataloaders. The __call__ function takes in keyword arguments which are datasets, and outputs a dictionary of dataloaders (one dataloader for each input dataset). Source code in pytorch_adapt\\datasets\\dataloader_creator.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class DataloaderCreator : \"\"\" This is a factory class for creating dataloaders. The ```__call__``` function takes in keyword arguments which are datasets, and outputs a dictionary of dataloaders (one dataloader for each input dataset). \"\"\" def __init__ ( self , train_kwargs : Dict [ str , Any ] = None , val_kwargs : Dict [ str , Any ] = None , train_names : List [ str ] = None , val_names : List [ str ] = None , all_train : bool = False , all_val : bool = False , batch_size : int = 32 , num_workers : int = 0 , ): \"\"\" Arguments: train_kwargs: The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. If ```None```, it defaults to: ```python { \"batch_size\": batch_size, \"num_workers\": num_workers, \"shuffle\": True, \"drop_last\": True, }, ``` val_kwargs: The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. If ```None```, it defaults to: ```python { \"batch_size\": batch_size, \"num_workers\": num_workers, \"shuffle\": False, \"drop_last\": False, } ``` train_names: A list of the dataset names that are used during training. If ```None```, it defaults to ```[\"train\"]```. val_names: A list of the dataset names that are used during validation. If ```None```, it defaults to ```[\"src_train\", \"target_train\", \"src_val\", \"target_val\"]```. all_train: If True, then all input datasets are assumed to be for training, regardless of their names. all_val: If True, then all input datasets are assumed to be for validation, regardless of their names. batch_size: The ```batch_size``` used in the default ```train_kwargs``` and in the default ```val_kwargs```. num_workers: The ```num_workers``` used in the default ```train_kwargs``` and in the default ```val_kwargs```. \"\"\" self . train_kwargs = c_f . default ( train_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : True , \"drop_last\" : True , }, ) self . val_kwargs = c_f . default ( val_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : False , \"drop_last\" : False , }, ) self . train_names = c_f . default ( train_names , [ \"train\" ]) self . val_names = c_f . default ( val_names , [ \"src_train\" , \"target_train\" , \"src_val\" , \"target_val\" ] ) if not set ( self . train_names ) . isdisjoint ( self . val_names ): raise ValueError ( f \"train_names { self . train_names } must be disjoint from val_names { self . val_names } \" ) if all_train and all_val : raise ValueError ( \"all_train and all_val cannot both be True\" ) self . all_train = all_train self . all_val = all_val def __call__ ( self , ** kwargs ) -> Dict [ str , DataLoader ]: \"\"\" Arguments: **kwargs: keyword arguments mapping from dataset names to datasets. Returns: a dictionary mapping from dataset names to dataloaders. \"\"\" output = {} for k , v in kwargs . items (): if self . all_train : dataloader_kwargs = self . train_kwargs elif self . all_val : dataloader_kwargs = self . val_kwargs elif k in self . train_names : dataloader_kwargs = self . train_kwargs elif k in self . val_names : dataloader_kwargs = self . val_kwargs else : raise ValueError ( f \"Dataset split name must be in { self . train_names } or { self . val_names } , or one of self.all_train or self.all_val must be true\" ) output [ k ] = torch . utils . data . DataLoader ( v , ** dataloader_kwargs ) return output __call__ ( ** kwargs ) \u00b6 Parameters: Name Type Description Default **kwargs keyword arguments mapping from dataset names to datasets. {} Returns: Type Description Dict [ str , DataLoader ] a dictionary mapping from dataset names to dataloaders. Source code in pytorch_adapt\\datasets\\dataloader_creator.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def __call__ ( self , ** kwargs ) -> Dict [ str , DataLoader ]: \"\"\" Arguments: **kwargs: keyword arguments mapping from dataset names to datasets. Returns: a dictionary mapping from dataset names to dataloaders. \"\"\" output = {} for k , v in kwargs . items (): if self . all_train : dataloader_kwargs = self . train_kwargs elif self . all_val : dataloader_kwargs = self . val_kwargs elif k in self . train_names : dataloader_kwargs = self . train_kwargs elif k in self . val_names : dataloader_kwargs = self . val_kwargs else : raise ValueError ( f \"Dataset split name must be in { self . train_names } or { self . val_names } , or one of self.all_train or self.all_val must be true\" ) output [ k ] = torch . utils . data . DataLoader ( v , ** dataloader_kwargs ) return output __init__ ( train_kwargs = None , val_kwargs = None , train_names = None , val_names = None , all_train = False , all_val = False , batch_size = 32 , num_workers = 0 ) \u00b6 Parameters: Name Type Description Default train_kwargs Dict [ str , Any ] The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. If None , it defaults to: { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : True , \"drop_last\" : True , }, None val_kwargs Dict [ str , Any ] The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. If None , it defaults to: { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : False , \"drop_last\" : False , } None train_names List [ str ] A list of the dataset names that are used during training. If None , it defaults to [\"train\"] . None val_names List [ str ] A list of the dataset names that are used during validation. If None , it defaults to [\"src_train\", \"target_train\", \"src_val\", \"target_val\"] . None all_train bool If True, then all input datasets are assumed to be for training, regardless of their names. False all_val bool If True, then all input datasets are assumed to be for validation, regardless of their names. False batch_size int The batch_size used in the default train_kwargs and in the default val_kwargs . 32 num_workers int The num_workers used in the default train_kwargs and in the default val_kwargs . 0 Source code in pytorch_adapt\\datasets\\dataloader_creator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , train_kwargs : Dict [ str , Any ] = None , val_kwargs : Dict [ str , Any ] = None , train_names : List [ str ] = None , val_names : List [ str ] = None , all_train : bool = False , all_val : bool = False , batch_size : int = 32 , num_workers : int = 0 , ): \"\"\" Arguments: train_kwargs: The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. If ```None```, it defaults to: ```python { \"batch_size\": batch_size, \"num_workers\": num_workers, \"shuffle\": True, \"drop_last\": True, }, ``` val_kwargs: The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. If ```None```, it defaults to: ```python { \"batch_size\": batch_size, \"num_workers\": num_workers, \"shuffle\": False, \"drop_last\": False, } ``` train_names: A list of the dataset names that are used during training. If ```None```, it defaults to ```[\"train\"]```. val_names: A list of the dataset names that are used during validation. If ```None```, it defaults to ```[\"src_train\", \"target_train\", \"src_val\", \"target_val\"]```. all_train: If True, then all input datasets are assumed to be for training, regardless of their names. all_val: If True, then all input datasets are assumed to be for validation, regardless of their names. batch_size: The ```batch_size``` used in the default ```train_kwargs``` and in the default ```val_kwargs```. num_workers: The ```num_workers``` used in the default ```train_kwargs``` and in the default ```val_kwargs```. \"\"\" self . train_kwargs = c_f . default ( train_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : True , \"drop_last\" : True , }, ) self . val_kwargs = c_f . default ( val_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : False , \"drop_last\" : False , }, ) self . train_names = c_f . default ( train_names , [ \"train\" ]) self . val_names = c_f . default ( val_names , [ \"src_train\" , \"target_train\" , \"src_val\" , \"target_val\" ] ) if not set ( self . train_names ) . isdisjoint ( self . val_names ): raise ValueError ( f \"train_names { self . train_names } must be disjoint from val_names { self . val_names } \" ) if all_train and all_val : raise ValueError ( \"all_train and all_val cannot both be True\" ) self . all_train = all_train self . all_val = all_val","title":"dataloader_creator"},{"location":"docs/datasets/dataloader_creator/#pytorch_adapt.datasets.dataloader_creator.DataloaderCreator","text":"This is a factory class for creating dataloaders. The __call__ function takes in keyword arguments which are datasets, and outputs a dictionary of dataloaders (one dataloader for each input dataset). Source code in pytorch_adapt\\datasets\\dataloader_creator.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class DataloaderCreator : \"\"\" This is a factory class for creating dataloaders. The ```__call__``` function takes in keyword arguments which are datasets, and outputs a dictionary of dataloaders (one dataloader for each input dataset). \"\"\" def __init__ ( self , train_kwargs : Dict [ str , Any ] = None , val_kwargs : Dict [ str , Any ] = None , train_names : List [ str ] = None , val_names : List [ str ] = None , all_train : bool = False , all_val : bool = False , batch_size : int = 32 , num_workers : int = 0 , ): \"\"\" Arguments: train_kwargs: The keyword arguments that will be passed to every DataLoader constructor for train-time datasets. If ```None```, it defaults to: ```python { \"batch_size\": batch_size, \"num_workers\": num_workers, \"shuffle\": True, \"drop_last\": True, }, ``` val_kwargs: The keyword arguments that will be passed to every DataLoader constructor for validation-time datasets. If ```None```, it defaults to: ```python { \"batch_size\": batch_size, \"num_workers\": num_workers, \"shuffle\": False, \"drop_last\": False, } ``` train_names: A list of the dataset names that are used during training. If ```None```, it defaults to ```[\"train\"]```. val_names: A list of the dataset names that are used during validation. If ```None```, it defaults to ```[\"src_train\", \"target_train\", \"src_val\", \"target_val\"]```. all_train: If True, then all input datasets are assumed to be for training, regardless of their names. all_val: If True, then all input datasets are assumed to be for validation, regardless of their names. batch_size: The ```batch_size``` used in the default ```train_kwargs``` and in the default ```val_kwargs```. num_workers: The ```num_workers``` used in the default ```train_kwargs``` and in the default ```val_kwargs```. \"\"\" self . train_kwargs = c_f . default ( train_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : True , \"drop_last\" : True , }, ) self . val_kwargs = c_f . default ( val_kwargs , { \"batch_size\" : batch_size , \"num_workers\" : num_workers , \"shuffle\" : False , \"drop_last\" : False , }, ) self . train_names = c_f . default ( train_names , [ \"train\" ]) self . val_names = c_f . default ( val_names , [ \"src_train\" , \"target_train\" , \"src_val\" , \"target_val\" ] ) if not set ( self . train_names ) . isdisjoint ( self . val_names ): raise ValueError ( f \"train_names { self . train_names } must be disjoint from val_names { self . val_names } \" ) if all_train and all_val : raise ValueError ( \"all_train and all_val cannot both be True\" ) self . all_train = all_train self . all_val = all_val def __call__ ( self , ** kwargs ) -> Dict [ str , DataLoader ]: \"\"\" Arguments: **kwargs: keyword arguments mapping from dataset names to datasets. Returns: a dictionary mapping from dataset names to dataloaders. \"\"\" output = {} for k , v in kwargs . items (): if self . all_train : dataloader_kwargs = self . train_kwargs elif self . all_val : dataloader_kwargs = self . val_kwargs elif k in self . train_names : dataloader_kwargs = self . train_kwargs elif k in self . val_names : dataloader_kwargs = self . val_kwargs else : raise ValueError ( f \"Dataset split name must be in { self . train_names } or { self . val_names } , or one of self.all_train or self.all_val must be true\" ) output [ k ] = torch . utils . data . DataLoader ( v , ** dataloader_kwargs ) return output","title":"DataloaderCreator"},{"location":"docs/datasets/domainnet/","text":"DomainNet \u00b6 Bases: BaseDataset A large dataset used in \"Moment Matching for Multi-Source Domain Adaptation\". It consists of 345 classes in 6 domains: clipart, infograph, painting, quickdraw, real, sketch Source code in pytorch_adapt\\datasets\\domainnet.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class DomainNet ( BaseDataset ): \"\"\" A large dataset used in \"Moment Matching for Multi-Source Domain Adaptation\". It consists of 345 classes in 6 domains: clipart, infograph, painting, quickdraw, real, sketch \"\"\" def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 6 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" { domain } _ { name } .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 33525 , \"test\" : 14604 }[ name ], \"infograph\" : { \"train\" : 36023 , \"test\" : 15582 }[ name ], \"painting\" : { \"train\" : 50416 , \"test\" : 21850 }[ name ], \"quickdraw\" : { \"train\" : 120750 , \"test\" : 51750 }[ name ], \"real\" : { \"train\" : 120906 , \"test\" : 52041 }[ name ], \"sketch\" : { \"train\" : 48212 , \"test\" : 20916 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform __init__ ( root , domain , train , transform , ** kwargs ) \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 6 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 6 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" { domain } _ { name } .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 33525 , \"test\" : 14604 }[ name ], \"infograph\" : { \"train\" : 36023 , \"test\" : 15582 }[ name ], \"painting\" : { \"train\" : 50416 , \"test\" : 21850 }[ name ], \"quickdraw\" : { \"train\" : 120750 , \"test\" : 51750 }[ name ], \"real\" : { \"train\" : 120906 , \"test\" : 52041 }[ name ], \"sketch\" : { \"train\" : 48212 , \"test\" : 20916 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform DomainNet126 \u00b6 Bases: BaseDataset A custom train/test split of DomainNet126Full. Source code in pytorch_adapt\\datasets\\domainnet.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 class DomainNet126 ( BaseDataset ): \"\"\" A custom train/test split of DomainNet126Full. \"\"\" def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" { domain } 126_ { name } .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 14962 , \"test\" : 3741 }[ name ], \"painting\" : { \"train\" : 25201 , \"test\" : 6301 }[ name ], \"real\" : { \"train\" : 56286 , \"test\" : 14072 }[ name ], \"sketch\" : { \"train\" : 19665 , \"test\" : 4917 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform __init__ ( root , domain , train , transform , ** kwargs ) \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 4 domains required train bool Whether or not to use the training set. required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" { domain } 126_ { name } .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 14962 , \"test\" : 3741 }[ name ], \"painting\" : { \"train\" : 25201 , \"test\" : 6301 }[ name ], \"real\" : { \"train\" : 56286 , \"test\" : 14072 }[ name ], \"sketch\" : { \"train\" : 19665 , \"test\" : 4917 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform DomainNet126Full \u00b6 Bases: BaseDataset A subset of DomainNet consisting of 126 classes and 4 domains: clipart, painting, real, sketch Source code in pytorch_adapt\\datasets\\domainnet.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 class DomainNet126Full ( BaseDataset ): \"\"\" A subset of DomainNet consisting of 126 classes and 4 domains: clipart, painting, real, sketch \"\"\" def __init__ ( self , root : str , domain : str , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) filenames = [ f \"labeled_source_images_ { domain } \" , f \"labeled_target_images_ { domain } _1\" , f \"labeled_target_images_ { domain } _3\" , f \"unlabeled_target_images_ { domain } _1\" , f \"unlabeled_target_images_ { domain } _3\" , f \"validation_target_images_ { domain } _3\" , ] filenames = [ os . path . join ( root , \"domainnet\" , f \" { f } .txt\" ) for f in filenames ] img_dir = os . path . join ( root , \"domainnet\" ) content = OrderedDict () for f in filenames : with open ( f ) as fff : for line in fff : path , label = line . rstrip () . split ( \" \" ) content [ path ] = label self . img_paths = [ os . path . join ( img_dir , x ) for x in content . keys ()] check_img_paths ( img_dir , self . img_paths , domain ) self . labels = [ int ( x ) for x in content . values ()] self . transform = transform __init__ ( root , domain , transform , ** kwargs ) \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/domainnet required domain str One of the 4 domains required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\domainnet.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def __init__ ( self , root : str , domain : str , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) filenames = [ f \"labeled_source_images_ { domain } \" , f \"labeled_target_images_ { domain } _1\" , f \"labeled_target_images_ { domain } _3\" , f \"unlabeled_target_images_ { domain } _1\" , f \"unlabeled_target_images_ { domain } _3\" , f \"validation_target_images_ { domain } _3\" , ] filenames = [ os . path . join ( root , \"domainnet\" , f \" { f } .txt\" ) for f in filenames ] img_dir = os . path . join ( root , \"domainnet\" ) content = OrderedDict () for f in filenames : with open ( f ) as fff : for line in fff : path , label = line . rstrip () . split ( \" \" ) content [ path ] = label self . img_paths = [ os . path . join ( img_dir , x ) for x in content . keys ()] check_img_paths ( img_dir , self . img_paths , domain ) self . labels = [ int ( x ) for x in content . values ()] self . transform = transform","title":"domainnet"},{"location":"docs/datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet","text":"Bases: BaseDataset A large dataset used in \"Moment Matching for Multi-Source Domain Adaptation\". It consists of 345 classes in 6 domains: clipart, infograph, painting, quickdraw, real, sketch Source code in pytorch_adapt\\datasets\\domainnet.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class DomainNet ( BaseDataset ): \"\"\" A large dataset used in \"Moment Matching for Multi-Source Domain Adaptation\". It consists of 345 classes in 6 domains: clipart, infograph, painting, quickdraw, real, sketch \"\"\" def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 6 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" { domain } _ { name } .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 33525 , \"test\" : 14604 }[ name ], \"infograph\" : { \"train\" : 36023 , \"test\" : 15582 }[ name ], \"painting\" : { \"train\" : 50416 , \"test\" : 21850 }[ name ], \"quickdraw\" : { \"train\" : 120750 , \"test\" : 51750 }[ name ], \"real\" : { \"train\" : 120906 , \"test\" : 52041 }[ name ], \"sketch\" : { \"train\" : 48212 , \"test\" : 20916 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"DomainNet"},{"location":"docs/datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet126","text":"Bases: BaseDataset A custom train/test split of DomainNet126Full. Source code in pytorch_adapt\\datasets\\domainnet.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 class DomainNet126 ( BaseDataset ): \"\"\" A custom train/test split of DomainNet126Full. \"\"\" def __init__ ( self , root : str , domain : str , train : bool , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) if not isinstance ( train , bool ): raise TypeError ( \"train should be True or False\" ) name = \"train\" if train else \"test\" labels_file = os . path . join ( root , \"domainnet\" , f \" { domain } 126_ { name } .txt\" ) img_dir = os . path . join ( root , \"domainnet\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , domain ) check_length ( self , { \"clipart\" : { \"train\" : 14962 , \"test\" : 3741 }[ name ], \"painting\" : { \"train\" : 25201 , \"test\" : 6301 }[ name ], \"real\" : { \"train\" : 56286 , \"test\" : 14072 }[ name ], \"sketch\" : { \"train\" : 19665 , \"test\" : 4917 }[ name ], }[ domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] self . transform = transform","title":"DomainNet126"},{"location":"docs/datasets/domainnet/#pytorch_adapt.datasets.domainnet.DomainNet126Full","text":"Bases: BaseDataset A subset of DomainNet consisting of 126 classes and 4 domains: clipart, painting, real, sketch Source code in pytorch_adapt\\datasets\\domainnet.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 class DomainNet126Full ( BaseDataset ): \"\"\" A subset of DomainNet consisting of 126 classes and 4 domains: clipart, painting, real, sketch \"\"\" def __init__ ( self , root : str , domain : str , transform , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/domainnet``` domain: One of the 4 domains transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain , ** kwargs ) filenames = [ f \"labeled_source_images_ { domain } \" , f \"labeled_target_images_ { domain } _1\" , f \"labeled_target_images_ { domain } _3\" , f \"unlabeled_target_images_ { domain } _1\" , f \"unlabeled_target_images_ { domain } _3\" , f \"validation_target_images_ { domain } _3\" , ] filenames = [ os . path . join ( root , \"domainnet\" , f \" { f } .txt\" ) for f in filenames ] img_dir = os . path . join ( root , \"domainnet\" ) content = OrderedDict () for f in filenames : with open ( f ) as fff : for line in fff : path , label = line . rstrip () . split ( \" \" ) content [ path ] = label self . img_paths = [ os . path . join ( img_dir , x ) for x in content . keys ()] check_img_paths ( img_dir , self . img_paths , domain ) self . labels = [ int ( x ) for x in content . values ()] self . transform = transform","title":"DomainNet126Full"},{"location":"docs/datasets/mnistm/","text":"MNISTM \u00b6 Bases: BaseDownloadableDataset The dataset used in \"Domain-Adversarial Training of Neural Networks\". It consists of colored MNIST digits. Extends BaseDownloadableDataset , so the dataset can be downloaded by setting download=True when initializing. Source code in pytorch_adapt\\datasets\\mnistm.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class MNISTM ( BaseDownloadableDataset ): \"\"\" The dataset used in \"Domain-Adversarial Training of Neural Networks\". It consists of colored MNIST digits. Extends [BaseDownloadableDataset][pytorch_adapt.datasets.BaseDownloadableDataset], so the dataset can be downloaded by setting ```download=True``` when initializing. \"\"\" url = \"https://cornell.box.com/shared/static/jado7quprg6hzzdubvwzh9umr75damwi\" filename = \"mnist_m.tar.gz\" md5 = \"859df31c91afe82e80e5012ba928f279\" def __init__ ( self , root : str , train : bool , transform = None , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/mnist_m``` train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" self . train = check_train ( train ) super () . __init__ ( root = root , domain = \"MNISTM\" , ** kwargs ) self . transform = transform def set_paths_and_labels ( self , root ): name = \"train\" if self . train else \"test\" labels_file = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ { name } _labels.txt\" ) img_dir = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ { name } \" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_length ( self , { \"train\" : 59001 , \"test\" : 9001 }[ name ]) self . labels = [ int ( x [ 1 ]) for x in content ] __init__ ( root , train , transform = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/mnist_m required train bool Whether or not to use the training set. required transform The image transform applied to each sample. None Source code in pytorch_adapt\\datasets\\mnistm.py 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , root : str , train : bool , transform = None , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/mnist_m``` train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" self . train = check_train ( train ) super () . __init__ ( root = root , domain = \"MNISTM\" , ** kwargs ) self . transform = transform","title":"mnistm"},{"location":"docs/datasets/mnistm/#pytorch_adapt.datasets.mnistm.MNISTM","text":"Bases: BaseDownloadableDataset The dataset used in \"Domain-Adversarial Training of Neural Networks\". It consists of colored MNIST digits. Extends BaseDownloadableDataset , so the dataset can be downloaded by setting download=True when initializing. Source code in pytorch_adapt\\datasets\\mnistm.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class MNISTM ( BaseDownloadableDataset ): \"\"\" The dataset used in \"Domain-Adversarial Training of Neural Networks\". It consists of colored MNIST digits. Extends [BaseDownloadableDataset][pytorch_adapt.datasets.BaseDownloadableDataset], so the dataset can be downloaded by setting ```download=True``` when initializing. \"\"\" url = \"https://cornell.box.com/shared/static/jado7quprg6hzzdubvwzh9umr75damwi\" filename = \"mnist_m.tar.gz\" md5 = \"859df31c91afe82e80e5012ba928f279\" def __init__ ( self , root : str , train : bool , transform = None , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/mnist_m``` train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" self . train = check_train ( train ) super () . __init__ ( root = root , domain = \"MNISTM\" , ** kwargs ) self . transform = transform def set_paths_and_labels ( self , root ): name = \"train\" if self . train else \"test\" labels_file = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ { name } _labels.txt\" ) img_dir = os . path . join ( root , \"mnist_m\" , f \"mnist_m_ { name } \" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_length ( self , { \"train\" : 59001 , \"test\" : 9001 }[ name ]) self . labels = [ int ( x [ 1 ]) for x in content ]","title":"MNISTM"},{"location":"docs/datasets/office31/","text":"Office31 \u00b6 Bases: BaseDownloadableDataset A custom train/test split of Office31Full . Extends BaseDownloadableDataset , so the dataset can be downloaded by setting download=True when initializing. Source code in pytorch_adapt\\datasets\\office31.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 class Office31 ( BaseDownloadableDataset ): \"\"\" A custom train/test split of [Office31Full][pytorch_adapt.datasets.Office31Full]. Extends [BaseDownloadableDataset][pytorch_adapt.datasets.BaseDownloadableDataset], so the dataset can be downloaded by setting ```download=True``` when initializing. \"\"\" url = \"https://cornell.box.com/shared/static/3v2ftdkdhpz1lbbr4uhu0135w7m79p7q\" filename = \"office31.tar.gz\" md5 = \"89818e596f3cdda1d56da0f077435faa\" def __init__ ( self , root : str , domain : str , train : bool , transform = None , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of ```\"amazon\", \"dslr\", \"webcam\"```. train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" self . train = check_train ( train ) super () . __init__ ( root = root , domain = domain , ** kwargs ) self . transform = transform def set_paths_and_labels ( self , root ): name = \"train\" if self . train else \"test\" labels_file = os . path . join ( root , \"office31\" , f \" { self . domain } _ { name } .txt\" ) img_dir = os . path . join ( root , \"office31\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , self . domain ) check_length ( self , { \"amazon\" : { \"train\" : 2253 , \"test\" : 564 }[ name ], \"dslr\" : { \"train\" : 398 , \"test\" : 100 }[ name ], \"webcam\" : { \"train\" : 636 , \"test\" : 159 }[ name ], }[ self . domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] __init__ ( root , domain , train , transform = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/office31 required domain str One of \"amazon\", \"dslr\", \"webcam\" . required train bool Whether or not to use the training set. required transform The image transform applied to each sample. None Source code in pytorch_adapt\\datasets\\office31.py 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , root : str , domain : str , train : bool , transform = None , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of ```\"amazon\", \"dslr\", \"webcam\"```. train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" self . train = check_train ( train ) super () . __init__ ( root = root , domain = domain , ** kwargs ) self . transform = transform Office31Full \u00b6 Bases: BaseDataset A small dataset consisting of 31 classes in 3 domains: amazon, dslr, webcam. Source code in pytorch_adapt\\datasets\\office31.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class Office31Full ( BaseDataset ): \"\"\" A small dataset consisting of 31 classes in 3 domains: amazon, dslr, webcam. \"\"\" def __init__ ( self , root : str , domain : str , transform ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of ```\"amazon\", \"dslr\", \"webcam\"```. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain ) self . transform = transform self . dataset = torch_datasets . ImageFolder ( os . path . join ( root , \"office31\" , domain , \"images\" ), transform = self . transform ) check_length ( self , { \"amazon\" : 2817 , \"dslr\" : 498 , \"webcam\" : 795 }[ domain ]) def __len__ ( self ): return len ( self . dataset ) def __getitem__ ( self , idx ): return self . dataset [ idx ] __init__ ( root , domain , transform ) \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/office31 required domain str One of \"amazon\", \"dslr\", \"webcam\" . required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\office31.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def __init__ ( self , root : str , domain : str , transform ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of ```\"amazon\", \"dslr\", \"webcam\"```. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain ) self . transform = transform self . dataset = torch_datasets . ImageFolder ( os . path . join ( root , \"office31\" , domain , \"images\" ), transform = self . transform ) check_length ( self , { \"amazon\" : 2817 , \"dslr\" : 498 , \"webcam\" : 795 }[ domain ])","title":"office31"},{"location":"docs/datasets/office31/#pytorch_adapt.datasets.office31.Office31","text":"Bases: BaseDownloadableDataset A custom train/test split of Office31Full . Extends BaseDownloadableDataset , so the dataset can be downloaded by setting download=True when initializing. Source code in pytorch_adapt\\datasets\\office31.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 class Office31 ( BaseDownloadableDataset ): \"\"\" A custom train/test split of [Office31Full][pytorch_adapt.datasets.Office31Full]. Extends [BaseDownloadableDataset][pytorch_adapt.datasets.BaseDownloadableDataset], so the dataset can be downloaded by setting ```download=True``` when initializing. \"\"\" url = \"https://cornell.box.com/shared/static/3v2ftdkdhpz1lbbr4uhu0135w7m79p7q\" filename = \"office31.tar.gz\" md5 = \"89818e596f3cdda1d56da0f077435faa\" def __init__ ( self , root : str , domain : str , train : bool , transform = None , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of ```\"amazon\", \"dslr\", \"webcam\"```. train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" self . train = check_train ( train ) super () . __init__ ( root = root , domain = domain , ** kwargs ) self . transform = transform def set_paths_and_labels ( self , root ): name = \"train\" if self . train else \"test\" labels_file = os . path . join ( root , \"office31\" , f \" { self . domain } _ { name } .txt\" ) img_dir = os . path . join ( root , \"office31\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , self . domain ) check_length ( self , { \"amazon\" : { \"train\" : 2253 , \"test\" : 564 }[ name ], \"dslr\" : { \"train\" : 398 , \"test\" : 100 }[ name ], \"webcam\" : { \"train\" : 636 , \"test\" : 159 }[ name ], }[ self . domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ]","title":"Office31"},{"location":"docs/datasets/office31/#pytorch_adapt.datasets.office31.Office31Full","text":"Bases: BaseDataset A small dataset consisting of 31 classes in 3 domains: amazon, dslr, webcam. Source code in pytorch_adapt\\datasets\\office31.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class Office31Full ( BaseDataset ): \"\"\" A small dataset consisting of 31 classes in 3 domains: amazon, dslr, webcam. \"\"\" def __init__ ( self , root : str , domain : str , transform ): \"\"\" Arguments: root: The dataset must be located at ```<root>/office31``` domain: One of ```\"amazon\", \"dslr\", \"webcam\"```. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain ) self . transform = transform self . dataset = torch_datasets . ImageFolder ( os . path . join ( root , \"office31\" , domain , \"images\" ), transform = self . transform ) check_length ( self , { \"amazon\" : 2817 , \"dslr\" : 498 , \"webcam\" : 795 }[ domain ]) def __len__ ( self ): return len ( self . dataset ) def __getitem__ ( self , idx ): return self . dataset [ idx ]","title":"Office31Full"},{"location":"docs/datasets/officehome/","text":"OfficeHome \u00b6 Bases: BaseDownloadableDataset A custom train/test split of OfficeHomeFull . Extends BaseDownloadableDataset , so the dataset can be downloaded by setting download=True when initializing. Source code in pytorch_adapt\\datasets\\officehome.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 class OfficeHome ( BaseDownloadableDataset ): \"\"\" A custom train/test split of [OfficeHomeFull][pytorch_adapt.datasets.OfficeHomeFull]. Extends [BaseDownloadableDataset][pytorch_adapt.datasets.BaseDownloadableDataset], so the dataset can be downloaded by setting ```download=True``` when initializing. \"\"\" url = \"https://cornell.box.com/shared/static/xwsbubtcr8flqfuds5f6okqbr3z0w82t\" filename = \"officehome_resized.tar.gz\" md5 = \"52d6039512434aa561d66de9c10828c3\" def __init__ ( self , root : str , domain : str , train : bool , transform = None , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/officehome``` domain: One of ```\"art\", \"clipart\", \"product\", \"real\"```. train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" self . train = check_train ( train ) super () . __init__ ( root = root , domain = domain , ** kwargs ) self . transform = transform def set_paths_and_labels ( self , root ): name = \"train\" if self . train else \"test\" labels_file = os . path . join ( root , \"officehome\" , f \" { self . domain } _ { name } .txt\" ) img_dir = os . path . join ( root , \"officehome\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , self . domain ) check_length ( self , { \"art\" : { \"train\" : 1941 , \"test\" : 486 }[ name ], \"clipart\" : { \"train\" : 3492 , \"test\" : 873 }[ name ], \"product\" : { \"train\" : 3551 , \"test\" : 888 }[ name ], \"real\" : { \"train\" : 3485 , \"test\" : 872 }[ name ], }[ self . domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ] __init__ ( root , domain , train , transform = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/officehome required domain str One of \"art\", \"clipart\", \"product\", \"real\" . required train bool Whether or not to use the training set. required transform The image transform applied to each sample. None Source code in pytorch_adapt\\datasets\\officehome.py 52 53 54 55 56 57 58 59 60 61 62 def __init__ ( self , root : str , domain : str , train : bool , transform = None , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/officehome``` domain: One of ```\"art\", \"clipart\", \"product\", \"real\"```. train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" self . train = check_train ( train ) super () . __init__ ( root = root , domain = domain , ** kwargs ) self . transform = transform OfficeHomeFull \u00b6 Bases: BaseDataset A dataset consisting of 65 classes in 4 domains: art, clipart, product, and real. Source code in pytorch_adapt\\datasets\\officehome.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class OfficeHomeFull ( BaseDataset ): \"\"\" A dataset consisting of 65 classes in 4 domains: art, clipart, product, and real. \"\"\" def __init__ ( self , root : str , domain : str , transform ): \"\"\" Arguments: root: The dataset must be located at ```<root>/officehome``` domain: One of ```\"art\", \"clipart\", \"product\", \"real\"```. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain ) self . transform = transform self . dataset = torch_datasets . ImageFolder ( os . path . join ( root , \"officehome\" , domain ), transform = self . transform , ) check_length ( self , { \"art\" : 2427 , \"clipart\" : 4365 , \"product\" : 4439 , \"real\" : 4357 }[ domain ] ) def __len__ ( self ): return len ( self . dataset ) def __getitem__ ( self , idx ): return self . dataset [ idx ] __init__ ( root , domain , transform ) \u00b6 Parameters: Name Type Description Default root str The dataset must be located at <root>/officehome required domain str One of \"art\", \"clipart\", \"product\", \"real\" . required transform The image transform applied to each sample. required Source code in pytorch_adapt\\datasets\\officehome.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , root : str , domain : str , transform ): \"\"\" Arguments: root: The dataset must be located at ```<root>/officehome``` domain: One of ```\"art\", \"clipart\", \"product\", \"real\"```. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain ) self . transform = transform self . dataset = torch_datasets . ImageFolder ( os . path . join ( root , \"officehome\" , domain ), transform = self . transform , ) check_length ( self , { \"art\" : 2427 , \"clipart\" : 4365 , \"product\" : 4439 , \"real\" : 4357 }[ domain ] )","title":"officehome"},{"location":"docs/datasets/officehome/#pytorch_adapt.datasets.officehome.OfficeHome","text":"Bases: BaseDownloadableDataset A custom train/test split of OfficeHomeFull . Extends BaseDownloadableDataset , so the dataset can be downloaded by setting download=True when initializing. Source code in pytorch_adapt\\datasets\\officehome.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 class OfficeHome ( BaseDownloadableDataset ): \"\"\" A custom train/test split of [OfficeHomeFull][pytorch_adapt.datasets.OfficeHomeFull]. Extends [BaseDownloadableDataset][pytorch_adapt.datasets.BaseDownloadableDataset], so the dataset can be downloaded by setting ```download=True``` when initializing. \"\"\" url = \"https://cornell.box.com/shared/static/xwsbubtcr8flqfuds5f6okqbr3z0w82t\" filename = \"officehome_resized.tar.gz\" md5 = \"52d6039512434aa561d66de9c10828c3\" def __init__ ( self , root : str , domain : str , train : bool , transform = None , ** kwargs ): \"\"\" Arguments: root: The dataset must be located at ```<root>/officehome``` domain: One of ```\"art\", \"clipart\", \"product\", \"real\"```. train: Whether or not to use the training set. transform: The image transform applied to each sample. \"\"\" self . train = check_train ( train ) super () . __init__ ( root = root , domain = domain , ** kwargs ) self . transform = transform def set_paths_and_labels ( self , root ): name = \"train\" if self . train else \"test\" labels_file = os . path . join ( root , \"officehome\" , f \" { self . domain } _ { name } .txt\" ) img_dir = os . path . join ( root , \"officehome\" ) with open ( labels_file ) as f : content = [ line . rstrip () . split ( \" \" ) for line in f ] self . img_paths = [ os . path . join ( img_dir , x [ 0 ]) for x in content ] check_img_paths ( img_dir , self . img_paths , self . domain ) check_length ( self , { \"art\" : { \"train\" : 1941 , \"test\" : 486 }[ name ], \"clipart\" : { \"train\" : 3492 , \"test\" : 873 }[ name ], \"product\" : { \"train\" : 3551 , \"test\" : 888 }[ name ], \"real\" : { \"train\" : 3485 , \"test\" : 872 }[ name ], }[ self . domain ], ) self . labels = [ int ( x [ 1 ]) for x in content ]","title":"OfficeHome"},{"location":"docs/datasets/officehome/#pytorch_adapt.datasets.officehome.OfficeHomeFull","text":"Bases: BaseDataset A dataset consisting of 65 classes in 4 domains: art, clipart, product, and real. Source code in pytorch_adapt\\datasets\\officehome.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class OfficeHomeFull ( BaseDataset ): \"\"\" A dataset consisting of 65 classes in 4 domains: art, clipart, product, and real. \"\"\" def __init__ ( self , root : str , domain : str , transform ): \"\"\" Arguments: root: The dataset must be located at ```<root>/officehome``` domain: One of ```\"art\", \"clipart\", \"product\", \"real\"```. transform: The image transform applied to each sample. \"\"\" super () . __init__ ( domain = domain ) self . transform = transform self . dataset = torch_datasets . ImageFolder ( os . path . join ( root , \"officehome\" , domain ), transform = self . transform , ) check_length ( self , { \"art\" : 2427 , \"clipart\" : 4365 , \"product\" : 4439 , \"real\" : 4357 }[ domain ] ) def __len__ ( self ): return len ( self . dataset ) def __getitem__ ( self , idx ): return self . dataset [ idx ]","title":"OfficeHomeFull"},{"location":"docs/datasets/pseudo_labeled_dataset/","text":"PseudoLabeledDataset \u00b6 Bases: DomainDataset The wrapped dataset's __getitem__ function should return a tuple of (data, label) . The label will then be discarded, and the pseudo label will be returned instead. Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class PseudoLabeledDataset ( DomainDataset ): \"\"\" The wrapped dataset's ```__getitem__``` function should return a tuple of ```(data, label)```. The label will then be discarded, and the pseudo label will be returned instead. \"\"\" def __init__ ( self , dataset : Dataset , pseudo_labels : List [ int ], domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap pseudo_labels: The class labels that will be used instead of the labels contained in self.dataset domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain ) if len ( self . dataset ) != len ( pseudo_labels ): raise ValueError ( \"len(dataset) must equal len(pseudo_labels)\" ) self . pseudo_labels = pseudo_labels def __len__ ( self ): return len ( self . dataset ) def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the pseudo label) - \"src_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : self . pseudo_labels [ idx ], \"src_sample_idx\" : idx , } __getitem__ ( idx ) \u00b6 Returns: Type Description Dict [ str , Any ] A dictionary with keys Dict [ str , Any ] \"src_imgs\" (the data) Dict [ str , Any ] \"src_domain\" (the integer representing the domain) Dict [ str , Any ] \"src_labels\" (the pseudo label) Dict [ str , Any ] \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the pseudo label) - \"src_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : self . pseudo_labels [ idx ], \"src_sample_idx\" : idx , } __init__ ( dataset , pseudo_labels , domain = 0 ) \u00b6 Parameters: Name Type Description Default dataset Dataset The dataset to wrap required pseudo_labels List [ int ] The class labels that will be used instead of the labels contained in self.dataset required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py 16 17 18 19 20 21 22 23 24 25 26 27 28 def __init__ ( self , dataset : Dataset , pseudo_labels : List [ int ], domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap pseudo_labels: The class labels that will be used instead of the labels contained in self.dataset domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain ) if len ( self . dataset ) != len ( pseudo_labels ): raise ValueError ( \"len(dataset) must equal len(pseudo_labels)\" ) self . pseudo_labels = pseudo_labels","title":"pseudo_labeled_dataset"},{"location":"docs/datasets/pseudo_labeled_dataset/#pytorch_adapt.datasets.pseudo_labeled_dataset.PseudoLabeledDataset","text":"Bases: DomainDataset The wrapped dataset's __getitem__ function should return a tuple of (data, label) . The label will then be discarded, and the pseudo label will be returned instead. Source code in pytorch_adapt\\datasets\\pseudo_labeled_dataset.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class PseudoLabeledDataset ( DomainDataset ): \"\"\" The wrapped dataset's ```__getitem__``` function should return a tuple of ```(data, label)```. The label will then be discarded, and the pseudo label will be returned instead. \"\"\" def __init__ ( self , dataset : Dataset , pseudo_labels : List [ int ], domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap pseudo_labels: The class labels that will be used instead of the labels contained in self.dataset domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain ) if len ( self . dataset ) != len ( pseudo_labels ): raise ValueError ( \"len(dataset) must equal len(pseudo_labels)\" ) self . pseudo_labels = pseudo_labels def __len__ ( self ): return len ( self . dataset ) def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the pseudo label) - \"src_sample_idx\" (idx) \"\"\" img , _ = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : self . pseudo_labels [ idx ], \"src_sample_idx\" : idx , }","title":"PseudoLabeledDataset"},{"location":"docs/datasets/source_dataset/","text":"SourceDataset \u00b6 Bases: DomainDataset Wrap your source dataset with this. Your source dataset's __getitem__ function should return a tuple of (data, label) . Source code in pytorch_adapt\\datasets\\source_dataset.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class SourceDataset ( DomainDataset ): \"\"\" Wrap your source dataset with this. Your source dataset's ```__getitem__``` function should return a tuple of ```(data, label)```. \"\"\" def __init__ ( self , dataset : Dataset , domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain ) def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the class label) - \"src_sample_idx\" (idx) \"\"\" img , src_labels = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : src_labels , \"src_sample_idx\" : idx , } __getitem__ ( idx ) \u00b6 Returns: Type Description Dict [ str , Any ] A dictionary with keys Dict [ str , Any ] \"src_imgs\" (the data) Dict [ str , Any ] \"src_domain\" (the integer representing the domain) Dict [ str , Any ] \"src_labels\" (the class label) Dict [ str , Any ] \"src_sample_idx\" (idx) Source code in pytorch_adapt\\datasets\\source_dataset.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the class label) - \"src_sample_idx\" (idx) \"\"\" img , src_labels = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : src_labels , \"src_sample_idx\" : idx , } __init__ ( dataset , domain = 0 ) \u00b6 Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 0 Source code in pytorch_adapt\\datasets\\source_dataset.py 14 15 16 17 18 19 20 def __init__ ( self , dataset : Dataset , domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain )","title":"source_dataset"},{"location":"docs/datasets/source_dataset/#pytorch_adapt.datasets.source_dataset.SourceDataset","text":"Bases: DomainDataset Wrap your source dataset with this. Your source dataset's __getitem__ function should return a tuple of (data, label) . Source code in pytorch_adapt\\datasets\\source_dataset.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class SourceDataset ( DomainDataset ): \"\"\" Wrap your source dataset with this. Your source dataset's ```__getitem__``` function should return a tuple of ```(data, label)```. \"\"\" def __init__ ( self , dataset : Dataset , domain : int = 0 ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. \"\"\" super () . __init__ ( dataset , domain ) def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys - \"src_imgs\" (the data) - \"src_domain\" (the integer representing the domain) - \"src_labels\" (the class label) - \"src_sample_idx\" (idx) \"\"\" img , src_labels = self . dataset [ idx ] return { \"src_imgs\" : img , \"src_domain\" : self . domain , \"src_labels\" : src_labels , \"src_sample_idx\" : idx , }","title":"SourceDataset"},{"location":"docs/datasets/target_dataset/","text":"TargetDataset \u00b6 Bases: DomainDataset Wrap your target dataset with this. If supervised = True , the wrapped dataset's __getitem__ must return a tuple of (data, label) . Otherwise it can return either (data, label) or data . Source code in pytorch_adapt\\datasets\\target_dataset.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class TargetDataset ( DomainDataset ): \"\"\" Wrap your target dataset with this. If ```supervised = True```, the wrapped dataset's ```__getitem__``` must return a tuple of ```(data, label)```. Otherwise it can return either ```(data, label)``` or ```data```. \"\"\" def __init__ ( self , dataset : Dataset , domain : int = 1 , supervised : bool = False ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. supervised: A boolean for if the target dataset should return labels. \"\"\" super () . __init__ ( dataset , domain ) self . supervised = supervised def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys - \"target_imgs\" (the data) - \"target_domain\" (the integer representing the domain) - \"target_sample_idx\" (idx) If ```supervised = True``` it returns an extra key - \"target_labels\" (the class label) \"\"\" has_labels = False img = self . dataset [ idx ] if isinstance ( img , ( list , tuple )): has_labels = True img , labels = img if self . supervised and not has_labels : raise ValueError ( \"if TargetDataset is instantiated with supervised=True, the wrapped dataset must include labels\" ) item = { \"target_imgs\" : img , \"target_domain\" : self . domain , \"target_sample_idx\" : idx , } if self . supervised : item [ \"target_labels\" ] = labels return item __getitem__ ( idx ) \u00b6 Returns: Type Description Dict [ str , Any ] A dictionary with keys Dict [ str , Any ] \"target_imgs\" (the data) Dict [ str , Any ] \"target_domain\" (the integer representing the domain) Dict [ str , Any ] \"target_sample_idx\" (idx) Dict [ str , Any ] If supervised = True it returns an extra key Dict [ str , Any ] \"target_labels\" (the class label) Source code in pytorch_adapt\\datasets\\target_dataset.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys - \"target_imgs\" (the data) - \"target_domain\" (the integer representing the domain) - \"target_sample_idx\" (idx) If ```supervised = True``` it returns an extra key - \"target_labels\" (the class label) \"\"\" has_labels = False img = self . dataset [ idx ] if isinstance ( img , ( list , tuple )): has_labels = True img , labels = img if self . supervised and not has_labels : raise ValueError ( \"if TargetDataset is instantiated with supervised=True, the wrapped dataset must include labels\" ) item = { \"target_imgs\" : img , \"target_domain\" : self . domain , \"target_sample_idx\" : idx , } if self . supervised : item [ \"target_labels\" ] = labels return item __init__ ( dataset , domain = 1 , supervised = False ) \u00b6 Parameters: Name Type Description Default dataset Dataset The dataset to wrap required domain int An integer representing the domain. 1 supervised bool A boolean for if the target dataset should return labels. False Source code in pytorch_adapt\\datasets\\target_dataset.py 17 18 19 20 21 22 23 24 25 def __init__ ( self , dataset : Dataset , domain : int = 1 , supervised : bool = False ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. supervised: A boolean for if the target dataset should return labels. \"\"\" super () . __init__ ( dataset , domain ) self . supervised = supervised","title":"target_dataset"},{"location":"docs/datasets/target_dataset/#pytorch_adapt.datasets.target_dataset.TargetDataset","text":"Bases: DomainDataset Wrap your target dataset with this. If supervised = True , the wrapped dataset's __getitem__ must return a tuple of (data, label) . Otherwise it can return either (data, label) or data . Source code in pytorch_adapt\\datasets\\target_dataset.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class TargetDataset ( DomainDataset ): \"\"\" Wrap your target dataset with this. If ```supervised = True```, the wrapped dataset's ```__getitem__``` must return a tuple of ```(data, label)```. Otherwise it can return either ```(data, label)``` or ```data```. \"\"\" def __init__ ( self , dataset : Dataset , domain : int = 1 , supervised : bool = False ): \"\"\" Arguments: dataset: The dataset to wrap domain: An integer representing the domain. supervised: A boolean for if the target dataset should return labels. \"\"\" super () . __init__ ( dataset , domain ) self . supervised = supervised def __getitem__ ( self , idx : int ) -> Dict [ str , Any ]: \"\"\" Returns: A dictionary with keys - \"target_imgs\" (the data) - \"target_domain\" (the integer representing the domain) - \"target_sample_idx\" (idx) If ```supervised = True``` it returns an extra key - \"target_labels\" (the class label) \"\"\" has_labels = False img = self . dataset [ idx ] if isinstance ( img , ( list , tuple )): has_labels = True img , labels = img if self . supervised and not has_labels : raise ValueError ( \"if TargetDataset is instantiated with supervised=True, the wrapped dataset must include labels\" ) item = { \"target_imgs\" : img , \"target_domain\" : self . domain , \"target_sample_idx\" : idx , } if self . supervised : item [ \"target_labels\" ] = labels return item","title":"TargetDataset"},{"location":"docs/frameworks/","text":"","title":"frameworks"},{"location":"docs/frameworks/ignite/","text":"The following can be imported like this (using CheckpointFnCreator as an example): from pytorch_adapt.frameworks.ignite import CheckpointFnCreator Direct module members \u00b6 CheckpointFnCreator Ignite IgnitePredsAsFeatures IgniteValHookWrapper","title":"ignite"},{"location":"docs/frameworks/ignite/#direct-module-members","text":"CheckpointFnCreator Ignite IgnitePredsAsFeatures IgniteValHookWrapper","title":"Direct module members"},{"location":"docs/frameworks/ignite/checkpoint_utils/","text":"CheckpointFnCreator \u00b6 This class creates a checkpointing function for use with the Ignite wrapper. Source code in pytorch_adapt\\frameworks\\ignite\\checkpoint_utils.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 class CheckpointFnCreator : \"\"\" This class creates a checkpointing function for use with the [```Ignite```][pytorch_adapt.frameworks.ignite.Ignite] wrapper. \"\"\" def __init__ ( self , ** kwargs ): \"\"\" Arguments: **kwargs: Optional arguments that will be passed to PyTorch Ignite's [```ModelCheckpoint```](https://pytorch.org/ignite/v0.4.8/generated/ignite.handlers.checkpoint.ModelCheckpoint.html) class. \"\"\" self . kwargs = { \"filename_prefix\" : \"\" , \"global_step_transform\" : global_step_transform , \"filename_pattern\" : \" {filename_prefix}{name} _ {global_step} . {ext} \" , ** kwargs , } # Create handler here in case needed by load_objects or last_checkpoint # before __call__ is used self . objs = ModelCheckpoint ( ** self . kwargs ) # For saving self.objs. Only save the very latest (n_saved = 1) self . ckpter = ModelCheckpoint ( ** { ** self . kwargs , \"n_saved\" : 1 }) def __call__ ( self , adapter = None , validator = None , val_hooks = None , ** kwargs , ): \"\"\" Creates the checkpointing function. Arguments: adapter: An [```Adapter```][pytorch_adapt.adapters.BaseAdapter] object. validator: A [```ScoreHistory```][pytorch_adapt.validators.ScoreHistory] object. val_hooks: A list of functions called during validation. See [```Ignite```][pytorch_adapt.frameworks.ignite.Ignite] for details. \"\"\" self . objs = ModelCheckpoint ( ** { ** self . kwargs , ** kwargs }) dict_to_save = {} if adapter : dict_to_save . update ( adapter_to_dict ( adapter )) if validator : dict_to_save [ \"validator\" ] = validator if val_hooks : dict_to_save . update ( val_hooks_to_dict ( val_hooks )) def fn ( engine ): self . objs ( engine , { \"engine\" : engine , ** dict_to_save }) self . ckpter ( engine , { \"checkpointer\" : self . objs }) return fn def load_objects ( self , to_load , checkpoint = None , global_step = None , ** kwargs ): # This can be simplified once this issue is resolved https://github.com/pytorch/ignite/issues/2480 if global_step is not None : dirname = self . objs . save_handler . dirname filename_dict = { \"filename_prefix\" : self . objs . filename_prefix , \"name\" : \"checkpoint\" , \"ext\" : self . objs . ext , \"score_name\" : self . objs . score_name , \"global_step\" : global_step , } filename = self . objs . filename_pattern . format ( ** filename_dict ) checkpoint = os . path . join ( dirname , filename ) to_load = { k : v for k , v in to_load . items () if v } self . objs . load_objects ( to_load , str ( checkpoint ), ** kwargs ) def load_best_checkpoint ( self , to_load ): last_checkpoint = self . get_best_checkpoint () self . load_objects ( to_load , last_checkpoint ) def get_best_checkpoint ( self ): if self . objs . last_checkpoint : return self . objs . last_checkpoint ckpter_last_checkpoint = self . ckpter . last_checkpoint if not ckpter_last_checkpoint : files = glob . glob ( os . path . join ( self . ckpter . save_handler . dirname , \"*checkpointer*.pt\" ) ) if len ( files ) > 1 : raise ValueError ( \"there should only be 1 matching checkpointer file\" ) ckpter_last_checkpoint = files [ 0 ] self . ckpter . load_objects ( { \"checkpointer\" : self . objs }, str ( ckpter_last_checkpoint ) ) return self . objs . last_checkpoint __call__ ( adapter = None , validator = None , val_hooks = None , ** kwargs ) \u00b6 Creates the checkpointing function. Parameters: Name Type Description Default adapter An Adapter object. None validator A ScoreHistory object. None val_hooks A list of functions called during validation. See Ignite for details. None Source code in pytorch_adapt\\frameworks\\ignite\\checkpoint_utils.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def __call__ ( self , adapter = None , validator = None , val_hooks = None , ** kwargs , ): \"\"\" Creates the checkpointing function. Arguments: adapter: An [```Adapter```][pytorch_adapt.adapters.BaseAdapter] object. validator: A [```ScoreHistory```][pytorch_adapt.validators.ScoreHistory] object. val_hooks: A list of functions called during validation. See [```Ignite```][pytorch_adapt.frameworks.ignite.Ignite] for details. \"\"\" self . objs = ModelCheckpoint ( ** { ** self . kwargs , ** kwargs }) dict_to_save = {} if adapter : dict_to_save . update ( adapter_to_dict ( adapter )) if validator : dict_to_save [ \"validator\" ] = validator if val_hooks : dict_to_save . update ( val_hooks_to_dict ( val_hooks )) def fn ( engine ): self . objs ( engine , { \"engine\" : engine , ** dict_to_save }) self . ckpter ( engine , { \"checkpointer\" : self . objs }) return fn __init__ ( ** kwargs ) \u00b6 Parameters: Name Type Description Default **kwargs Optional arguments that will be passed to PyTorch Ignite's ModelCheckpoint class. {} Source code in pytorch_adapt\\frameworks\\ignite\\checkpoint_utils.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , ** kwargs ): \"\"\" Arguments: **kwargs: Optional arguments that will be passed to PyTorch Ignite's [```ModelCheckpoint```](https://pytorch.org/ignite/v0.4.8/generated/ignite.handlers.checkpoint.ModelCheckpoint.html) class. \"\"\" self . kwargs = { \"filename_prefix\" : \"\" , \"global_step_transform\" : global_step_transform , \"filename_pattern\" : \" {filename_prefix}{name} _ {global_step} . {ext} \" , ** kwargs , } # Create handler here in case needed by load_objects or last_checkpoint # before __call__ is used self . objs = ModelCheckpoint ( ** self . kwargs ) # For saving self.objs. Only save the very latest (n_saved = 1) self . ckpter = ModelCheckpoint ( ** { ** self . kwargs , \"n_saved\" : 1 })","title":"checkpoint_utils"},{"location":"docs/frameworks/ignite/checkpoint_utils/#pytorch_adapt.frameworks.ignite.checkpoint_utils.CheckpointFnCreator","text":"This class creates a checkpointing function for use with the Ignite wrapper. Source code in pytorch_adapt\\frameworks\\ignite\\checkpoint_utils.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 class CheckpointFnCreator : \"\"\" This class creates a checkpointing function for use with the [```Ignite```][pytorch_adapt.frameworks.ignite.Ignite] wrapper. \"\"\" def __init__ ( self , ** kwargs ): \"\"\" Arguments: **kwargs: Optional arguments that will be passed to PyTorch Ignite's [```ModelCheckpoint```](https://pytorch.org/ignite/v0.4.8/generated/ignite.handlers.checkpoint.ModelCheckpoint.html) class. \"\"\" self . kwargs = { \"filename_prefix\" : \"\" , \"global_step_transform\" : global_step_transform , \"filename_pattern\" : \" {filename_prefix}{name} _ {global_step} . {ext} \" , ** kwargs , } # Create handler here in case needed by load_objects or last_checkpoint # before __call__ is used self . objs = ModelCheckpoint ( ** self . kwargs ) # For saving self.objs. Only save the very latest (n_saved = 1) self . ckpter = ModelCheckpoint ( ** { ** self . kwargs , \"n_saved\" : 1 }) def __call__ ( self , adapter = None , validator = None , val_hooks = None , ** kwargs , ): \"\"\" Creates the checkpointing function. Arguments: adapter: An [```Adapter```][pytorch_adapt.adapters.BaseAdapter] object. validator: A [```ScoreHistory```][pytorch_adapt.validators.ScoreHistory] object. val_hooks: A list of functions called during validation. See [```Ignite```][pytorch_adapt.frameworks.ignite.Ignite] for details. \"\"\" self . objs = ModelCheckpoint ( ** { ** self . kwargs , ** kwargs }) dict_to_save = {} if adapter : dict_to_save . update ( adapter_to_dict ( adapter )) if validator : dict_to_save [ \"validator\" ] = validator if val_hooks : dict_to_save . update ( val_hooks_to_dict ( val_hooks )) def fn ( engine ): self . objs ( engine , { \"engine\" : engine , ** dict_to_save }) self . ckpter ( engine , { \"checkpointer\" : self . objs }) return fn def load_objects ( self , to_load , checkpoint = None , global_step = None , ** kwargs ): # This can be simplified once this issue is resolved https://github.com/pytorch/ignite/issues/2480 if global_step is not None : dirname = self . objs . save_handler . dirname filename_dict = { \"filename_prefix\" : self . objs . filename_prefix , \"name\" : \"checkpoint\" , \"ext\" : self . objs . ext , \"score_name\" : self . objs . score_name , \"global_step\" : global_step , } filename = self . objs . filename_pattern . format ( ** filename_dict ) checkpoint = os . path . join ( dirname , filename ) to_load = { k : v for k , v in to_load . items () if v } self . objs . load_objects ( to_load , str ( checkpoint ), ** kwargs ) def load_best_checkpoint ( self , to_load ): last_checkpoint = self . get_best_checkpoint () self . load_objects ( to_load , last_checkpoint ) def get_best_checkpoint ( self ): if self . objs . last_checkpoint : return self . objs . last_checkpoint ckpter_last_checkpoint = self . ckpter . last_checkpoint if not ckpter_last_checkpoint : files = glob . glob ( os . path . join ( self . ckpter . save_handler . dirname , \"*checkpointer*.pt\" ) ) if len ( files ) > 1 : raise ValueError ( \"there should only be 1 matching checkpointer file\" ) ckpter_last_checkpoint = files [ 0 ] self . ckpter . load_objects ( { \"checkpointer\" : self . objs }, str ( ckpter_last_checkpoint ) ) return self . objs . last_checkpoint","title":"CheckpointFnCreator"},{"location":"docs/frameworks/ignite/ignite/","text":"Ignite \u00b6 Wraps an Adapter and takes care of validation, model saving, etc. by using the event handler system of PyTorch Ignite. Source code in pytorch_adapt\\frameworks\\ignite\\ignite.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 class Ignite : \"\"\" Wraps an [Adapter][pytorch_adapt.adapters.BaseAdapter] and takes care of validation, model saving, etc. by using the event handler system of PyTorch Ignite. \"\"\" def __init__ ( self , adapter , validator = None , val_hooks = None , checkpoint_fn = None , logger = None , log_freq = 50 , with_pbars = True , device = None , auto_dist = True , ): \"\"\" Arguments: adapter: An [```Adapter```][pytorch_adapt.adapters.BaseAdapter] object, which contains the training and inference steps. validator: An optional [```ScoreHistory```][pytorch_adapt.validators.ScoreHistory] object, which is used to determine the best checkpoint. val_hooks: A list of functions that are called during validation. Each function should be one of the following types: - [```Validator```][pytorch_adapt.validators.BaseValidator] - [```ScoreHistory```][pytorch_adapt.validators.ScoreHistory] - A callable that accepts the following arguments: - ```epoch``` (an integer) - ```**kwargs```, keyword arguments mapping from dataset split name to dictionaries containing features, labels etc. You can give your custom callable a list attribute named ```required_data```. For example, setting ```self.required_data = [\"src_train\", \"target_train\"]```, will ensure that the validation runner gathers data for the ```\"src_train\"``` and ```\"target_train\"``` splits. checkpoint_fn: A [```CheckpointFnCreator```][pytorch_adapt.frameworks.ignite.CheckpointFnCreator] object. logger: An object with the following functions: - ```add_training(adapter)``` - ```add_validation(data, epoch)```, where ```data``` is ```{validator_name: validator}```, and ```validator_name``` is usually just ```\"validator\"```. - ```write(engine)``` log_freq: The number of iterations between logging with_pbars: If ```True```, progress bars are shown during each epoch. device: If ```None```, then it defaults to [```idist.device()```](https://pytorch.org/ignite/v0.4.8/distributed.html#ignite.distributed.utils.device). auto_dist: If ```True``` and ```device == None``` then [```auto_model```](https://pytorch.org/ignite/v0.4.8/generated/ignite.distributed.auto.auto_model.html) and [```auto_optim```](https://pytorch.org/ignite/v0.4.8/generated/ignite.distributed.auto.auto_optim.html) are applied to the models and optimizers. \"\"\" self . adapter = adapter self . validator = validator self . val_hooks = c_f . default ( val_hooks , []) self . checkpoint_fn = checkpoint_fn self . logger = logger self . log_freq = log_freq self . with_pbars = with_pbars self . device = c_f . default ( device , idist . device , {}) self . trainer_init () self . collector_init () self . dist_init_done = False if device is None and auto_dist : self . dist_init () self . temp_events = [] def training_step ( self , engine , batch ): batch = c_f . batch_to_device ( batch , self . device ) return self . adapter . training_step ( batch ) def trainer_init ( self ): self . trainer = Engine ( self . training_step ) i_g . register ( self . trainer , Events . STARTED , * self . trainer_started_events ()) i_g . register ( self . trainer , Events . EPOCH_STARTED , * self . trainer_epoch_started_events () ) i_g . register ( self . trainer , Events . ITERATION_COMPLETED , * self . trainer_iteration_complete_events (), ) i_g . register ( self . trainer , Events . ITERATION_COMPLETED ( every = self . log_freq ), * self . trainer_log_freq_events (), ) i_g . register ( self . trainer , Events . EPOCH_COMPLETED , * self . trainer_epoch_complete_events () ) def trainer_started_events ( self ): def fn ( engine ): self . adapter . before_training_starts ( self ) return [ fn ] def trainer_epoch_started_events ( self ): return [ self . set_to_train ( self . adapter . models )] def trainer_iteration_complete_events ( self ): output = [ i_g . step_lr_schedulers ( self . adapter . lr_schedulers , \"per_step\" ), TerminateOnNan (), ] pbars = i_g . set_loggers_and_pbars ( self , [ \"trainer\" ]) if self . with_pbars : output . append ( i_g . pbar_print_losses ( pbars [ \"trainer\" ])) return output def trainer_log_freq_events ( self ): output = [] if self . logger : output . append ( self . logger . add_training ( self . adapter )) return output def trainer_epoch_complete_events ( self ): output = [ i_g . step_lr_schedulers ( self . adapter . lr_schedulers , \"per_epoch\" ), i_g . zero_grad ( self . adapter ), ] if self . logger : output . append ( self . logger . write ) return output def collector_init ( self ): self . collector = Engine ( self . get_collector_step ( self . adapter . inference )) i_g . set_loggers_and_pbars ( self , [ \"collector\" ]) i_g . register ( self . collector , Events . EPOCH_STARTED , self . set_to_eval ( self . adapter . models ), ) def dist_init ( self , * args , ** kwargs ): if not self . dist_init_done : self . adapter . models . apply ( i_g . auto_model ( * args , ** kwargs )) self . adapter . optimizers . apply ( idist . auto_optim ) self . dist_init_done = True def get_training_length ( self ): max_epochs = self . trainer . state . max_epochs max_iters = max_epochs * self . trainer . state . epoch_length return max_epochs , max_iters def get_all_outputs ( self , dataloader , split_name ): dataloaders = { split_name : dataloader } return i_g . collect_from_dataloaders ( self . collector , dataloaders , [ split_name ]) def run ( self , datasets = None , dataloader_creator = None , dataloaders = None , val_interval = 1 , early_stopper_kwargs = None , resume = None , check_initial_score = False , ** trainer_kwargs , ) -> Union [ Tuple [ float , int ], Tuple [ None , None ]]: \"\"\" Trains and optionally validates on the input datasets. Arguments: datasets: A dictionary mapping from dataset split names to datasets. dataloader_creator: Creates dataloaders using ```datasets```, when ```dataloaders == None```. Default value is [```DataloaderCreator()```][pytorch_adapt.datasets.DataloaderCreator]. dataloaders: A dictionary mapping from dataset split names to dataloaders. If ```None```, then ```datasets``` must be provided. val_interval: ```self.validator``` and ```self.val_hooks``` will be called every ```val_interval``` epochs. early_stopper_kwargs: A dictionary of keyword arguments to be passed to [```EarlyStopping```](https://pytorch.org/ignite/v0.4.8/generated/ignite.handlers.early_stopping.EarlyStopping.html). resume: Resume training from a checkpoint. This should be either a string representing a file path, or an integer representing a global step. check_initial_score: If ```True```, then ```self.validator``` and ```self.val_hooks``` will be called before training starts. For example, you might use this to check the performance of a pretrained model before finetuning. **trainer_kwargs: Keyword arguments that are passed to the PyTorch Ignite [```run()```](https://pytorch.org/ignite/v0.4.8/generated/ignite.engine.engine.Engine.html#ignite.engine.engine.Engine.run) function. Returns: A tuple of ```(best_score, best_epoch)``` or ```(None, None)``` if no validator is used. \"\"\" if dataloaders is None : dataloader_creator = c_f . default ( dataloader_creator , DataloaderCreator ()) dataloaders = dataloader_creator ( ** datasets ) self . remove_temp_events () max_epochs = trainer_kwargs . get ( \"max_epochs\" , 1 ) condition = i_g . interval_condition ( val_interval , max_epochs ) if self . checkpoint_fn : self . add_checkpoint_fn ( condition , dataloaders ) elif self . validator or self . val_hooks : self . add_validation_runner ( condition , dataloaders ) if check_initial_score : self . add_validation_runner ( Events . STARTED , dataloaders ) if self . validator and early_stopper_kwargs : self . add_early_stopper ( val_interval , ** early_stopper_kwargs ) if resume is not None : self . load_checkpoint ( resume ) if not i_g . is_done ( self . trainer , max_epochs ): self . trainer . run ( dataloaders [ \"train\" ], ** trainer_kwargs ) self . remove_temp_events () if self . validator : return self . validator . best_score , self . validator . best_epoch return None , None def get_validation_runner ( self , dataloaders , ): return i_g . get_validation_runner ( self . collector , dataloaders , self . validator , self . val_hooks , self . logger , ) def add_validation_runner ( self , condition , dataloaders ): val_runner = self . get_validation_runner ( dataloaders ) self . add_temp_event_handler ( condition , val_runner ) def add_checkpoint_fn ( self , condition , dataloaders ): score_function = ( self . get_validation_runner ( dataloaders ) if self . validator else None ) self . add_temp_event_handler ( condition , self . checkpoint_fn ( adapter = self . adapter , validator = self . validator , val_hooks = self . val_hooks , score_function = score_function , ), ) if not self . validator and self . val_hooks : self . add_validation_runner ( condition , dataloaders ) def add_early_stopper ( self , val_interval , ** kwargs ): def score_fn ( _ ): return self . validator . latest_score self . add_temp_event_handler ( Events . EPOCH_COMPLETED ( every = val_interval ), i_g . early_stopper ( ** kwargs )( trainer = self . trainer , score_function = score_fn , ), ) def load_checkpoint ( self , resume ): to_load = { \"engine\" : self . trainer , \"validator\" : self . validator , ** checkpoint_utils . adapter_to_dict ( self . adapter ), ** checkpoint_utils . val_hooks_to_dict ( self . val_hooks ), } if isinstance ( resume , str ): kwargs = { \"checkpoint\" : resume } elif isinstance ( resume , int ): kwargs = { \"global_step\" : resume } else : raise TypeError ( \"resume must be a string representing a file path, or an integer representing a global step\" ) self . checkpoint_fn . load_objects ( to_load , ** kwargs ) i_g . resume_checks ( self . trainer , self . validator ) def evaluate_best_model ( self , datasets , validator , dataloader_creator = None ) -> float : \"\"\" Loads the best checkpoint and computes the score on the given datasets. This requires ```self.checkpoint_fn``` to be not ```None```. Arguments: datasets: A dictionary mapping from dataset split names to datasets. validator: A [Validator][pytorch_adapt.validators.BaseValidator] or [ScoreHistory][pytorch_adapt.validators.ScoreHistory] object, which will compute and return the score. dataloader_creator: If ```None```, it will default to [```DataloaderCreator()```][pytorch_adapt.datasets.DataloaderCreator]. Returns: The validator's score for the best checkpoint. \"\"\" c_f . LOGGER . info ( \"***EVALUATING BEST MODEL***\" ) dataloader_creator = c_f . default ( dataloader_creator , DataloaderCreator , {}) dataloaders = dataloader_creator ( ** datasets ) self . checkpoint_fn . load_best_checkpoint ({ \"models\" : self . adapter . models }) collected_data = i_g . collect_from_dataloaders ( self . collector , dataloaders , validator . required_data ) return val_utils . call_val_hook ( validator , collected_data ) def get_collector_step ( self , inference ): def collector_step ( engine , batch ): batch = c_f . batch_to_device ( batch , self . device ) return f_utils . collector_step ( inference , batch , f_utils . create_output_dict ) return collector_step def set_to_train ( self , models ): def handler ( engine ): c_f . LOGGER . info ( \"Setting models to train() mode\" ) models . train () return handler def set_to_eval ( self , models ): def handler ( engine ): c_f . LOGGER . info ( \"Setting models to eval() mode\" ) models . eval () return handler def add_temp_event_handler ( self , event , handler ): removable = self . trainer . add_event_handler ( event , handler ) self . temp_events . append ( removable ) def remove_temp_events ( self ): for h in self . temp_events : h . remove () self . temp_events = [] __init__ ( adapter , validator = None , val_hooks = None , checkpoint_fn = None , logger = None , log_freq = 50 , with_pbars = True , device = None , auto_dist = True ) \u00b6 Parameters: Name Type Description Default adapter An Adapter object, which contains the training and inference steps. required validator An optional ScoreHistory object, which is used to determine the best checkpoint. None val_hooks A list of functions that are called during validation. Each function should be one of the following types: Validator ScoreHistory A callable that accepts the following arguments: epoch (an integer) **kwargs , keyword arguments mapping from dataset split name to dictionaries containing features, labels etc. You can give your custom callable a list attribute named required_data . For example, setting self.required_data = [\"src_train\", \"target_train\"] , will ensure that the validation runner gathers data for the \"src_train\" and \"target_train\" splits. None checkpoint_fn A CheckpointFnCreator object. None logger An object with the following functions: add_training(adapter) add_validation(data, epoch) , where data is {validator_name: validator} , and validator_name is usually just \"validator\" . write(engine) None log_freq The number of iterations between logging 50 with_pbars If True , progress bars are shown during each epoch. True device If None , then it defaults to idist.device() . None auto_dist If True and device == None then auto_model and auto_optim are applied to the models and optimizers. True Source code in pytorch_adapt\\frameworks\\ignite\\ignite.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , adapter , validator = None , val_hooks = None , checkpoint_fn = None , logger = None , log_freq = 50 , with_pbars = True , device = None , auto_dist = True , ): \"\"\" Arguments: adapter: An [```Adapter```][pytorch_adapt.adapters.BaseAdapter] object, which contains the training and inference steps. validator: An optional [```ScoreHistory```][pytorch_adapt.validators.ScoreHistory] object, which is used to determine the best checkpoint. val_hooks: A list of functions that are called during validation. Each function should be one of the following types: - [```Validator```][pytorch_adapt.validators.BaseValidator] - [```ScoreHistory```][pytorch_adapt.validators.ScoreHistory] - A callable that accepts the following arguments: - ```epoch``` (an integer) - ```**kwargs```, keyword arguments mapping from dataset split name to dictionaries containing features, labels etc. You can give your custom callable a list attribute named ```required_data```. For example, setting ```self.required_data = [\"src_train\", \"target_train\"]```, will ensure that the validation runner gathers data for the ```\"src_train\"``` and ```\"target_train\"``` splits. checkpoint_fn: A [```CheckpointFnCreator```][pytorch_adapt.frameworks.ignite.CheckpointFnCreator] object. logger: An object with the following functions: - ```add_training(adapter)``` - ```add_validation(data, epoch)```, where ```data``` is ```{validator_name: validator}```, and ```validator_name``` is usually just ```\"validator\"```. - ```write(engine)``` log_freq: The number of iterations between logging with_pbars: If ```True```, progress bars are shown during each epoch. device: If ```None```, then it defaults to [```idist.device()```](https://pytorch.org/ignite/v0.4.8/distributed.html#ignite.distributed.utils.device). auto_dist: If ```True``` and ```device == None``` then [```auto_model```](https://pytorch.org/ignite/v0.4.8/generated/ignite.distributed.auto.auto_model.html) and [```auto_optim```](https://pytorch.org/ignite/v0.4.8/generated/ignite.distributed.auto.auto_optim.html) are applied to the models and optimizers. \"\"\" self . adapter = adapter self . validator = validator self . val_hooks = c_f . default ( val_hooks , []) self . checkpoint_fn = checkpoint_fn self . logger = logger self . log_freq = log_freq self . with_pbars = with_pbars self . device = c_f . default ( device , idist . device , {}) self . trainer_init () self . collector_init () self . dist_init_done = False if device is None and auto_dist : self . dist_init () self . temp_events = [] evaluate_best_model ( datasets , validator , dataloader_creator = None ) \u00b6 Loads the best checkpoint and computes the score on the given datasets. This requires self.checkpoint_fn to be not None . Parameters: Name Type Description Default datasets A dictionary mapping from dataset split names to datasets. required validator A Validator or ScoreHistory object, which will compute and return the score. required dataloader_creator If None , it will default to DataloaderCreator() . None Returns: Type Description float The validator's score for the best checkpoint. Source code in pytorch_adapt\\frameworks\\ignite\\ignite.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def evaluate_best_model ( self , datasets , validator , dataloader_creator = None ) -> float : \"\"\" Loads the best checkpoint and computes the score on the given datasets. This requires ```self.checkpoint_fn``` to be not ```None```. Arguments: datasets: A dictionary mapping from dataset split names to datasets. validator: A [Validator][pytorch_adapt.validators.BaseValidator] or [ScoreHistory][pytorch_adapt.validators.ScoreHistory] object, which will compute and return the score. dataloader_creator: If ```None```, it will default to [```DataloaderCreator()```][pytorch_adapt.datasets.DataloaderCreator]. Returns: The validator's score for the best checkpoint. \"\"\" c_f . LOGGER . info ( \"***EVALUATING BEST MODEL***\" ) dataloader_creator = c_f . default ( dataloader_creator , DataloaderCreator , {}) dataloaders = dataloader_creator ( ** datasets ) self . checkpoint_fn . load_best_checkpoint ({ \"models\" : self . adapter . models }) collected_data = i_g . collect_from_dataloaders ( self . collector , dataloaders , validator . required_data ) return val_utils . call_val_hook ( validator , collected_data ) run ( datasets = None , dataloader_creator = None , dataloaders = None , val_interval = 1 , early_stopper_kwargs = None , resume = None , check_initial_score = False , ** trainer_kwargs ) \u00b6 Trains and optionally validates on the input datasets. Parameters: Name Type Description Default datasets A dictionary mapping from dataset split names to datasets. None dataloader_creator Creates dataloaders using datasets , when dataloaders == None . Default value is DataloaderCreator() . None dataloaders A dictionary mapping from dataset split names to dataloaders. If None , then datasets must be provided. None val_interval self.validator and self.val_hooks will be called every val_interval epochs. 1 early_stopper_kwargs A dictionary of keyword arguments to be passed to EarlyStopping . None resume Resume training from a checkpoint. This should be either a string representing a file path, or an integer representing a global step. None check_initial_score If True , then self.validator and self.val_hooks will be called before training starts. For example, you might use this to check the performance of a pretrained model before finetuning. False **trainer_kwargs Keyword arguments that are passed to the PyTorch Ignite run() function. {} Returns: Type Description Union [ Tuple [ float , int ], Tuple [None, None]] A tuple of (best_score, best_epoch) or (None, None) Union [ Tuple [ float , int ], Tuple [None, None]] if no validator is used. Source code in pytorch_adapt\\frameworks\\ignite\\ignite.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def run ( self , datasets = None , dataloader_creator = None , dataloaders = None , val_interval = 1 , early_stopper_kwargs = None , resume = None , check_initial_score = False , ** trainer_kwargs , ) -> Union [ Tuple [ float , int ], Tuple [ None , None ]]: \"\"\" Trains and optionally validates on the input datasets. Arguments: datasets: A dictionary mapping from dataset split names to datasets. dataloader_creator: Creates dataloaders using ```datasets```, when ```dataloaders == None```. Default value is [```DataloaderCreator()```][pytorch_adapt.datasets.DataloaderCreator]. dataloaders: A dictionary mapping from dataset split names to dataloaders. If ```None```, then ```datasets``` must be provided. val_interval: ```self.validator``` and ```self.val_hooks``` will be called every ```val_interval``` epochs. early_stopper_kwargs: A dictionary of keyword arguments to be passed to [```EarlyStopping```](https://pytorch.org/ignite/v0.4.8/generated/ignite.handlers.early_stopping.EarlyStopping.html). resume: Resume training from a checkpoint. This should be either a string representing a file path, or an integer representing a global step. check_initial_score: If ```True```, then ```self.validator``` and ```self.val_hooks``` will be called before training starts. For example, you might use this to check the performance of a pretrained model before finetuning. **trainer_kwargs: Keyword arguments that are passed to the PyTorch Ignite [```run()```](https://pytorch.org/ignite/v0.4.8/generated/ignite.engine.engine.Engine.html#ignite.engine.engine.Engine.run) function. Returns: A tuple of ```(best_score, best_epoch)``` or ```(None, None)``` if no validator is used. \"\"\" if dataloaders is None : dataloader_creator = c_f . default ( dataloader_creator , DataloaderCreator ()) dataloaders = dataloader_creator ( ** datasets ) self . remove_temp_events () max_epochs = trainer_kwargs . get ( \"max_epochs\" , 1 ) condition = i_g . interval_condition ( val_interval , max_epochs ) if self . checkpoint_fn : self . add_checkpoint_fn ( condition , dataloaders ) elif self . validator or self . val_hooks : self . add_validation_runner ( condition , dataloaders ) if check_initial_score : self . add_validation_runner ( Events . STARTED , dataloaders ) if self . validator and early_stopper_kwargs : self . add_early_stopper ( val_interval , ** early_stopper_kwargs ) if resume is not None : self . load_checkpoint ( resume ) if not i_g . is_done ( self . trainer , max_epochs ): self . trainer . run ( dataloaders [ \"train\" ], ** trainer_kwargs ) self . remove_temp_events () if self . validator : return self . validator . best_score , self . validator . best_epoch return None , None","title":"ignite"},{"location":"docs/frameworks/ignite/ignite/#pytorch_adapt.frameworks.ignite.ignite.Ignite","text":"Wraps an Adapter and takes care of validation, model saving, etc. by using the event handler system of PyTorch Ignite. Source code in pytorch_adapt\\frameworks\\ignite\\ignite.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 class Ignite : \"\"\" Wraps an [Adapter][pytorch_adapt.adapters.BaseAdapter] and takes care of validation, model saving, etc. by using the event handler system of PyTorch Ignite. \"\"\" def __init__ ( self , adapter , validator = None , val_hooks = None , checkpoint_fn = None , logger = None , log_freq = 50 , with_pbars = True , device = None , auto_dist = True , ): \"\"\" Arguments: adapter: An [```Adapter```][pytorch_adapt.adapters.BaseAdapter] object, which contains the training and inference steps. validator: An optional [```ScoreHistory```][pytorch_adapt.validators.ScoreHistory] object, which is used to determine the best checkpoint. val_hooks: A list of functions that are called during validation. Each function should be one of the following types: - [```Validator```][pytorch_adapt.validators.BaseValidator] - [```ScoreHistory```][pytorch_adapt.validators.ScoreHistory] - A callable that accepts the following arguments: - ```epoch``` (an integer) - ```**kwargs```, keyword arguments mapping from dataset split name to dictionaries containing features, labels etc. You can give your custom callable a list attribute named ```required_data```. For example, setting ```self.required_data = [\"src_train\", \"target_train\"]```, will ensure that the validation runner gathers data for the ```\"src_train\"``` and ```\"target_train\"``` splits. checkpoint_fn: A [```CheckpointFnCreator```][pytorch_adapt.frameworks.ignite.CheckpointFnCreator] object. logger: An object with the following functions: - ```add_training(adapter)``` - ```add_validation(data, epoch)```, where ```data``` is ```{validator_name: validator}```, and ```validator_name``` is usually just ```\"validator\"```. - ```write(engine)``` log_freq: The number of iterations between logging with_pbars: If ```True```, progress bars are shown during each epoch. device: If ```None```, then it defaults to [```idist.device()```](https://pytorch.org/ignite/v0.4.8/distributed.html#ignite.distributed.utils.device). auto_dist: If ```True``` and ```device == None``` then [```auto_model```](https://pytorch.org/ignite/v0.4.8/generated/ignite.distributed.auto.auto_model.html) and [```auto_optim```](https://pytorch.org/ignite/v0.4.8/generated/ignite.distributed.auto.auto_optim.html) are applied to the models and optimizers. \"\"\" self . adapter = adapter self . validator = validator self . val_hooks = c_f . default ( val_hooks , []) self . checkpoint_fn = checkpoint_fn self . logger = logger self . log_freq = log_freq self . with_pbars = with_pbars self . device = c_f . default ( device , idist . device , {}) self . trainer_init () self . collector_init () self . dist_init_done = False if device is None and auto_dist : self . dist_init () self . temp_events = [] def training_step ( self , engine , batch ): batch = c_f . batch_to_device ( batch , self . device ) return self . adapter . training_step ( batch ) def trainer_init ( self ): self . trainer = Engine ( self . training_step ) i_g . register ( self . trainer , Events . STARTED , * self . trainer_started_events ()) i_g . register ( self . trainer , Events . EPOCH_STARTED , * self . trainer_epoch_started_events () ) i_g . register ( self . trainer , Events . ITERATION_COMPLETED , * self . trainer_iteration_complete_events (), ) i_g . register ( self . trainer , Events . ITERATION_COMPLETED ( every = self . log_freq ), * self . trainer_log_freq_events (), ) i_g . register ( self . trainer , Events . EPOCH_COMPLETED , * self . trainer_epoch_complete_events () ) def trainer_started_events ( self ): def fn ( engine ): self . adapter . before_training_starts ( self ) return [ fn ] def trainer_epoch_started_events ( self ): return [ self . set_to_train ( self . adapter . models )] def trainer_iteration_complete_events ( self ): output = [ i_g . step_lr_schedulers ( self . adapter . lr_schedulers , \"per_step\" ), TerminateOnNan (), ] pbars = i_g . set_loggers_and_pbars ( self , [ \"trainer\" ]) if self . with_pbars : output . append ( i_g . pbar_print_losses ( pbars [ \"trainer\" ])) return output def trainer_log_freq_events ( self ): output = [] if self . logger : output . append ( self . logger . add_training ( self . adapter )) return output def trainer_epoch_complete_events ( self ): output = [ i_g . step_lr_schedulers ( self . adapter . lr_schedulers , \"per_epoch\" ), i_g . zero_grad ( self . adapter ), ] if self . logger : output . append ( self . logger . write ) return output def collector_init ( self ): self . collector = Engine ( self . get_collector_step ( self . adapter . inference )) i_g . set_loggers_and_pbars ( self , [ \"collector\" ]) i_g . register ( self . collector , Events . EPOCH_STARTED , self . set_to_eval ( self . adapter . models ), ) def dist_init ( self , * args , ** kwargs ): if not self . dist_init_done : self . adapter . models . apply ( i_g . auto_model ( * args , ** kwargs )) self . adapter . optimizers . apply ( idist . auto_optim ) self . dist_init_done = True def get_training_length ( self ): max_epochs = self . trainer . state . max_epochs max_iters = max_epochs * self . trainer . state . epoch_length return max_epochs , max_iters def get_all_outputs ( self , dataloader , split_name ): dataloaders = { split_name : dataloader } return i_g . collect_from_dataloaders ( self . collector , dataloaders , [ split_name ]) def run ( self , datasets = None , dataloader_creator = None , dataloaders = None , val_interval = 1 , early_stopper_kwargs = None , resume = None , check_initial_score = False , ** trainer_kwargs , ) -> Union [ Tuple [ float , int ], Tuple [ None , None ]]: \"\"\" Trains and optionally validates on the input datasets. Arguments: datasets: A dictionary mapping from dataset split names to datasets. dataloader_creator: Creates dataloaders using ```datasets```, when ```dataloaders == None```. Default value is [```DataloaderCreator()```][pytorch_adapt.datasets.DataloaderCreator]. dataloaders: A dictionary mapping from dataset split names to dataloaders. If ```None```, then ```datasets``` must be provided. val_interval: ```self.validator``` and ```self.val_hooks``` will be called every ```val_interval``` epochs. early_stopper_kwargs: A dictionary of keyword arguments to be passed to [```EarlyStopping```](https://pytorch.org/ignite/v0.4.8/generated/ignite.handlers.early_stopping.EarlyStopping.html). resume: Resume training from a checkpoint. This should be either a string representing a file path, or an integer representing a global step. check_initial_score: If ```True```, then ```self.validator``` and ```self.val_hooks``` will be called before training starts. For example, you might use this to check the performance of a pretrained model before finetuning. **trainer_kwargs: Keyword arguments that are passed to the PyTorch Ignite [```run()```](https://pytorch.org/ignite/v0.4.8/generated/ignite.engine.engine.Engine.html#ignite.engine.engine.Engine.run) function. Returns: A tuple of ```(best_score, best_epoch)``` or ```(None, None)``` if no validator is used. \"\"\" if dataloaders is None : dataloader_creator = c_f . default ( dataloader_creator , DataloaderCreator ()) dataloaders = dataloader_creator ( ** datasets ) self . remove_temp_events () max_epochs = trainer_kwargs . get ( \"max_epochs\" , 1 ) condition = i_g . interval_condition ( val_interval , max_epochs ) if self . checkpoint_fn : self . add_checkpoint_fn ( condition , dataloaders ) elif self . validator or self . val_hooks : self . add_validation_runner ( condition , dataloaders ) if check_initial_score : self . add_validation_runner ( Events . STARTED , dataloaders ) if self . validator and early_stopper_kwargs : self . add_early_stopper ( val_interval , ** early_stopper_kwargs ) if resume is not None : self . load_checkpoint ( resume ) if not i_g . is_done ( self . trainer , max_epochs ): self . trainer . run ( dataloaders [ \"train\" ], ** trainer_kwargs ) self . remove_temp_events () if self . validator : return self . validator . best_score , self . validator . best_epoch return None , None def get_validation_runner ( self , dataloaders , ): return i_g . get_validation_runner ( self . collector , dataloaders , self . validator , self . val_hooks , self . logger , ) def add_validation_runner ( self , condition , dataloaders ): val_runner = self . get_validation_runner ( dataloaders ) self . add_temp_event_handler ( condition , val_runner ) def add_checkpoint_fn ( self , condition , dataloaders ): score_function = ( self . get_validation_runner ( dataloaders ) if self . validator else None ) self . add_temp_event_handler ( condition , self . checkpoint_fn ( adapter = self . adapter , validator = self . validator , val_hooks = self . val_hooks , score_function = score_function , ), ) if not self . validator and self . val_hooks : self . add_validation_runner ( condition , dataloaders ) def add_early_stopper ( self , val_interval , ** kwargs ): def score_fn ( _ ): return self . validator . latest_score self . add_temp_event_handler ( Events . EPOCH_COMPLETED ( every = val_interval ), i_g . early_stopper ( ** kwargs )( trainer = self . trainer , score_function = score_fn , ), ) def load_checkpoint ( self , resume ): to_load = { \"engine\" : self . trainer , \"validator\" : self . validator , ** checkpoint_utils . adapter_to_dict ( self . adapter ), ** checkpoint_utils . val_hooks_to_dict ( self . val_hooks ), } if isinstance ( resume , str ): kwargs = { \"checkpoint\" : resume } elif isinstance ( resume , int ): kwargs = { \"global_step\" : resume } else : raise TypeError ( \"resume must be a string representing a file path, or an integer representing a global step\" ) self . checkpoint_fn . load_objects ( to_load , ** kwargs ) i_g . resume_checks ( self . trainer , self . validator ) def evaluate_best_model ( self , datasets , validator , dataloader_creator = None ) -> float : \"\"\" Loads the best checkpoint and computes the score on the given datasets. This requires ```self.checkpoint_fn``` to be not ```None```. Arguments: datasets: A dictionary mapping from dataset split names to datasets. validator: A [Validator][pytorch_adapt.validators.BaseValidator] or [ScoreHistory][pytorch_adapt.validators.ScoreHistory] object, which will compute and return the score. dataloader_creator: If ```None```, it will default to [```DataloaderCreator()```][pytorch_adapt.datasets.DataloaderCreator]. Returns: The validator's score for the best checkpoint. \"\"\" c_f . LOGGER . info ( \"***EVALUATING BEST MODEL***\" ) dataloader_creator = c_f . default ( dataloader_creator , DataloaderCreator , {}) dataloaders = dataloader_creator ( ** datasets ) self . checkpoint_fn . load_best_checkpoint ({ \"models\" : self . adapter . models }) collected_data = i_g . collect_from_dataloaders ( self . collector , dataloaders , validator . required_data ) return val_utils . call_val_hook ( validator , collected_data ) def get_collector_step ( self , inference ): def collector_step ( engine , batch ): batch = c_f . batch_to_device ( batch , self . device ) return f_utils . collector_step ( inference , batch , f_utils . create_output_dict ) return collector_step def set_to_train ( self , models ): def handler ( engine ): c_f . LOGGER . info ( \"Setting models to train() mode\" ) models . train () return handler def set_to_eval ( self , models ): def handler ( engine ): c_f . LOGGER . info ( \"Setting models to eval() mode\" ) models . eval () return handler def add_temp_event_handler ( self , event , handler ): removable = self . trainer . add_event_handler ( event , handler ) self . temp_events . append ( removable ) def remove_temp_events ( self ): for h in self . temp_events : h . remove () self . temp_events = []","title":"Ignite"},{"location":"docs/frameworks/ignite/loggers/","text":"The following can be imported like this (using BasicLossLogger as an example): from pytorch_adapt.frameworks.ignite.loggers import BasicLossLogger Direct module members \u00b6 BasicLossLogger IgniteEmptyLogger IgniteRecordKeeperLogger","title":"loggers"},{"location":"docs/frameworks/ignite/loggers/#direct-module-members","text":"BasicLossLogger IgniteEmptyLogger IgniteRecordKeeperLogger","title":"Direct module members"},{"location":"docs/frameworks/ignite/loggers/ignite_record_keeper_logger/","text":"IgniteRecordKeeperLogger \u00b6 Uses record-keeper to record data tensorboard, csv, and sqlite. Source code in pytorch_adapt\\frameworks\\ignite\\loggers\\ignite_record_keeper_logger.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 class IgniteRecordKeeperLogger : \"\"\" Uses [record-keeper](https://github.com/KevinMusgrave/record-keeper) to record data tensorboard, csv, and sqlite. \"\"\" def __init__ ( self , folder = None , tensorboard_writer = None , record_writer = None , attr_list_names = None , ): \"\"\" Arguments: folder: path where records will be saved. tensorboard_writer: record_writer: a ```RecordWriter``` object (see record-keeper) \"\"\" from record_keeper import RecordKeeper , RecordWriter from torch.utils.tensorboard import SummaryWriter tensorboard_writer = c_f . default ( tensorboard_writer , SummaryWriter , { \"log_dir\" : folder , \"max_queue\" : 1000000 , \"flush_secs\" : 30 }, ) record_writer = c_f . default ( record_writer , RecordWriter , { \"folder\" : folder }) attr_list_names = c_f . default ( attr_list_names , pml_cf . list_of_recordable_attributes_list_names () ) self . record_keeper = RecordKeeper ( tensorboard_writer = tensorboard_writer , record_writer = record_writer , attributes_to_search_for = attr_list_names , ) def add_training ( self , adapter ): def fn ( engine ): record_these = [ ({ \"engine_output\" : engine . state . output }, {}), ( adapter . optimizers , { \"parent_name\" : \"optimizers\" , \"custom_attr_func\" : optimizer_attr_func , }, ), ({ \"misc\" : adapter . misc }, {}), ( { \"hook\" : adapter . hook }, { \"recursive_types\" : [ BaseHook , BaseWeighter , torch . nn . Module ]}, ), ] for record , kwargs in record_these : self . record_keeper . update_records ( record , engine . state . iteration , ** kwargs ) return fn def add_validation ( self , data , epoch ): self . record_keeper . update_records ( data , epoch ) def write ( self , engine ): self . record_keeper . save_records () self . record_keeper . tensorboard_writer . flush () __init__ ( folder = None , tensorboard_writer = None , record_writer = None , attr_list_names = None ) \u00b6 Parameters: Name Type Description Default folder path where records will be saved. None tensorboard_writer None record_writer a RecordWriter object (see record-keeper) None Source code in pytorch_adapt\\frameworks\\ignite\\loggers\\ignite_record_keeper_logger.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , folder = None , tensorboard_writer = None , record_writer = None , attr_list_names = None , ): \"\"\" Arguments: folder: path where records will be saved. tensorboard_writer: record_writer: a ```RecordWriter``` object (see record-keeper) \"\"\" from record_keeper import RecordKeeper , RecordWriter from torch.utils.tensorboard import SummaryWriter tensorboard_writer = c_f . default ( tensorboard_writer , SummaryWriter , { \"log_dir\" : folder , \"max_queue\" : 1000000 , \"flush_secs\" : 30 }, ) record_writer = c_f . default ( record_writer , RecordWriter , { \"folder\" : folder }) attr_list_names = c_f . default ( attr_list_names , pml_cf . list_of_recordable_attributes_list_names () ) self . record_keeper = RecordKeeper ( tensorboard_writer = tensorboard_writer , record_writer = record_writer , attributes_to_search_for = attr_list_names , )","title":"ignite_record_keeper_logger"},{"location":"docs/frameworks/ignite/loggers/ignite_record_keeper_logger/#pytorch_adapt.frameworks.ignite.loggers.ignite_record_keeper_logger.IgniteRecordKeeperLogger","text":"Uses record-keeper to record data tensorboard, csv, and sqlite. Source code in pytorch_adapt\\frameworks\\ignite\\loggers\\ignite_record_keeper_logger.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 class IgniteRecordKeeperLogger : \"\"\" Uses [record-keeper](https://github.com/KevinMusgrave/record-keeper) to record data tensorboard, csv, and sqlite. \"\"\" def __init__ ( self , folder = None , tensorboard_writer = None , record_writer = None , attr_list_names = None , ): \"\"\" Arguments: folder: path where records will be saved. tensorboard_writer: record_writer: a ```RecordWriter``` object (see record-keeper) \"\"\" from record_keeper import RecordKeeper , RecordWriter from torch.utils.tensorboard import SummaryWriter tensorboard_writer = c_f . default ( tensorboard_writer , SummaryWriter , { \"log_dir\" : folder , \"max_queue\" : 1000000 , \"flush_secs\" : 30 }, ) record_writer = c_f . default ( record_writer , RecordWriter , { \"folder\" : folder }) attr_list_names = c_f . default ( attr_list_names , pml_cf . list_of_recordable_attributes_list_names () ) self . record_keeper = RecordKeeper ( tensorboard_writer = tensorboard_writer , record_writer = record_writer , attributes_to_search_for = attr_list_names , ) def add_training ( self , adapter ): def fn ( engine ): record_these = [ ({ \"engine_output\" : engine . state . output }, {}), ( adapter . optimizers , { \"parent_name\" : \"optimizers\" , \"custom_attr_func\" : optimizer_attr_func , }, ), ({ \"misc\" : adapter . misc }, {}), ( { \"hook\" : adapter . hook }, { \"recursive_types\" : [ BaseHook , BaseWeighter , torch . nn . Module ]}, ), ] for record , kwargs in record_these : self . record_keeper . update_records ( record , engine . state . iteration , ** kwargs ) return fn def add_validation ( self , data , epoch ): self . record_keeper . update_records ( data , epoch ) def write ( self , engine ): self . record_keeper . save_records () self . record_keeper . tensorboard_writer . flush ()","title":"IgniteRecordKeeperLogger"},{"location":"docs/frameworks/lightning/","text":"The following can be imported like this (using Lightning as an example): from pytorch_adapt.frameworks.lightning import Lightning Direct module members \u00b6 Lightning","title":"lightning"},{"location":"docs/frameworks/lightning/#direct-module-members","text":"Lightning","title":"Direct module members"},{"location":"docs/frameworks/lightning/lightning/","text":"Lightning \u00b6 Bases: pl . LightningModule Converts an Adapter into a PyTorch Lightning module. Source code in pytorch_adapt\\frameworks\\lightning\\lightning.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class Lightning ( pl . LightningModule ): \"\"\" Converts an [Adapter](../../adapters/index.md) into a PyTorch Lightning module. \"\"\" def __init__ ( self , adapter , validator = None ): \"\"\" Arguments: adapter: validator: \"\"\" super () . __init__ () self . models = torch . nn . ModuleDict ( adapter . models ) self . misc = torch . nn . ModuleDict ( adapter . misc ) adapter . models = self . models adapter . misc = self . misc self . validator = validator self . adapter = adapter self . automatic_optimization = False def forward ( self , x , domain = None ): \"\"\"\"\"\" return self . adapter . inference ( x , domain = domain ) def training_step ( self , batch , batch_idx ): \"\"\"\"\"\" set_adapter_optimizers_to_pl ( self . adapter , self . optimizers ()) losses = self . adapter . training_step ( batch , custom_backward = self . manual_backward , ) for k , v in losses . items (): self . log ( k , v ) def validation_step ( self , batch , batch_idx , dataloader_idx = 0 ): \"\"\"\"\"\" return f_utils . collector_step ( self , batch , f_utils . create_output_dict ) def validation_epoch_end ( self , outputs ): \"\"\"\"\"\" required_data = self . validator . required_data if len ( required_data ) > 1 : outputs = multi_dataloader_collect ( outputs ) data = { k : v for k , v in zip ( required_data , outputs )} else : outputs = single_dataloader_collect ( outputs ) data = { required_data [ 0 ]: outputs } score = self . validator ( ** data ) self . log ( \"validation_score\" , score ) def configure_optimizers ( self ): \"\"\"\"\"\" optimizers = list ( self . adapter . optimizers . values ()) lr_schedulers = [] for interval in [ \"epoch\" , \"step\" ]: for v in self . adapter . lr_schedulers . filter_by_scheduler_type ( f \"per_ { interval } \" ): lr_schedulers . append ({ \"lr_scheduler\" : v , \"interval\" : interval }) return optimizers , lr_schedulers __init__ ( adapter , validator = None ) \u00b6 Parameters: Name Type Description Default adapter required validator None Source code in pytorch_adapt\\frameworks\\lightning\\lightning.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , adapter , validator = None ): \"\"\" Arguments: adapter: validator: \"\"\" super () . __init__ () self . models = torch . nn . ModuleDict ( adapter . models ) self . misc = torch . nn . ModuleDict ( adapter . misc ) adapter . models = self . models adapter . misc = self . misc self . validator = validator self . adapter = adapter self . automatic_optimization = False","title":"lightning"},{"location":"docs/frameworks/lightning/lightning/#pytorch_adapt.frameworks.lightning.lightning.Lightning","text":"Bases: pl . LightningModule Converts an Adapter into a PyTorch Lightning module. Source code in pytorch_adapt\\frameworks\\lightning\\lightning.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class Lightning ( pl . LightningModule ): \"\"\" Converts an [Adapter](../../adapters/index.md) into a PyTorch Lightning module. \"\"\" def __init__ ( self , adapter , validator = None ): \"\"\" Arguments: adapter: validator: \"\"\" super () . __init__ () self . models = torch . nn . ModuleDict ( adapter . models ) self . misc = torch . nn . ModuleDict ( adapter . misc ) adapter . models = self . models adapter . misc = self . misc self . validator = validator self . adapter = adapter self . automatic_optimization = False def forward ( self , x , domain = None ): \"\"\"\"\"\" return self . adapter . inference ( x , domain = domain ) def training_step ( self , batch , batch_idx ): \"\"\"\"\"\" set_adapter_optimizers_to_pl ( self . adapter , self . optimizers ()) losses = self . adapter . training_step ( batch , custom_backward = self . manual_backward , ) for k , v in losses . items (): self . log ( k , v ) def validation_step ( self , batch , batch_idx , dataloader_idx = 0 ): \"\"\"\"\"\" return f_utils . collector_step ( self , batch , f_utils . create_output_dict ) def validation_epoch_end ( self , outputs ): \"\"\"\"\"\" required_data = self . validator . required_data if len ( required_data ) > 1 : outputs = multi_dataloader_collect ( outputs ) data = { k : v for k , v in zip ( required_data , outputs )} else : outputs = single_dataloader_collect ( outputs ) data = { required_data [ 0 ]: outputs } score = self . validator ( ** data ) self . log ( \"validation_score\" , score ) def configure_optimizers ( self ): \"\"\"\"\"\" optimizers = list ( self . adapter . optimizers . values ()) lr_schedulers = [] for interval in [ \"epoch\" , \"step\" ]: for v in self . adapter . lr_schedulers . filter_by_scheduler_type ( f \"per_ { interval } \" ): lr_schedulers . append ({ \"lr_scheduler\" : v , \"interval\" : interval }) return optimizers , lr_schedulers","title":"Lightning"},{"location":"docs/hooks/","text":"The following can be imported like this (using ADDAHook as an example): from pytorch_adapt.hooks import ADDAHook Direct module members \u00b6 ADDAHook AFNHook ATDOCHook AdaBNHook AlignerHook AlignerPlusCHook ApplyToListHook AssertHook BNMHook BSPHook BaseLossHook CDANDomainHookD CDANDomainHookG CDANEHook CDANHook CDANNEHook CLossHook ChainHook ClassifierHook CombinedFeaturesHook DANNEHook DANNHook DANNLogitsHook DANNSoftmaxLogitsHook DLogitsHook DomainConfusionHook DomainLossHook EmptyHook EntropyReducer FalseHook FeaturesAndLogitsHook FeaturesForDomainLossHook FeaturesHook FeaturesLogitsAlignerHook FeaturesWithGradAndDetachedHook FinetunerHook FrozenModelHook GANEHook GANHook GVBEHook GVBGANHook GVBHook GradientReversalHook GradientReversalLocallyHook ISTLossHook JointAlignerHook LogitsHook MCCHook MCDHook MCDLossHook ManyAlignerHook MeanReducer MultipleCLossHook MultipleReducers ParallelHook RTNAlignerHook RTNHook RTNLogitsHook RepeatHook ResidualHook SoftmaxGradientReversalHook SoftmaxGradientReversalLocallyHook SoftmaxHook StrongDHook SymNetsCHook SymNetsCategoryLossHook SymNetsDomainLossHook SymNetsEntropyHook SymNetsGHook SymNetsHook TargetDiversityHook TargetEntropyHook TrueHook VADAHook VATHook VATPlusEntropyHook validate_hook","title":"hooks"},{"location":"docs/hooks/#direct-module-members","text":"ADDAHook AFNHook ATDOCHook AdaBNHook AlignerHook AlignerPlusCHook ApplyToListHook AssertHook BNMHook BSPHook BaseLossHook CDANDomainHookD CDANDomainHookG CDANEHook CDANHook CDANNEHook CLossHook ChainHook ClassifierHook CombinedFeaturesHook DANNEHook DANNHook DANNLogitsHook DANNSoftmaxLogitsHook DLogitsHook DomainConfusionHook DomainLossHook EmptyHook EntropyReducer FalseHook FeaturesAndLogitsHook FeaturesForDomainLossHook FeaturesHook FeaturesLogitsAlignerHook FeaturesWithGradAndDetachedHook FinetunerHook FrozenModelHook GANEHook GANHook GVBEHook GVBGANHook GVBHook GradientReversalHook GradientReversalLocallyHook ISTLossHook JointAlignerHook LogitsHook MCCHook MCDHook MCDLossHook ManyAlignerHook MeanReducer MultipleCLossHook MultipleReducers ParallelHook RTNAlignerHook RTNHook RTNLogitsHook RepeatHook ResidualHook SoftmaxGradientReversalHook SoftmaxGradientReversalLocallyHook SoftmaxHook StrongDHook SymNetsCHook SymNetsCategoryLossHook SymNetsDomainLossHook SymNetsEntropyHook SymNetsGHook SymNetsHook TargetDiversityHook TargetEntropyHook TrueHook VADAHook VATHook VATPlusEntropyHook validate_hook","title":"Direct module members"},{"location":"docs/hooks/adabn/","text":"AdaBNHook \u00b6 Bases: BaseWrapperHook Passes inputs into model without doing any optimization. The model is expected to receive a domain argument and update its BatchNorm parameters itself. Source code in pytorch_adapt\\hooks\\adabn.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class AdaBNHook ( BaseWrapperHook ): \"\"\" Passes inputs into model without doing any optimization. The model is expected to receive a ```domain``` argument and update its BatchNorm parameters itself. \"\"\" def __init__ ( self , domains = None , ** kwargs ): super () . __init__ ( ** kwargs ) domains = c_f . default ( domains , [ \"src\" , \"target\" ]) hooks = [] for d in domains : f_hook = DomainSpecificFeaturesHook ( domains = [ d ], detach = True ) l_hook = DomainSpecificLogitsHook ( domains = [ d ], detach = True ) hooks . append ( FeaturesChainHook ( f_hook , l_hook )) self . hook = ParallelHook ( * hooks ) def call ( self , inputs , losses ): with torch . no_grad (): return self . hook ( inputs , losses )","title":"adabn"},{"location":"docs/hooks/adabn/#pytorch_adapt.hooks.adabn.AdaBNHook","text":"Bases: BaseWrapperHook Passes inputs into model without doing any optimization. The model is expected to receive a domain argument and update its BatchNorm parameters itself. Source code in pytorch_adapt\\hooks\\adabn.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class AdaBNHook ( BaseWrapperHook ): \"\"\" Passes inputs into model without doing any optimization. The model is expected to receive a ```domain``` argument and update its BatchNorm parameters itself. \"\"\" def __init__ ( self , domains = None , ** kwargs ): super () . __init__ ( ** kwargs ) domains = c_f . default ( domains , [ \"src\" , \"target\" ]) hooks = [] for d in domains : f_hook = DomainSpecificFeaturesHook ( domains = [ d ], detach = True ) l_hook = DomainSpecificLogitsHook ( domains = [ d ], detach = True ) hooks . append ( FeaturesChainHook ( f_hook , l_hook )) self . hook = ParallelHook ( * hooks ) def call ( self , inputs , losses ): with torch . no_grad (): return self . hook ( inputs , losses )","title":"AdaBNHook"},{"location":"docs/hooks/adda/","text":"ADDAHook \u00b6 Bases: GANHook Implementation of Adversarial Discriminative Domain Adaptation . Source code in pytorch_adapt\\hooks\\adda.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class ADDAHook ( GANHook ): \"\"\" Implementation of [Adversarial Discriminative Domain Adaptation](https://arxiv.org/abs/1702.05464). \"\"\" def __init__ ( self , threshold : float = 0.6 , pre_g = None , post_g = None , ** kwargs ): \"\"\" Arguments: threshold: In each training iteration, the generator is only updated if the discriminator's accuracy is greater than ```threshold```. \"\"\" [ pre_g , post_g ] = c_f . many_default ([ pre_g , post_g ], [[], []]) sf_frozen = FrozenModelHook ( FeaturesHook ( detach = True , domains = [ \"src\" ]), \"G\" ) tf_all = FeaturesWithGradAndDetachedHook ( model_name = \"T\" , domains = [ \"target\" ]) pre_d = ChainHook ( sf_frozen , tf_all ) num_pre_g = len ( pre_g ) gen_conditions = [ TrueHook () for _ in range ( num_pre_g + len ( post_g ) + 2 )] # generator condition, classifier condition gen_conditions [ num_pre_g : num_pre_g + 2 ] = [ StrongDHook ( threshold ), FalseHook (), ] super () . __init__ ( pre_d = [ pre_d ], pre_g = pre_g , post_g = post_g , gen_conditions = gen_conditions , gen_domains = [ \"target\" ], c_hook = EmptyHook (), ** kwargs ) __init__ ( threshold = 0.6 , pre_g = None , post_g = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default threshold float In each training iteration, the generator is only updated if the discriminator's accuracy is greater than threshold . 0.6 Source code in pytorch_adapt\\hooks\\adda.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , threshold : float = 0.6 , pre_g = None , post_g = None , ** kwargs ): \"\"\" Arguments: threshold: In each training iteration, the generator is only updated if the discriminator's accuracy is greater than ```threshold```. \"\"\" [ pre_g , post_g ] = c_f . many_default ([ pre_g , post_g ], [[], []]) sf_frozen = FrozenModelHook ( FeaturesHook ( detach = True , domains = [ \"src\" ]), \"G\" ) tf_all = FeaturesWithGradAndDetachedHook ( model_name = \"T\" , domains = [ \"target\" ]) pre_d = ChainHook ( sf_frozen , tf_all ) num_pre_g = len ( pre_g ) gen_conditions = [ TrueHook () for _ in range ( num_pre_g + len ( post_g ) + 2 )] # generator condition, classifier condition gen_conditions [ num_pre_g : num_pre_g + 2 ] = [ StrongDHook ( threshold ), FalseHook (), ] super () . __init__ ( pre_d = [ pre_d ], pre_g = pre_g , post_g = post_g , gen_conditions = gen_conditions , gen_domains = [ \"target\" ], c_hook = EmptyHook (), ** kwargs )","title":"adda"},{"location":"docs/hooks/adda/#pytorch_adapt.hooks.adda.ADDAHook","text":"Bases: GANHook Implementation of Adversarial Discriminative Domain Adaptation . Source code in pytorch_adapt\\hooks\\adda.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class ADDAHook ( GANHook ): \"\"\" Implementation of [Adversarial Discriminative Domain Adaptation](https://arxiv.org/abs/1702.05464). \"\"\" def __init__ ( self , threshold : float = 0.6 , pre_g = None , post_g = None , ** kwargs ): \"\"\" Arguments: threshold: In each training iteration, the generator is only updated if the discriminator's accuracy is greater than ```threshold```. \"\"\" [ pre_g , post_g ] = c_f . many_default ([ pre_g , post_g ], [[], []]) sf_frozen = FrozenModelHook ( FeaturesHook ( detach = True , domains = [ \"src\" ]), \"G\" ) tf_all = FeaturesWithGradAndDetachedHook ( model_name = \"T\" , domains = [ \"target\" ]) pre_d = ChainHook ( sf_frozen , tf_all ) num_pre_g = len ( pre_g ) gen_conditions = [ TrueHook () for _ in range ( num_pre_g + len ( post_g ) + 2 )] # generator condition, classifier condition gen_conditions [ num_pre_g : num_pre_g + 2 ] = [ StrongDHook ( threshold ), FalseHook (), ] super () . __init__ ( pre_d = [ pre_d ], pre_g = pre_g , post_g = post_g , gen_conditions = gen_conditions , gen_domains = [ \"target\" ], c_hook = EmptyHook (), ** kwargs )","title":"ADDAHook"},{"location":"docs/hooks/aligners/","text":"AlignerHook \u00b6 Bases: BaseWrapperHook Computes an alignment loss (e.g MMD) based on features from two domains. Source code in pytorch_adapt\\hooks\\aligners.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class AlignerHook ( BaseWrapperHook ): \"\"\" Computes an alignment loss (e.g MMD) based on features from two domains. \"\"\" def __init__ ( self , loss_fn : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , hook : BaseHook = None , layer : str = \"features\" , ** kwargs , ): \"\"\" Arguments: loss_fn: a function that computes a distance between two tensors. If ```None```, it defaults to [```MMDLoss```][pytorch_adapt.layers.MMDLoss]. hook: the hook for computing features layer: the layer for which the loss is computed. Must be either ```\"features\"``` or ```\"logits\"```. \"\"\" super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , MMDLoss , {}) if layer == \"features\" : default_hook = FeaturesHook elif layer == \"logits\" : default_hook = FeaturesAndLogitsHook else : raise ValueError ( \"AlignerHook layer must be 'features' or 'logits'\" ) self . hook = c_f . default ( hook , default_hook , {}) self . layer = layer def call ( self , inputs , losses ): outputs = self . hook ( inputs , losses )[ 0 ] strs = c_f . filter ( self . hook . out_keys , f \"_ { self . layer } $\" , [ \"^src\" , \"^target\" ]) [ src , target ] = c_f . extract ([ outputs , inputs ], strs ) confusion_loss = self . loss_fn ( src , target ) return outputs , { self . _loss_keys ()[ 0 ]: confusion_loss } def _loss_keys ( self ): return [ f \" { self . layer } _confusion_loss\" ] __init__ ( loss_fn = None , hook = None , layer = 'features' , ** kwargs ) \u00b6 Parameters: Name Type Description Default loss_fn Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] a function that computes a distance between two tensors. If None , it defaults to MMDLoss . None hook BaseHook the hook for computing features None layer str the layer for which the loss is computed. Must be either \"features\" or \"logits\" . 'features' Source code in pytorch_adapt\\hooks\\aligners.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , loss_fn : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , hook : BaseHook = None , layer : str = \"features\" , ** kwargs , ): \"\"\" Arguments: loss_fn: a function that computes a distance between two tensors. If ```None```, it defaults to [```MMDLoss```][pytorch_adapt.layers.MMDLoss]. hook: the hook for computing features layer: the layer for which the loss is computed. Must be either ```\"features\"``` or ```\"logits\"```. \"\"\" super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , MMDLoss , {}) if layer == \"features\" : default_hook = FeaturesHook elif layer == \"logits\" : default_hook = FeaturesAndLogitsHook else : raise ValueError ( \"AlignerHook layer must be 'features' or 'logits'\" ) self . hook = c_f . default ( hook , default_hook , {}) self . layer = layer AlignerPlusCHook \u00b6 Bases: BaseWrapperHook Computes an alignment loss plus a classification loss, and then optimizes the models. Source code in pytorch_adapt\\hooks\\aligners.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 class AlignerPlusCHook ( BaseWrapperHook ): \"\"\" Computes an alignment loss plus a classification loss, and then optimizes the models. \"\"\" def __init__ ( self , opts , weighter = None , reducer = None , loss_fn = None , aligner_hook = None , pre = None , post = None , softmax = True , ** kwargs , ): super () . __init__ ( ** kwargs ) [ pre , post ] = c_f . many_default ([ pre , post ], [[], []]) aligner_hook = ManyAlignerHook ( loss_fn = loss_fn , aligner_hook = aligner_hook , softmax = softmax ) hook = ChainHook ( * pre , aligner_hook , CLossHook (), * post ) hook = OptimizerHook ( hook , opts , weighter , reducer ) s_hook = SummaryHook ({ \"total_loss\" : hook }) self . hook = ChainHook ( hook , s_hook ) FeaturesLogitsAlignerHook \u00b6 Bases: BaseWrapperHook This chains together an AlignerHook for \"features\" followed by an AlignerHook for \"logits\" . Source code in pytorch_adapt\\hooks\\aligners.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 class FeaturesLogitsAlignerHook ( BaseWrapperHook ): \"\"\" This chains together an [```AlignerHook```][pytorch_adapt.hooks.AlignerHook] for ```\"features\"``` followed by an ```AlignerHook``` for ```\"logits\"```. \"\"\" def __init__ ( self , loss_fn : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , ** kwargs , ): \"\"\" Arguments: loss_fn: The loss used by both aligner hooks. \"\"\" super () . __init__ ( ** kwargs ) loss_fn = c_f . default ( loss_fn , MMDLoss , {}) a1_hook = AlignerHook ( loss_fn , layer = \"features\" ) a2_hook = AlignerHook ( loss_fn , layer = \"logits\" ) self . hook = ChainHook ( a1_hook , a2_hook ) __init__ ( loss_fn = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default loss_fn Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] The loss used by both aligner hooks. None Source code in pytorch_adapt\\hooks\\aligners.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def __init__ ( self , loss_fn : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , ** kwargs , ): \"\"\" Arguments: loss_fn: The loss used by both aligner hooks. \"\"\" super () . __init__ ( ** kwargs ) loss_fn = c_f . default ( loss_fn , MMDLoss , {}) a1_hook = AlignerHook ( loss_fn , layer = \"features\" ) a2_hook = AlignerHook ( loss_fn , layer = \"logits\" ) self . hook = ChainHook ( a1_hook , a2_hook ) JointAlignerHook \u00b6 Bases: BaseWrapperHook Computes a joint alignment loss (e.g Joint MMD) based on multiple features from two domains. The default setting is to use the features and logits from the source and target domains. Source code in pytorch_adapt\\hooks\\aligners.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class JointAlignerHook ( BaseWrapperHook ): \"\"\" Computes a joint alignment loss (e.g Joint MMD) based on multiple features from two domains. The default setting is to use the features and logits from the source and target domains. \"\"\" def __init__ ( self , loss_fn : Callable [ [ List [ torch . Tensor ], List [ torch . Tensor ]], torch . Tensor ] = None , hook : BaseHook = None , ** kwargs , ): \"\"\" Arguments: loss_fn: a function that computes a distance between two **lists** of tensors. If ```None```, it defaults to [```MMDLoss```][pytorch_adapt.layers.MMDLoss]. hook: the hook for computing features and logits \"\"\" super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , MMDLoss , {}) self . hook = c_f . default ( hook , FeaturesAndLogitsHook , {}) def call ( self , inputs , losses ): outputs = self . hook ( inputs , losses )[ 0 ] src = self . get_all_domain_features ( inputs , outputs , \"src\" ) target = self . get_all_domain_features ( inputs , outputs , \"target\" ) confusion_loss = self . loss_fn ( src , target ) return outputs , { self . _loss_keys ()[ 0 ]: confusion_loss } def _loss_keys ( self ): return [ \"joint_confusion_loss\" ] def get_all_domain_features ( self , inputs , outputs , domain ): return c_f . extract ( [ outputs , inputs ], sorted ( c_f . filter ( self . hook . out_keys , f \"^ { domain } \" )) ) __init__ ( loss_fn = None , hook = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default loss_fn Callable [[ List [ torch . Tensor ], List [ torch . Tensor ]], torch . Tensor ] a function that computes a distance between two lists of tensors. If None , it defaults to MMDLoss . None hook BaseHook the hook for computing features and logits None Source code in pytorch_adapt\\hooks\\aligners.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , loss_fn : Callable [ [ List [ torch . Tensor ], List [ torch . Tensor ]], torch . Tensor ] = None , hook : BaseHook = None , ** kwargs , ): \"\"\" Arguments: loss_fn: a function that computes a distance between two **lists** of tensors. If ```None```, it defaults to [```MMDLoss```][pytorch_adapt.layers.MMDLoss]. hook: the hook for computing features and logits \"\"\" super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , MMDLoss , {}) self . hook = c_f . default ( hook , FeaturesAndLogitsHook , {})","title":"aligners"},{"location":"docs/hooks/aligners/#pytorch_adapt.hooks.aligners.AlignerHook","text":"Bases: BaseWrapperHook Computes an alignment loss (e.g MMD) based on features from two domains. Source code in pytorch_adapt\\hooks\\aligners.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class AlignerHook ( BaseWrapperHook ): \"\"\" Computes an alignment loss (e.g MMD) based on features from two domains. \"\"\" def __init__ ( self , loss_fn : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , hook : BaseHook = None , layer : str = \"features\" , ** kwargs , ): \"\"\" Arguments: loss_fn: a function that computes a distance between two tensors. If ```None```, it defaults to [```MMDLoss```][pytorch_adapt.layers.MMDLoss]. hook: the hook for computing features layer: the layer for which the loss is computed. Must be either ```\"features\"``` or ```\"logits\"```. \"\"\" super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , MMDLoss , {}) if layer == \"features\" : default_hook = FeaturesHook elif layer == \"logits\" : default_hook = FeaturesAndLogitsHook else : raise ValueError ( \"AlignerHook layer must be 'features' or 'logits'\" ) self . hook = c_f . default ( hook , default_hook , {}) self . layer = layer def call ( self , inputs , losses ): outputs = self . hook ( inputs , losses )[ 0 ] strs = c_f . filter ( self . hook . out_keys , f \"_ { self . layer } $\" , [ \"^src\" , \"^target\" ]) [ src , target ] = c_f . extract ([ outputs , inputs ], strs ) confusion_loss = self . loss_fn ( src , target ) return outputs , { self . _loss_keys ()[ 0 ]: confusion_loss } def _loss_keys ( self ): return [ f \" { self . layer } _confusion_loss\" ]","title":"AlignerHook"},{"location":"docs/hooks/aligners/#pytorch_adapt.hooks.aligners.AlignerPlusCHook","text":"Bases: BaseWrapperHook Computes an alignment loss plus a classification loss, and then optimizes the models. Source code in pytorch_adapt\\hooks\\aligners.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 class AlignerPlusCHook ( BaseWrapperHook ): \"\"\" Computes an alignment loss plus a classification loss, and then optimizes the models. \"\"\" def __init__ ( self , opts , weighter = None , reducer = None , loss_fn = None , aligner_hook = None , pre = None , post = None , softmax = True , ** kwargs , ): super () . __init__ ( ** kwargs ) [ pre , post ] = c_f . many_default ([ pre , post ], [[], []]) aligner_hook = ManyAlignerHook ( loss_fn = loss_fn , aligner_hook = aligner_hook , softmax = softmax ) hook = ChainHook ( * pre , aligner_hook , CLossHook (), * post ) hook = OptimizerHook ( hook , opts , weighter , reducer ) s_hook = SummaryHook ({ \"total_loss\" : hook }) self . hook = ChainHook ( hook , s_hook )","title":"AlignerPlusCHook"},{"location":"docs/hooks/aligners/#pytorch_adapt.hooks.aligners.FeaturesLogitsAlignerHook","text":"Bases: BaseWrapperHook This chains together an AlignerHook for \"features\" followed by an AlignerHook for \"logits\" . Source code in pytorch_adapt\\hooks\\aligners.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 class FeaturesLogitsAlignerHook ( BaseWrapperHook ): \"\"\" This chains together an [```AlignerHook```][pytorch_adapt.hooks.AlignerHook] for ```\"features\"``` followed by an ```AlignerHook``` for ```\"logits\"```. \"\"\" def __init__ ( self , loss_fn : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , ** kwargs , ): \"\"\" Arguments: loss_fn: The loss used by both aligner hooks. \"\"\" super () . __init__ ( ** kwargs ) loss_fn = c_f . default ( loss_fn , MMDLoss , {}) a1_hook = AlignerHook ( loss_fn , layer = \"features\" ) a2_hook = AlignerHook ( loss_fn , layer = \"logits\" ) self . hook = ChainHook ( a1_hook , a2_hook )","title":"FeaturesLogitsAlignerHook"},{"location":"docs/hooks/aligners/#pytorch_adapt.hooks.aligners.JointAlignerHook","text":"Bases: BaseWrapperHook Computes a joint alignment loss (e.g Joint MMD) based on multiple features from two domains. The default setting is to use the features and logits from the source and target domains. Source code in pytorch_adapt\\hooks\\aligners.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class JointAlignerHook ( BaseWrapperHook ): \"\"\" Computes a joint alignment loss (e.g Joint MMD) based on multiple features from two domains. The default setting is to use the features and logits from the source and target domains. \"\"\" def __init__ ( self , loss_fn : Callable [ [ List [ torch . Tensor ], List [ torch . Tensor ]], torch . Tensor ] = None , hook : BaseHook = None , ** kwargs , ): \"\"\" Arguments: loss_fn: a function that computes a distance between two **lists** of tensors. If ```None```, it defaults to [```MMDLoss```][pytorch_adapt.layers.MMDLoss]. hook: the hook for computing features and logits \"\"\" super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , MMDLoss , {}) self . hook = c_f . default ( hook , FeaturesAndLogitsHook , {}) def call ( self , inputs , losses ): outputs = self . hook ( inputs , losses )[ 0 ] src = self . get_all_domain_features ( inputs , outputs , \"src\" ) target = self . get_all_domain_features ( inputs , outputs , \"target\" ) confusion_loss = self . loss_fn ( src , target ) return outputs , { self . _loss_keys ()[ 0 ]: confusion_loss } def _loss_keys ( self ): return [ \"joint_confusion_loss\" ] def get_all_domain_features ( self , inputs , outputs , domain ): return c_f . extract ( [ outputs , inputs ], sorted ( c_f . filter ( self . hook . out_keys , f \"^ { domain } \" )) )","title":"JointAlignerHook"},{"location":"docs/hooks/atdoc/","text":"ATDOCHook \u00b6 Bases: BaseHook Creates pseudo labels for the target domain using k-nearest neighbors. Then computes a classification loss based on these pseudo labels. Implementation of Domain Adaptation with Auxiliary Target Domain-Oriented Classifier . Source code in pytorch_adapt\\hooks\\atdoc.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class ATDOCHook ( BaseHook ): \"\"\" Creates pseudo labels for the target domain using k-nearest neighbors. Then computes a classification loss based on these pseudo labels. Implementation of [Domain Adaptation with Auxiliary Target Domain-Oriented Classifier](https://arxiv.org/abs/2007.04171). \"\"\" def __init__ ( self , dataset_size , feature_dim , num_classes , k = 5 , loss_fn = None , ** kwargs ): \"\"\" Arguments: dataset_size: The number of samples in the target dataset. feature_dim: The feature dimensionality, i.e at each iteration the features should be size ```(N, D)``` where N is batch size and D is ```feature_dim```. num_classes: The number of class labels in the target dataset. k: The number of nearest neighbors used to determine each sample's pseudolabel loss_fn: The classification loss function. If ```None``` it defaults to ```torch.nn.CrossEntropyLoss```. \"\"\" super () . __init__ ( ** kwargs ) self . labeler = NeighborhoodAggregation ( dataset_size , feature_dim , num_classes , k = k ) self . weighter = ConfidenceWeights () self . loss_fn = c_f . default ( loss_fn , torch . nn . CrossEntropyLoss , { \"reduction\" : \"none\" } ) self . hook = FeaturesAndLogitsHook ( domains = [ \"target\" ]) def call ( self , inputs , losses ) -> Tuple [ Dict [ str , Any ], Dict [ str , Any ]]: outputs = self . hook ( inputs , losses )[ 0 ] [ features , logits ] = c_f . extract ( [ outputs , inputs ], c_f . filter ( self . hook . out_keys , \"\" , [ \"_features$\" , \"_logits$\" ]), ) pseudo_labels , neighbor_preds = self . labeler ( features , logits , update = True , idx = inputs [ \"target_sample_idx\" ] ) loss = self . loss_fn ( logits , pseudo_labels ) weights = self . weighter ( neighbor_preds ) loss = torch . mean ( weights * loss ) return outputs , { \"pseudo_label_loss\" : loss } def _loss_keys ( self ): \"\"\"\"\"\" return [ \"pseudo_label_loss\" ] def _out_keys ( self ): \"\"\"\"\"\" return self . hook . out_keys __init__ ( dataset_size , feature_dim , num_classes , k = 5 , loss_fn = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default dataset_size The number of samples in the target dataset. required feature_dim The feature dimensionality, i.e at each iteration the features should be size (N, D) where N is batch size and D is feature_dim . required num_classes The number of class labels in the target dataset. required k The number of nearest neighbors used to determine each sample's pseudolabel 5 loss_fn The classification loss function. If None it defaults to torch.nn.CrossEntropyLoss . None Source code in pytorch_adapt\\hooks\\atdoc.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , dataset_size , feature_dim , num_classes , k = 5 , loss_fn = None , ** kwargs ): \"\"\" Arguments: dataset_size: The number of samples in the target dataset. feature_dim: The feature dimensionality, i.e at each iteration the features should be size ```(N, D)``` where N is batch size and D is ```feature_dim```. num_classes: The number of class labels in the target dataset. k: The number of nearest neighbors used to determine each sample's pseudolabel loss_fn: The classification loss function. If ```None``` it defaults to ```torch.nn.CrossEntropyLoss```. \"\"\" super () . __init__ ( ** kwargs ) self . labeler = NeighborhoodAggregation ( dataset_size , feature_dim , num_classes , k = k ) self . weighter = ConfidenceWeights () self . loss_fn = c_f . default ( loss_fn , torch . nn . CrossEntropyLoss , { \"reduction\" : \"none\" } ) self . hook = FeaturesAndLogitsHook ( domains = [ \"target\" ])","title":"atdoc"},{"location":"docs/hooks/atdoc/#pytorch_adapt.hooks.atdoc.ATDOCHook","text":"Bases: BaseHook Creates pseudo labels for the target domain using k-nearest neighbors. Then computes a classification loss based on these pseudo labels. Implementation of Domain Adaptation with Auxiliary Target Domain-Oriented Classifier . Source code in pytorch_adapt\\hooks\\atdoc.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class ATDOCHook ( BaseHook ): \"\"\" Creates pseudo labels for the target domain using k-nearest neighbors. Then computes a classification loss based on these pseudo labels. Implementation of [Domain Adaptation with Auxiliary Target Domain-Oriented Classifier](https://arxiv.org/abs/2007.04171). \"\"\" def __init__ ( self , dataset_size , feature_dim , num_classes , k = 5 , loss_fn = None , ** kwargs ): \"\"\" Arguments: dataset_size: The number of samples in the target dataset. feature_dim: The feature dimensionality, i.e at each iteration the features should be size ```(N, D)``` where N is batch size and D is ```feature_dim```. num_classes: The number of class labels in the target dataset. k: The number of nearest neighbors used to determine each sample's pseudolabel loss_fn: The classification loss function. If ```None``` it defaults to ```torch.nn.CrossEntropyLoss```. \"\"\" super () . __init__ ( ** kwargs ) self . labeler = NeighborhoodAggregation ( dataset_size , feature_dim , num_classes , k = k ) self . weighter = ConfidenceWeights () self . loss_fn = c_f . default ( loss_fn , torch . nn . CrossEntropyLoss , { \"reduction\" : \"none\" } ) self . hook = FeaturesAndLogitsHook ( domains = [ \"target\" ]) def call ( self , inputs , losses ) -> Tuple [ Dict [ str , Any ], Dict [ str , Any ]]: outputs = self . hook ( inputs , losses )[ 0 ] [ features , logits ] = c_f . extract ( [ outputs , inputs ], c_f . filter ( self . hook . out_keys , \"\" , [ \"_features$\" , \"_logits$\" ]), ) pseudo_labels , neighbor_preds = self . labeler ( features , logits , update = True , idx = inputs [ \"target_sample_idx\" ] ) loss = self . loss_fn ( logits , pseudo_labels ) weights = self . weighter ( neighbor_preds ) loss = torch . mean ( weights * loss ) return outputs , { \"pseudo_label_loss\" : loss } def _loss_keys ( self ): \"\"\"\"\"\" return [ \"pseudo_label_loss\" ] def _out_keys ( self ): \"\"\"\"\"\" return self . hook . out_keys","title":"ATDOCHook"},{"location":"docs/hooks/base/","text":"BaseConditionHook \u00b6 Bases: BaseHook The base class for hooks that return a boolean Source code in pytorch_adapt\\hooks\\base.py 176 177 178 179 180 181 182 183 184 185 class BaseConditionHook ( BaseHook ): \"\"\"The base class for hooks that return a boolean\"\"\" def _loss_keys ( self ): \"\"\"\"\"\" return [] def _out_keys ( self ): \"\"\"\"\"\" return [] BaseHook \u00b6 Bases: ABC All hooks extend BaseHook Source code in pytorch_adapt\\hooks\\base.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class BaseHook ( ABC ): \"\"\"All hooks extend ```BaseHook```\"\"\" def __init__ ( self , loss_prefix : str = \"\" , loss_suffix : str = \"\" , out_prefix : str = \"\" , out_suffix : str = \"\" , key_map : Dict [ str , str ] = None , ): \"\"\" Arguments: loss_prefix: prepended to all new loss keys loss_suffix: appended to all new loss keys out_prefix: prepended to all new output keys out_suffix: appended to all new output keys key_map: a mapping from ```input_key``` to ```new_key```. For example, if key_map = {\"A\": \"B\"}, and the input dict to ```__call__``` is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting ```__call__```, the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. \"\"\" if any ( not isinstance ( x , str ) for x in [ loss_prefix , loss_suffix , out_prefix , out_suffix ] ): raise TypeError ( \"loss prefix/suffix and out prefix/suffix must be strings\" ) self . loss_prefix = loss_prefix self . loss_suffix = loss_suffix self . out_prefix = out_prefix self . out_suffix = out_suffix self . key_map = c_f . default ( key_map , {}) self . in_keys = [] self . logger = HookLogger ( c_f . cls_name ( self )) def __call__ ( self , inputs , losses = None ): self . logger ( \"__call__\" ) losses = c_f . default ( losses , {}) try : inputs = c_f . map_keys ( inputs , self . key_map ) x = self . call ( inputs , losses ) if isinstance ( x , ( bool , np . bool_ )): self . logger . reset () return x elif isinstance ( x , tuple ): outputs , losses = x outputs = replace_mapped_keys ( outputs , self . key_map ) inputs = replace_mapped_keys ( inputs , self . key_map ) outputs = wrap_keys ( outputs , self . out_prefix , self . out_suffix ) losses = wrap_keys ( losses , self . loss_prefix , self . loss_suffix ) self . check_losses_and_outputs ( outputs , losses , inputs ) self . logger . reset () return outputs , losses else : raise TypeError ( f \"Output is of type { type ( x ) } , but should be bool or tuple\" ) except Exception as e : c_f . add_error_message ( e , f \"in { self . logger . str } \\n \" , prepend = True ) self . logger . reset () raise @abstractmethod def call ( self , inputs : Dict [ str , Any ], losses : Dict [ str , Any ] ) -> Union [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]], bool ]: \"\"\" This gets called by ```__call__``` and must be implemented by the child class. Arguments: inputs: holds data and models losses: previously computed losses Returns: Either a tuple of ```(outputs, losses)``` that will be merged with the input context, or a boolean \"\"\" pass @abstractmethod def _loss_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the losses that will be added to the context. \"\"\" pass @property def loss_keys ( self ): return list ( set ( wrap_keys ( self . _loss_keys (), self . loss_prefix , self . loss_suffix )) ) @abstractmethod def _out_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the outputs that will be added to the context. \"\"\" pass @property def out_keys ( self ): x = replace_mapped_keys ( self . _out_keys (), self . key_map ) return list ( set ( wrap_keys ( x , self . out_prefix , self . out_suffix ))) def set_in_keys ( self , in_keys ): self . in_keys = in_keys def __repr__ ( self ): return c_f . nice_repr ( self , self . extra_repr (), self . children_repr ()) def extra_repr ( self ): return \"\" def children_repr ( self ): all_hooks = c_f . attrs_of_type ( self , BaseHook ) all_modules = c_f . attrs_of_type ( self , torch . nn . Module ) return c_f . assert_dicts_are_disjoint ( all_hooks , all_modules ) def check_losses_and_outputs ( self , outputs , losses , inputs ): check_keys_are_present ( self , self . loss_keys , [ losses ], \"loss_keys\" , \"losses\" ) check_keys_are_present ( self , self . out_keys , [ inputs , outputs ], \"out_keys\" , \"inputs or outputs\" ) check_keys_are_present ( self , losses , self . loss_keys , \"loss_keys\" , \"losses\" ) check_keys_are_present ( self , outputs , self . out_keys , \"outputs\" , \"out_keys\" ) __init__ ( loss_prefix = '' , loss_suffix = '' , out_prefix = '' , out_suffix = '' , key_map = None ) \u00b6 Parameters: Name Type Description Default loss_prefix str prepended to all new loss keys '' loss_suffix str appended to all new loss keys '' out_prefix str prepended to all new output keys '' out_suffix str appended to all new output keys '' key_map Dict [ str , str ] a mapping from input_key to new_key . For example, if key_map = {\"A\": \"B\"}, and the input dict to __call__ is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting __call__ , the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. None Source code in pytorch_adapt\\hooks\\base.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , loss_prefix : str = \"\" , loss_suffix : str = \"\" , out_prefix : str = \"\" , out_suffix : str = \"\" , key_map : Dict [ str , str ] = None , ): \"\"\" Arguments: loss_prefix: prepended to all new loss keys loss_suffix: appended to all new loss keys out_prefix: prepended to all new output keys out_suffix: appended to all new output keys key_map: a mapping from ```input_key``` to ```new_key```. For example, if key_map = {\"A\": \"B\"}, and the input dict to ```__call__``` is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting ```__call__```, the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. \"\"\" if any ( not isinstance ( x , str ) for x in [ loss_prefix , loss_suffix , out_prefix , out_suffix ] ): raise TypeError ( \"loss prefix/suffix and out prefix/suffix must be strings\" ) self . loss_prefix = loss_prefix self . loss_suffix = loss_suffix self . out_prefix = out_prefix self . out_suffix = out_suffix self . key_map = c_f . default ( key_map , {}) self . in_keys = [] self . logger = HookLogger ( c_f . cls_name ( self )) call ( inputs , losses ) abstractmethod \u00b6 This gets called by __call__ and must be implemented by the child class. Parameters: Name Type Description Default inputs Dict [ str , Any ] holds data and models required losses Dict [ str , Any ] previously computed losses required Returns: Type Description Union [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]], bool ] Either a tuple of (outputs, losses) that will be merged with the input context, Union [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]], bool ] or a boolean Source code in pytorch_adapt\\hooks\\base.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @abstractmethod def call ( self , inputs : Dict [ str , Any ], losses : Dict [ str , Any ] ) -> Union [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]], bool ]: \"\"\" This gets called by ```__call__``` and must be implemented by the child class. Arguments: inputs: holds data and models losses: previously computed losses Returns: Either a tuple of ```(outputs, losses)``` that will be merged with the input context, or a boolean \"\"\" pass BaseWrapperHook \u00b6 Bases: BaseHook A simple wrapper for calling self.hook , which should be defined in the child's __init__ function. Source code in pytorch_adapt\\hooks\\base.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 class BaseWrapperHook ( BaseHook ): \"\"\"A simple wrapper for calling ```self.hook```, which should be defined in the child's ```__init__``` function.\"\"\" def call ( self , * args , ** kwargs ): \"\"\"\"\"\" return self . hook ( * args , ** kwargs ) def _loss_keys ( self ): \"\"\"\"\"\" return self . hook . loss_keys def _out_keys ( self ): \"\"\"\"\"\" return self . hook . out_keys","title":"base"},{"location":"docs/hooks/base/#pytorch_adapt.hooks.base.BaseConditionHook","text":"Bases: BaseHook The base class for hooks that return a boolean Source code in pytorch_adapt\\hooks\\base.py 176 177 178 179 180 181 182 183 184 185 class BaseConditionHook ( BaseHook ): \"\"\"The base class for hooks that return a boolean\"\"\" def _loss_keys ( self ): \"\"\"\"\"\" return [] def _out_keys ( self ): \"\"\"\"\"\" return []","title":"BaseConditionHook"},{"location":"docs/hooks/base/#pytorch_adapt.hooks.base.BaseHook","text":"Bases: ABC All hooks extend BaseHook Source code in pytorch_adapt\\hooks\\base.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class BaseHook ( ABC ): \"\"\"All hooks extend ```BaseHook```\"\"\" def __init__ ( self , loss_prefix : str = \"\" , loss_suffix : str = \"\" , out_prefix : str = \"\" , out_suffix : str = \"\" , key_map : Dict [ str , str ] = None , ): \"\"\" Arguments: loss_prefix: prepended to all new loss keys loss_suffix: appended to all new loss keys out_prefix: prepended to all new output keys out_suffix: appended to all new output keys key_map: a mapping from ```input_key``` to ```new_key```. For example, if key_map = {\"A\": \"B\"}, and the input dict to ```__call__``` is {\"A\": 5}, then the input will be converted to {\"B\": 5} before being consumed. Before exiting ```__call__```, the mapping is undone so the input context is preserved. In other words, {\"B\": 5} will be converted back to {\"A\": 5}. \"\"\" if any ( not isinstance ( x , str ) for x in [ loss_prefix , loss_suffix , out_prefix , out_suffix ] ): raise TypeError ( \"loss prefix/suffix and out prefix/suffix must be strings\" ) self . loss_prefix = loss_prefix self . loss_suffix = loss_suffix self . out_prefix = out_prefix self . out_suffix = out_suffix self . key_map = c_f . default ( key_map , {}) self . in_keys = [] self . logger = HookLogger ( c_f . cls_name ( self )) def __call__ ( self , inputs , losses = None ): self . logger ( \"__call__\" ) losses = c_f . default ( losses , {}) try : inputs = c_f . map_keys ( inputs , self . key_map ) x = self . call ( inputs , losses ) if isinstance ( x , ( bool , np . bool_ )): self . logger . reset () return x elif isinstance ( x , tuple ): outputs , losses = x outputs = replace_mapped_keys ( outputs , self . key_map ) inputs = replace_mapped_keys ( inputs , self . key_map ) outputs = wrap_keys ( outputs , self . out_prefix , self . out_suffix ) losses = wrap_keys ( losses , self . loss_prefix , self . loss_suffix ) self . check_losses_and_outputs ( outputs , losses , inputs ) self . logger . reset () return outputs , losses else : raise TypeError ( f \"Output is of type { type ( x ) } , but should be bool or tuple\" ) except Exception as e : c_f . add_error_message ( e , f \"in { self . logger . str } \\n \" , prepend = True ) self . logger . reset () raise @abstractmethod def call ( self , inputs : Dict [ str , Any ], losses : Dict [ str , Any ] ) -> Union [ Tuple [ Dict [ str , Any ], Dict [ str , Any ]], bool ]: \"\"\" This gets called by ```__call__``` and must be implemented by the child class. Arguments: inputs: holds data and models losses: previously computed losses Returns: Either a tuple of ```(outputs, losses)``` that will be merged with the input context, or a boolean \"\"\" pass @abstractmethod def _loss_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the losses that will be added to the context. \"\"\" pass @property def loss_keys ( self ): return list ( set ( wrap_keys ( self . _loss_keys (), self . loss_prefix , self . loss_suffix )) ) @abstractmethod def _out_keys ( self ) -> List [ str ]: \"\"\" This must be implemented by the child class Returns: The names of the outputs that will be added to the context. \"\"\" pass @property def out_keys ( self ): x = replace_mapped_keys ( self . _out_keys (), self . key_map ) return list ( set ( wrap_keys ( x , self . out_prefix , self . out_suffix ))) def set_in_keys ( self , in_keys ): self . in_keys = in_keys def __repr__ ( self ): return c_f . nice_repr ( self , self . extra_repr (), self . children_repr ()) def extra_repr ( self ): return \"\" def children_repr ( self ): all_hooks = c_f . attrs_of_type ( self , BaseHook ) all_modules = c_f . attrs_of_type ( self , torch . nn . Module ) return c_f . assert_dicts_are_disjoint ( all_hooks , all_modules ) def check_losses_and_outputs ( self , outputs , losses , inputs ): check_keys_are_present ( self , self . loss_keys , [ losses ], \"loss_keys\" , \"losses\" ) check_keys_are_present ( self , self . out_keys , [ inputs , outputs ], \"out_keys\" , \"inputs or outputs\" ) check_keys_are_present ( self , losses , self . loss_keys , \"loss_keys\" , \"losses\" ) check_keys_are_present ( self , outputs , self . out_keys , \"outputs\" , \"out_keys\" )","title":"BaseHook"},{"location":"docs/hooks/base/#pytorch_adapt.hooks.base.BaseWrapperHook","text":"Bases: BaseHook A simple wrapper for calling self.hook , which should be defined in the child's __init__ function. Source code in pytorch_adapt\\hooks\\base.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 class BaseWrapperHook ( BaseHook ): \"\"\"A simple wrapper for calling ```self.hook```, which should be defined in the child's ```__init__``` function.\"\"\" def call ( self , * args , ** kwargs ): \"\"\"\"\"\" return self . hook ( * args , ** kwargs ) def _loss_keys ( self ): \"\"\"\"\"\" return self . hook . loss_keys def _out_keys ( self ): \"\"\"\"\"\" return self . hook . out_keys","title":"BaseWrapperHook"},{"location":"docs/hooks/cdan/","text":"CDANHook \u00b6 Bases: GANHook Implementation of Conditional Adversarial Domain Adaptation Source code in pytorch_adapt\\hooks\\cdan.py 77 78 79 80 81 82 83 84 85 86 87 88 class CDANHook ( GANHook ): \"\"\" Implementation of [Conditional Adversarial Domain Adaptation](https://arxiv.org/abs/1705.10667) \"\"\" def __init__ ( self , softmax = True , ** kwargs ): super () . __init__ ( disc_hook = CDANDomainHookD ( softmax = softmax ), gen_hook = CDANDomainHookG ( softmax = softmax ), ** kwargs )","title":"cdan"},{"location":"docs/hooks/cdan/#pytorch_adapt.hooks.cdan.CDANHook","text":"Bases: GANHook Implementation of Conditional Adversarial Domain Adaptation Source code in pytorch_adapt\\hooks\\cdan.py 77 78 79 80 81 82 83 84 85 86 87 88 class CDANHook ( GANHook ): \"\"\" Implementation of [Conditional Adversarial Domain Adaptation](https://arxiv.org/abs/1705.10667) \"\"\" def __init__ ( self , softmax = True , ** kwargs ): super () . __init__ ( disc_hook = CDANDomainHookD ( softmax = softmax ), gen_hook = CDANDomainHookG ( softmax = softmax ), ** kwargs )","title":"CDANHook"},{"location":"docs/hooks/classification/","text":"CLossHook \u00b6 Bases: BaseWrapperHook Computes a classification loss on the specified tensors. The default setting is to compute the cross entropy loss of the source domain logits. Source code in pytorch_adapt\\hooks\\classification.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class CLossHook ( BaseWrapperHook ): \"\"\" Computes a classification loss on the specified tensors. The default setting is to compute the cross entropy loss of the source domain logits. \"\"\" def __init__ ( self , loss_fn : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , detach_features : bool = False , f_hook : BaseHook = None , ** kwargs , ): \"\"\" Arguments: loss_fn: The classification loss function. If ```None```, it defaults to ```torch.nn.CrossEntropyLoss```. detach_features: Whether or not to detach the features, from which logits are computed. f_hook: The hook for computing logits. \"\"\" super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , torch . nn . CrossEntropyLoss , { \"reduction\" : \"none\" } ) self . hook = c_f . default ( f_hook , FeaturesAndLogitsHook , { \"domains\" : [ \"src\" ], \"detach_features\" : detach_features }, ) def call ( self , inputs , losses ): \"\"\"\"\"\" outputs = self . hook ( inputs , losses )[ 0 ] [ src_logits ] = c_f . extract ( [ outputs , inputs ], c_f . filter ( self . hook . out_keys , \"_logits$\" ) ) loss = self . loss_fn ( src_logits , inputs [ \"src_labels\" ]) return outputs , { self . _loss_keys ()[ 0 ]: loss } def _loss_keys ( self ): \"\"\"\"\"\" return [ \"c_loss\" ] __init__ ( loss_fn = None , detach_features = False , f_hook = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default loss_fn Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] The classification loss function. If None , it defaults to torch.nn.CrossEntropyLoss . None detach_features bool Whether or not to detach the features, from which logits are computed. False f_hook BaseHook The hook for computing logits. None Source code in pytorch_adapt\\hooks\\classification.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , loss_fn : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , detach_features : bool = False , f_hook : BaseHook = None , ** kwargs , ): \"\"\" Arguments: loss_fn: The classification loss function. If ```None```, it defaults to ```torch.nn.CrossEntropyLoss```. detach_features: Whether or not to detach the features, from which logits are computed. f_hook: The hook for computing logits. \"\"\" super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , torch . nn . CrossEntropyLoss , { \"reduction\" : \"none\" } ) self . hook = c_f . default ( f_hook , FeaturesAndLogitsHook , { \"domains\" : [ \"src\" ], \"detach_features\" : detach_features }, ) ClassifierHook \u00b6 Bases: BaseWrapperHook This computes the classification loss and also optimizes the models. Source code in pytorch_adapt\\hooks\\classification.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class ClassifierHook ( BaseWrapperHook ): \"\"\" This computes the classification loss and also optimizes the models. \"\"\" def __init__ ( self , opts , weighter = None , reducer = None , loss_fn = None , f_hook = None , detach_features = False , pre = None , post = None , ** kwargs , ): super () . __init__ ( ** kwargs ) [ pre , post ] = c_f . many_default ([ pre , post ], [[], []]) hook = CLossHook ( loss_fn , detach_features , f_hook ) hook = ChainHook ( * pre , hook , * post ) hook = OptimizerHook ( hook , opts , weighter , reducer ) s_hook = SummaryHook ({ \"total_loss\" : hook }) self . hook = ChainHook ( hook , s_hook ) FinetunerHook \u00b6 Bases: ClassifierHook This is the same as ClassifierHook , but it freezes the generator model (\"G\"). Source code in pytorch_adapt\\hooks\\classification.py 122 123 124 125 126 127 128 129 130 131 132 class FinetunerHook ( ClassifierHook ): \"\"\" This is the same as [```ClassifierHook```][pytorch_adapt.hooks.ClassifierHook], but it freezes the generator model (\"G\"). \"\"\" def __init__ ( self , ** kwargs ): f_hook = FrozenModelHook ( FeaturesHook ( detach = True , domains = [ \"src\" ]), \"G\" ) f_hook = FeaturesChainHook ( f_hook , LogitsHook ( domains = [ \"src\" ])) super () . __init__ ( f_hook = f_hook , ** kwargs ) SoftmaxHook \u00b6 Bases: ApplyFnHook Applies torch.nn.Softmax(dim=1) to the specified inputs. Source code in pytorch_adapt\\hooks\\classification.py 18 19 20 21 22 23 24 25 class SoftmaxHook ( ApplyFnHook ): \"\"\" Applies ```torch.nn.Softmax(dim=1)``` to the specified inputs. \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( fn = torch . nn . Softmax ( dim = 1 ), ** kwargs ) SoftmaxLocallyHook \u00b6 Bases: BaseWrapperHook Applies torch.nn.Softmax(dim=1) to the specified inputs, which are overwritten, but only inside this hook. Source code in pytorch_adapt\\hooks\\classification.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class SoftmaxLocallyHook ( BaseWrapperHook ): \"\"\" Applies ```torch.nn.Softmax(dim=1)``` to the specified inputs, which are overwritten, but only inside this hook. \"\"\" def __init__ ( self , apply_to : List [ str ], * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: apply_to: list of names of tensors that softmax will be applied to. *hooks: the hooks that will receive the softmaxed tensors. \"\"\" super () . __init__ ( ** kwargs ) s_hook = SoftmaxHook ( apply_to = apply_to ) self . hook = OnlyNewOutputsHook ( ChainHook ( s_hook , * hooks , overwrite = True )) __init__ ( apply_to , * hooks , ** kwargs ) \u00b6 Parameters: Name Type Description Default apply_to List [ str ] list of names of tensors that softmax will be applied to. required *hooks BaseHook the hooks that will receive the softmaxed tensors. () Source code in pytorch_adapt\\hooks\\classification.py 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , apply_to : List [ str ], * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: apply_to: list of names of tensors that softmax will be applied to. *hooks: the hooks that will receive the softmaxed tensors. \"\"\" super () . __init__ ( ** kwargs ) s_hook = SoftmaxHook ( apply_to = apply_to ) self . hook = OnlyNewOutputsHook ( ChainHook ( s_hook , * hooks , overwrite = True ))","title":"classification"},{"location":"docs/hooks/classification/#pytorch_adapt.hooks.classification.CLossHook","text":"Bases: BaseWrapperHook Computes a classification loss on the specified tensors. The default setting is to compute the cross entropy loss of the source domain logits. Source code in pytorch_adapt\\hooks\\classification.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class CLossHook ( BaseWrapperHook ): \"\"\" Computes a classification loss on the specified tensors. The default setting is to compute the cross entropy loss of the source domain logits. \"\"\" def __init__ ( self , loss_fn : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , detach_features : bool = False , f_hook : BaseHook = None , ** kwargs , ): \"\"\" Arguments: loss_fn: The classification loss function. If ```None```, it defaults to ```torch.nn.CrossEntropyLoss```. detach_features: Whether or not to detach the features, from which logits are computed. f_hook: The hook for computing logits. \"\"\" super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , torch . nn . CrossEntropyLoss , { \"reduction\" : \"none\" } ) self . hook = c_f . default ( f_hook , FeaturesAndLogitsHook , { \"domains\" : [ \"src\" ], \"detach_features\" : detach_features }, ) def call ( self , inputs , losses ): \"\"\"\"\"\" outputs = self . hook ( inputs , losses )[ 0 ] [ src_logits ] = c_f . extract ( [ outputs , inputs ], c_f . filter ( self . hook . out_keys , \"_logits$\" ) ) loss = self . loss_fn ( src_logits , inputs [ \"src_labels\" ]) return outputs , { self . _loss_keys ()[ 0 ]: loss } def _loss_keys ( self ): \"\"\"\"\"\" return [ \"c_loss\" ]","title":"CLossHook"},{"location":"docs/hooks/classification/#pytorch_adapt.hooks.classification.ClassifierHook","text":"Bases: BaseWrapperHook This computes the classification loss and also optimizes the models. Source code in pytorch_adapt\\hooks\\classification.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class ClassifierHook ( BaseWrapperHook ): \"\"\" This computes the classification loss and also optimizes the models. \"\"\" def __init__ ( self , opts , weighter = None , reducer = None , loss_fn = None , f_hook = None , detach_features = False , pre = None , post = None , ** kwargs , ): super () . __init__ ( ** kwargs ) [ pre , post ] = c_f . many_default ([ pre , post ], [[], []]) hook = CLossHook ( loss_fn , detach_features , f_hook ) hook = ChainHook ( * pre , hook , * post ) hook = OptimizerHook ( hook , opts , weighter , reducer ) s_hook = SummaryHook ({ \"total_loss\" : hook }) self . hook = ChainHook ( hook , s_hook )","title":"ClassifierHook"},{"location":"docs/hooks/classification/#pytorch_adapt.hooks.classification.FinetunerHook","text":"Bases: ClassifierHook This is the same as ClassifierHook , but it freezes the generator model (\"G\"). Source code in pytorch_adapt\\hooks\\classification.py 122 123 124 125 126 127 128 129 130 131 132 class FinetunerHook ( ClassifierHook ): \"\"\" This is the same as [```ClassifierHook```][pytorch_adapt.hooks.ClassifierHook], but it freezes the generator model (\"G\"). \"\"\" def __init__ ( self , ** kwargs ): f_hook = FrozenModelHook ( FeaturesHook ( detach = True , domains = [ \"src\" ]), \"G\" ) f_hook = FeaturesChainHook ( f_hook , LogitsHook ( domains = [ \"src\" ])) super () . __init__ ( f_hook = f_hook , ** kwargs )","title":"FinetunerHook"},{"location":"docs/hooks/classification/#pytorch_adapt.hooks.classification.SoftmaxHook","text":"Bases: ApplyFnHook Applies torch.nn.Softmax(dim=1) to the specified inputs. Source code in pytorch_adapt\\hooks\\classification.py 18 19 20 21 22 23 24 25 class SoftmaxHook ( ApplyFnHook ): \"\"\" Applies ```torch.nn.Softmax(dim=1)``` to the specified inputs. \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( fn = torch . nn . Softmax ( dim = 1 ), ** kwargs )","title":"SoftmaxHook"},{"location":"docs/hooks/classification/#pytorch_adapt.hooks.classification.SoftmaxLocallyHook","text":"Bases: BaseWrapperHook Applies torch.nn.Softmax(dim=1) to the specified inputs, which are overwritten, but only inside this hook. Source code in pytorch_adapt\\hooks\\classification.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class SoftmaxLocallyHook ( BaseWrapperHook ): \"\"\" Applies ```torch.nn.Softmax(dim=1)``` to the specified inputs, which are overwritten, but only inside this hook. \"\"\" def __init__ ( self , apply_to : List [ str ], * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: apply_to: list of names of tensors that softmax will be applied to. *hooks: the hooks that will receive the softmaxed tensors. \"\"\" super () . __init__ ( ** kwargs ) s_hook = SoftmaxHook ( apply_to = apply_to ) self . hook = OnlyNewOutputsHook ( ChainHook ( s_hook , * hooks , overwrite = True ))","title":"SoftmaxLocallyHook"},{"location":"docs/hooks/conditions/","text":"StrongDHook \u00b6 Bases: BaseConditionHook Returns True if the discriminator's accuracy is higher than some threshold. Source code in pytorch_adapt\\hooks\\conditions.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class StrongDHook ( BaseConditionHook ): \"\"\" Returns ```True``` if the discriminator's accuracy is higher than some threshold. \"\"\" def __init__ ( self , threshold : float = 0.6 , ** kwargs ): \"\"\" Arguments: threshold: The discriminator's accuracy must be higher than this threshold for the hook to return ```True```. \"\"\" super () . __init__ ( ** kwargs ) self . accuracy_fn = SufficientAccuracy ( threshold = threshold , to_probs_func = torch . nn . Sigmoid () ) self . hook = FeaturesChainHook ( FeaturesHook ( detach = True ), DLogitsHook ( detach = True ) ) def call ( self , inputs , losses ): \"\"\"\"\"\" with torch . no_grad (): outputs = self . hook ( inputs , losses )[ 0 ] [ d_src_logits , d_target_logits ] = c_f . extract ( [ outputs , inputs ], c_f . filter ( self . hook . out_keys , \"_dlogits_detached$\" , [ \"^src\" , \"^target\" ] ), ) [ src_domain , target_domain ] = c_f . extract ( inputs , [ \"src_domain\" , \"target_domain\" ] ) dlogits = torch . cat ([ d_src_logits , d_target_logits ], dim = 0 ) domain_labels = torch . cat ([ src_domain , target_domain ], dim = 0 ) return self . accuracy_fn ( dlogits , domain_labels ) __init__ ( threshold = 0.6 , ** kwargs ) \u00b6 Parameters: Name Type Description Default threshold float The discriminator's accuracy must be higher than this threshold for the hook to return True . 0.6 Source code in pytorch_adapt\\hooks\\conditions.py 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , threshold : float = 0.6 , ** kwargs ): \"\"\" Arguments: threshold: The discriminator's accuracy must be higher than this threshold for the hook to return ```True```. \"\"\" super () . __init__ ( ** kwargs ) self . accuracy_fn = SufficientAccuracy ( threshold = threshold , to_probs_func = torch . nn . Sigmoid () ) self . hook = FeaturesChainHook ( FeaturesHook ( detach = True ), DLogitsHook ( detach = True ) )","title":"conditions"},{"location":"docs/hooks/conditions/#pytorch_adapt.hooks.conditions.StrongDHook","text":"Bases: BaseConditionHook Returns True if the discriminator's accuracy is higher than some threshold. Source code in pytorch_adapt\\hooks\\conditions.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class StrongDHook ( BaseConditionHook ): \"\"\" Returns ```True``` if the discriminator's accuracy is higher than some threshold. \"\"\" def __init__ ( self , threshold : float = 0.6 , ** kwargs ): \"\"\" Arguments: threshold: The discriminator's accuracy must be higher than this threshold for the hook to return ```True```. \"\"\" super () . __init__ ( ** kwargs ) self . accuracy_fn = SufficientAccuracy ( threshold = threshold , to_probs_func = torch . nn . Sigmoid () ) self . hook = FeaturesChainHook ( FeaturesHook ( detach = True ), DLogitsHook ( detach = True ) ) def call ( self , inputs , losses ): \"\"\"\"\"\" with torch . no_grad (): outputs = self . hook ( inputs , losses )[ 0 ] [ d_src_logits , d_target_logits ] = c_f . extract ( [ outputs , inputs ], c_f . filter ( self . hook . out_keys , \"_dlogits_detached$\" , [ \"^src\" , \"^target\" ] ), ) [ src_domain , target_domain ] = c_f . extract ( inputs , [ \"src_domain\" , \"target_domain\" ] ) dlogits = torch . cat ([ d_src_logits , d_target_logits ], dim = 0 ) domain_labels = torch . cat ([ src_domain , target_domain ], dim = 0 ) return self . accuracy_fn ( dlogits , domain_labels )","title":"StrongDHook"},{"location":"docs/hooks/dann/","text":"DANNHook \u00b6 Bases: BaseWrapperHook Implementation of Domain-Adversarial Training of Neural Networks . This includes the model optimization step. Source code in pytorch_adapt\\hooks\\dann.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class DANNHook ( BaseWrapperHook ): \"\"\" Implementation of [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818). This includes the model optimization step. \"\"\" def __init__ ( self , opts , weighter = None , reducer = None , pre = None , pre_d = None , post_d = None , pre_g = None , post_g = None , gradient_reversal = None , gradient_reversal_weight = 1 , use_logits = False , f_hook = None , d_hook = None , c_hook = None , domain_loss_hook = None , d_hook_allowed = \"_dlogits$\" , ** kwargs ): \"\"\" Arguments: opts: List of optimizers for updating the models. weighter: Weights the losses before backpropagation. If ```None``` then it defaults to [```MeanWeighter```][pytorch_adapt.weighters.MeanWeighter] reducer: Reduces loss tensors. If ```None``` then it defaults to [```MeanReducer```][pytorch_adapt.hooks.MeanReducer] pre: List of hooks that will be executed at the very beginning of each iteration. pre_d: List of hooks that will be executed after gradient reversal, but before the domain loss. post_d: List of hooks that will be executed after gradient reversal, and after the domain loss. pre_g: List of hooks that will be executed outside of the gradient reversal step, and before the generator and classifier loss. post_g: List of hooks that will be executed after the generator and classifier losses. gradient_reversal: Called before all D hooks, including ```pre_d```. use_logits: If ```True```, then D receives the output of C instead of the output of G. f_hook: The hook used for computing features and logits. If ```None``` then it defaults to [```FeaturesForDomainLossHook```][pytorch_adapt.hooks.FeaturesForDomainLossHook] d_hook: The hook used for computing discriminator logits. If ```None``` then it defaults to [```DLogitsHook```][pytorch_adapt.hooks.DLogitsHook] c_hook: The hook used for computing the classifiers's loss. If ```None``` then it defaults to [```CLossHook```][pytorch_adapt.hooks.CLossHook] domain_loss_hook: The hook used for computing the domain loss. If ```None``` then it defaults to [```DomainLossHook```][pytorch_adapt.hooks.DomainLossHook]. d_hook_allowed: A regex string that specifies the allowed output names of the discriminator block. \"\"\" super () . __init__ ( ** kwargs ) [ pre , pre_d , post_d , pre_g , post_g ] = c_f . many_default ( [ pre , pre_d , post_d , pre_g , post_g ], [[], [], [], [], []] ) f_hook = c_f . default ( f_hook , FeaturesForDomainLossHook , { \"use_logits\" : use_logits } ) gradient_reversal = c_f . default ( gradient_reversal , GradientReversalHook , { \"weight\" : gradient_reversal_weight , \"apply_to\" : f_hook . out_keys }, ) c_hook = c_f . default ( c_hook , CLossHook , {}) domain_loss_hook = c_f . default ( domain_loss_hook , DomainLossHook , { \"f_hook\" : f_hook , \"d_hook\" : d_hook } ) disc_hook = AssertHook ( OnlyNewOutputsHook ( ChainHook ( gradient_reversal , * pre_d , domain_loss_hook , * post_d , overwrite = [ 1 ], ) ), d_hook_allowed , ) gen_hook = ChainHook ( * pre_g , c_hook , * post_g ) hook = ChainHook ( * pre , f_hook , disc_hook , gen_hook ) hook = OptimizerHook ( hook , opts , weighter , reducer ) s_hook = SummaryHook ({ \"total_loss\" : hook }) self . hook = ChainHook ( hook , s_hook ) __init__ ( opts , weighter = None , reducer = None , pre = None , pre_d = None , post_d = None , pre_g = None , post_g = None , gradient_reversal = None , gradient_reversal_weight = 1 , use_logits = False , f_hook = None , d_hook = None , c_hook = None , domain_loss_hook = None , d_hook_allowed = '_dlogits$' , ** kwargs ) \u00b6 Parameters: Name Type Description Default opts List of optimizers for updating the models. required weighter Weights the losses before backpropagation. If None then it defaults to MeanWeighter None reducer Reduces loss tensors. If None then it defaults to MeanReducer None pre List of hooks that will be executed at the very beginning of each iteration. None pre_d List of hooks that will be executed after gradient reversal, but before the domain loss. None post_d List of hooks that will be executed after gradient reversal, and after the domain loss. None pre_g List of hooks that will be executed outside of the gradient reversal step, and before the generator and classifier loss. None post_g List of hooks that will be executed after the generator and classifier losses. None gradient_reversal Called before all D hooks, including pre_d . None use_logits If True , then D receives the output of C instead of the output of G. False f_hook The hook used for computing features and logits. If None then it defaults to FeaturesForDomainLossHook None d_hook The hook used for computing discriminator logits. If None then it defaults to DLogitsHook None c_hook The hook used for computing the classifiers's loss. If None then it defaults to CLossHook None domain_loss_hook The hook used for computing the domain loss. If None then it defaults to DomainLossHook . None d_hook_allowed A regex string that specifies the allowed output names of the discriminator block. '_dlogits$' Source code in pytorch_adapt\\hooks\\dann.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def __init__ ( self , opts , weighter = None , reducer = None , pre = None , pre_d = None , post_d = None , pre_g = None , post_g = None , gradient_reversal = None , gradient_reversal_weight = 1 , use_logits = False , f_hook = None , d_hook = None , c_hook = None , domain_loss_hook = None , d_hook_allowed = \"_dlogits$\" , ** kwargs ): \"\"\" Arguments: opts: List of optimizers for updating the models. weighter: Weights the losses before backpropagation. If ```None``` then it defaults to [```MeanWeighter```][pytorch_adapt.weighters.MeanWeighter] reducer: Reduces loss tensors. If ```None``` then it defaults to [```MeanReducer```][pytorch_adapt.hooks.MeanReducer] pre: List of hooks that will be executed at the very beginning of each iteration. pre_d: List of hooks that will be executed after gradient reversal, but before the domain loss. post_d: List of hooks that will be executed after gradient reversal, and after the domain loss. pre_g: List of hooks that will be executed outside of the gradient reversal step, and before the generator and classifier loss. post_g: List of hooks that will be executed after the generator and classifier losses. gradient_reversal: Called before all D hooks, including ```pre_d```. use_logits: If ```True```, then D receives the output of C instead of the output of G. f_hook: The hook used for computing features and logits. If ```None``` then it defaults to [```FeaturesForDomainLossHook```][pytorch_adapt.hooks.FeaturesForDomainLossHook] d_hook: The hook used for computing discriminator logits. If ```None``` then it defaults to [```DLogitsHook```][pytorch_adapt.hooks.DLogitsHook] c_hook: The hook used for computing the classifiers's loss. If ```None``` then it defaults to [```CLossHook```][pytorch_adapt.hooks.CLossHook] domain_loss_hook: The hook used for computing the domain loss. If ```None``` then it defaults to [```DomainLossHook```][pytorch_adapt.hooks.DomainLossHook]. d_hook_allowed: A regex string that specifies the allowed output names of the discriminator block. \"\"\" super () . __init__ ( ** kwargs ) [ pre , pre_d , post_d , pre_g , post_g ] = c_f . many_default ( [ pre , pre_d , post_d , pre_g , post_g ], [[], [], [], [], []] ) f_hook = c_f . default ( f_hook , FeaturesForDomainLossHook , { \"use_logits\" : use_logits } ) gradient_reversal = c_f . default ( gradient_reversal , GradientReversalHook , { \"weight\" : gradient_reversal_weight , \"apply_to\" : f_hook . out_keys }, ) c_hook = c_f . default ( c_hook , CLossHook , {}) domain_loss_hook = c_f . default ( domain_loss_hook , DomainLossHook , { \"f_hook\" : f_hook , \"d_hook\" : d_hook } ) disc_hook = AssertHook ( OnlyNewOutputsHook ( ChainHook ( gradient_reversal , * pre_d , domain_loss_hook , * post_d , overwrite = [ 1 ], ) ), d_hook_allowed , ) gen_hook = ChainHook ( * pre_g , c_hook , * post_g ) hook = ChainHook ( * pre , f_hook , disc_hook , gen_hook ) hook = OptimizerHook ( hook , opts , weighter , reducer ) s_hook = SummaryHook ({ \"total_loss\" : hook }) self . hook = ChainHook ( hook , s_hook )","title":"dann"},{"location":"docs/hooks/dann/#pytorch_adapt.hooks.dann.DANNHook","text":"Bases: BaseWrapperHook Implementation of Domain-Adversarial Training of Neural Networks . This includes the model optimization step. Source code in pytorch_adapt\\hooks\\dann.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class DANNHook ( BaseWrapperHook ): \"\"\" Implementation of [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818). This includes the model optimization step. \"\"\" def __init__ ( self , opts , weighter = None , reducer = None , pre = None , pre_d = None , post_d = None , pre_g = None , post_g = None , gradient_reversal = None , gradient_reversal_weight = 1 , use_logits = False , f_hook = None , d_hook = None , c_hook = None , domain_loss_hook = None , d_hook_allowed = \"_dlogits$\" , ** kwargs ): \"\"\" Arguments: opts: List of optimizers for updating the models. weighter: Weights the losses before backpropagation. If ```None``` then it defaults to [```MeanWeighter```][pytorch_adapt.weighters.MeanWeighter] reducer: Reduces loss tensors. If ```None``` then it defaults to [```MeanReducer```][pytorch_adapt.hooks.MeanReducer] pre: List of hooks that will be executed at the very beginning of each iteration. pre_d: List of hooks that will be executed after gradient reversal, but before the domain loss. post_d: List of hooks that will be executed after gradient reversal, and after the domain loss. pre_g: List of hooks that will be executed outside of the gradient reversal step, and before the generator and classifier loss. post_g: List of hooks that will be executed after the generator and classifier losses. gradient_reversal: Called before all D hooks, including ```pre_d```. use_logits: If ```True```, then D receives the output of C instead of the output of G. f_hook: The hook used for computing features and logits. If ```None``` then it defaults to [```FeaturesForDomainLossHook```][pytorch_adapt.hooks.FeaturesForDomainLossHook] d_hook: The hook used for computing discriminator logits. If ```None``` then it defaults to [```DLogitsHook```][pytorch_adapt.hooks.DLogitsHook] c_hook: The hook used for computing the classifiers's loss. If ```None``` then it defaults to [```CLossHook```][pytorch_adapt.hooks.CLossHook] domain_loss_hook: The hook used for computing the domain loss. If ```None``` then it defaults to [```DomainLossHook```][pytorch_adapt.hooks.DomainLossHook]. d_hook_allowed: A regex string that specifies the allowed output names of the discriminator block. \"\"\" super () . __init__ ( ** kwargs ) [ pre , pre_d , post_d , pre_g , post_g ] = c_f . many_default ( [ pre , pre_d , post_d , pre_g , post_g ], [[], [], [], [], []] ) f_hook = c_f . default ( f_hook , FeaturesForDomainLossHook , { \"use_logits\" : use_logits } ) gradient_reversal = c_f . default ( gradient_reversal , GradientReversalHook , { \"weight\" : gradient_reversal_weight , \"apply_to\" : f_hook . out_keys }, ) c_hook = c_f . default ( c_hook , CLossHook , {}) domain_loss_hook = c_f . default ( domain_loss_hook , DomainLossHook , { \"f_hook\" : f_hook , \"d_hook\" : d_hook } ) disc_hook = AssertHook ( OnlyNewOutputsHook ( ChainHook ( gradient_reversal , * pre_d , domain_loss_hook , * post_d , overwrite = [ 1 ], ) ), d_hook_allowed , ) gen_hook = ChainHook ( * pre_g , c_hook , * post_g ) hook = ChainHook ( * pre , f_hook , disc_hook , gen_hook ) hook = OptimizerHook ( hook , opts , weighter , reducer ) s_hook = SummaryHook ({ \"total_loss\" : hook }) self . hook = ChainHook ( hook , s_hook )","title":"DANNHook"},{"location":"docs/hooks/domain/","text":"DomainLossHook \u00b6 Bases: BaseWrapperHook Computes the loss of a discriminator's output with respect to domain labels. Source code in pytorch_adapt\\hooks\\domain.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class DomainLossHook ( BaseWrapperHook ): \"\"\" Computes the loss of a discriminator's output with respect to domain labels. \"\"\" def __init__ ( self , d_loss_fn = None , detach_features = False , reverse_labels = False , domains = None , f_hook = None , d_hook = None , ** kwargs , ): \"\"\" Arguments: d_loss_fn: The loss applied to the discriminator's logits. If ```None``` it defaults to ```torch.nn.BCEWithLogitsLoss```. detach_features: If ```True```, the input to the discriminator will be detached first. reverse_labels: If ```True```, the ```\"src\"``` and ```\"target\"``` domain labels will be swapped. domains: The domains to apply the loss to. If ```None``` it defaults to ```[\"src\", \"target\"]```. f_hook: The hook for computing the input to the discriminator. d_hook: The hook for computing the discriminator logits. \"\"\" super () . __init__ ( ** kwargs ) self . d_loss_fn = c_f . default ( d_loss_fn , torch . nn . BCEWithLogitsLoss , { \"reduction\" : \"none\" } ) self . reverse_labels = reverse_labels self . domains = c_f . default ( domains , [ \"src\" , \"target\" ]) f_hook = c_f . default ( f_hook , FeaturesForDomainLossHook , { \"detach\" : detach_features , \"domains\" : domains }, ) d_hook = c_f . default ( d_hook , DLogitsHook , { \"domains\" : domains }) f_out = f_hook . last_hook_out_keys d_in = d_hook . in_keys d_hook . set_in_keys ( f_out ) self . check_fhook_dhook_keys ( f_hook , d_hook , detach_features ) self . hook = ChainHook ( f_hook , d_hook ) self . in_keys = self . hook . in_keys + [ \"src_domain\" , \"target_domain\" ] def call ( self , inputs , losses ): losses = {} outputs = self . hook ( inputs , losses )[ 0 ] labels = self . extract_domain_labels ( inputs ) for domain_name , labels in labels . items (): self . logger ( f \"Computing loss for { domain_name } domain\" ) [ dlogits ] = c_f . extract ( [ outputs , inputs ], c_f . filter ( self . hook . out_keys , f \"_dlogits$\" , [ f \"^ { domain_name } \" ]), ) if dlogits . dim () > 1 : labels = labels . type ( torch . long ) else : labels = labels . type ( torch . float ) loss = self . d_loss_fn ( dlogits , labels ) losses [ f \" { domain_name } _domain_loss\" ] = loss return outputs , losses def extract_domain_labels ( self , inputs ): self . logger ( \"Expecting 'src_domain' and 'target_domain' in inputs\" ) [ src_domain , target_domain ] = c_f . extract ( inputs , [ \"src_domain\" , \"target_domain\" ] ) if self . reverse_labels : labels = { \"src\" : target_domain , \"target\" : src_domain } else : labels = { \"src\" : src_domain , \"target\" : target_domain } return { k : v for k , v in labels . items () if k in self . domains } def _loss_keys ( self ): return [ f \" { x } _domain_loss\" for x in self . domains ] def check_fhook_dhook_keys ( self , f_hook , d_hook , detach_features ): if detach_features and len ( c_f . filter ( f_hook . out_keys , \"detached$\" , self . domains ) ) < len ( self . domains ): error_str = ( \"detach_features is True, but the number of f_hook's detached outputs \" ) error_str += \"doesn't match the number of domains.\" error_str += f \" \\n f_hook's outputs: { f_hook . out_keys } \" error_str += f \" \\n fdomains: { self . domains } \" raise ValueError ( error_str ) for name , keys in [( \"f_hook\" , f_hook . out_keys ), ( \"d_hook\" , d_hook . out_keys )]: if not all ( c_f . filter ( keys , f \"^ { self . domains [ i ] } \" ) for i in range ( len ( self . domains )) ): raise ValueError ( f \"domains = { self . domains } but d_hook.out_keys = { d_hook . out_keys } \" ) def extra_repr ( self ): return c_f . extra_repr ( self , [ \"reverse_labels\" ]) __init__ ( d_loss_fn = None , detach_features = False , reverse_labels = False , domains = None , f_hook = None , d_hook = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default d_loss_fn The loss applied to the discriminator's logits. If None it defaults to torch.nn.BCEWithLogitsLoss . None detach_features If True , the input to the discriminator will be detached first. False reverse_labels If True , the \"src\" and \"target\" domain labels will be swapped. False domains The domains to apply the loss to. If None it defaults to [\"src\", \"target\"] . None f_hook The hook for computing the input to the discriminator. None d_hook The hook for computing the discriminator logits. None Source code in pytorch_adapt\\hooks\\domain.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , d_loss_fn = None , detach_features = False , reverse_labels = False , domains = None , f_hook = None , d_hook = None , ** kwargs , ): \"\"\" Arguments: d_loss_fn: The loss applied to the discriminator's logits. If ```None``` it defaults to ```torch.nn.BCEWithLogitsLoss```. detach_features: If ```True```, the input to the discriminator will be detached first. reverse_labels: If ```True```, the ```\"src\"``` and ```\"target\"``` domain labels will be swapped. domains: The domains to apply the loss to. If ```None``` it defaults to ```[\"src\", \"target\"]```. f_hook: The hook for computing the input to the discriminator. d_hook: The hook for computing the discriminator logits. \"\"\" super () . __init__ ( ** kwargs ) self . d_loss_fn = c_f . default ( d_loss_fn , torch . nn . BCEWithLogitsLoss , { \"reduction\" : \"none\" } ) self . reverse_labels = reverse_labels self . domains = c_f . default ( domains , [ \"src\" , \"target\" ]) f_hook = c_f . default ( f_hook , FeaturesForDomainLossHook , { \"detach\" : detach_features , \"domains\" : domains }, ) d_hook = c_f . default ( d_hook , DLogitsHook , { \"domains\" : domains }) f_out = f_hook . last_hook_out_keys d_in = d_hook . in_keys d_hook . set_in_keys ( f_out ) self . check_fhook_dhook_keys ( f_hook , d_hook , detach_features ) self . hook = ChainHook ( f_hook , d_hook ) self . in_keys = self . hook . in_keys + [ \"src_domain\" , \"target_domain\" ] FeaturesForDomainLossHook \u00b6 Bases: FeaturesChainHook A FeaturesChainHook that has options specific to DomainLossHook . Source code in pytorch_adapt\\hooks\\domain.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class FeaturesForDomainLossHook ( FeaturesChainHook ): \"\"\" A [```FeaturesChainHook```][pytorch_adapt.hooks.features.FeaturesChainHook] that has options specific to [```DomainLossHook```][pytorch_adapt.hooks.DomainLossHook]. \"\"\" def __init__ ( self , f_hook = None , l_hook = None , use_logits = False , domains = None , detach = False , ** kwargs , ): \"\"\" Arguments: f_hook: hook for computing features l_hook: hook for computing logits. This will be used only if ```use_logits``` is ```True```. use_logits: If ```True```, the logits hook is executed after the features hook. domains: the domains for which features will be computed. detach: If ```True```, all outputs will be detached from the autograd graph. \"\"\" hooks = [ c_f . default ( f_hook , FeaturesHook ( detach = detach , domains = domains ), ) ] if use_logits : hooks . append ( c_f . default ( l_hook , LogitsHook ( detach = detach , domains = domains )) ) super () . __init__ ( * hooks , ** kwargs ) __init__ ( f_hook = None , l_hook = None , use_logits = False , domains = None , detach = False , ** kwargs ) \u00b6 Parameters: Name Type Description Default f_hook hook for computing features None l_hook hook for computing logits. This will be used only if use_logits is True . None use_logits If True , the logits hook is executed after the features hook. False domains the domains for which features will be computed. None detach If True , all outputs will be detached from the autograd graph. False Source code in pytorch_adapt\\hooks\\domain.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , f_hook = None , l_hook = None , use_logits = False , domains = None , detach = False , ** kwargs , ): \"\"\" Arguments: f_hook: hook for computing features l_hook: hook for computing logits. This will be used only if ```use_logits``` is ```True```. use_logits: If ```True```, the logits hook is executed after the features hook. domains: the domains for which features will be computed. detach: If ```True```, all outputs will be detached from the autograd graph. \"\"\" hooks = [ c_f . default ( f_hook , FeaturesHook ( detach = detach , domains = domains ), ) ] if use_logits : hooks . append ( c_f . default ( l_hook , LogitsHook ( detach = detach , domains = domains )) ) super () . __init__ ( * hooks , ** kwargs )","title":"domain"},{"location":"docs/hooks/domain/#pytorch_adapt.hooks.domain.DomainLossHook","text":"Bases: BaseWrapperHook Computes the loss of a discriminator's output with respect to domain labels. Source code in pytorch_adapt\\hooks\\domain.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class DomainLossHook ( BaseWrapperHook ): \"\"\" Computes the loss of a discriminator's output with respect to domain labels. \"\"\" def __init__ ( self , d_loss_fn = None , detach_features = False , reverse_labels = False , domains = None , f_hook = None , d_hook = None , ** kwargs , ): \"\"\" Arguments: d_loss_fn: The loss applied to the discriminator's logits. If ```None``` it defaults to ```torch.nn.BCEWithLogitsLoss```. detach_features: If ```True```, the input to the discriminator will be detached first. reverse_labels: If ```True```, the ```\"src\"``` and ```\"target\"``` domain labels will be swapped. domains: The domains to apply the loss to. If ```None``` it defaults to ```[\"src\", \"target\"]```. f_hook: The hook for computing the input to the discriminator. d_hook: The hook for computing the discriminator logits. \"\"\" super () . __init__ ( ** kwargs ) self . d_loss_fn = c_f . default ( d_loss_fn , torch . nn . BCEWithLogitsLoss , { \"reduction\" : \"none\" } ) self . reverse_labels = reverse_labels self . domains = c_f . default ( domains , [ \"src\" , \"target\" ]) f_hook = c_f . default ( f_hook , FeaturesForDomainLossHook , { \"detach\" : detach_features , \"domains\" : domains }, ) d_hook = c_f . default ( d_hook , DLogitsHook , { \"domains\" : domains }) f_out = f_hook . last_hook_out_keys d_in = d_hook . in_keys d_hook . set_in_keys ( f_out ) self . check_fhook_dhook_keys ( f_hook , d_hook , detach_features ) self . hook = ChainHook ( f_hook , d_hook ) self . in_keys = self . hook . in_keys + [ \"src_domain\" , \"target_domain\" ] def call ( self , inputs , losses ): losses = {} outputs = self . hook ( inputs , losses )[ 0 ] labels = self . extract_domain_labels ( inputs ) for domain_name , labels in labels . items (): self . logger ( f \"Computing loss for { domain_name } domain\" ) [ dlogits ] = c_f . extract ( [ outputs , inputs ], c_f . filter ( self . hook . out_keys , f \"_dlogits$\" , [ f \"^ { domain_name } \" ]), ) if dlogits . dim () > 1 : labels = labels . type ( torch . long ) else : labels = labels . type ( torch . float ) loss = self . d_loss_fn ( dlogits , labels ) losses [ f \" { domain_name } _domain_loss\" ] = loss return outputs , losses def extract_domain_labels ( self , inputs ): self . logger ( \"Expecting 'src_domain' and 'target_domain' in inputs\" ) [ src_domain , target_domain ] = c_f . extract ( inputs , [ \"src_domain\" , \"target_domain\" ] ) if self . reverse_labels : labels = { \"src\" : target_domain , \"target\" : src_domain } else : labels = { \"src\" : src_domain , \"target\" : target_domain } return { k : v for k , v in labels . items () if k in self . domains } def _loss_keys ( self ): return [ f \" { x } _domain_loss\" for x in self . domains ] def check_fhook_dhook_keys ( self , f_hook , d_hook , detach_features ): if detach_features and len ( c_f . filter ( f_hook . out_keys , \"detached$\" , self . domains ) ) < len ( self . domains ): error_str = ( \"detach_features is True, but the number of f_hook's detached outputs \" ) error_str += \"doesn't match the number of domains.\" error_str += f \" \\n f_hook's outputs: { f_hook . out_keys } \" error_str += f \" \\n fdomains: { self . domains } \" raise ValueError ( error_str ) for name , keys in [( \"f_hook\" , f_hook . out_keys ), ( \"d_hook\" , d_hook . out_keys )]: if not all ( c_f . filter ( keys , f \"^ { self . domains [ i ] } \" ) for i in range ( len ( self . domains )) ): raise ValueError ( f \"domains = { self . domains } but d_hook.out_keys = { d_hook . out_keys } \" ) def extra_repr ( self ): return c_f . extra_repr ( self , [ \"reverse_labels\" ])","title":"DomainLossHook"},{"location":"docs/hooks/domain/#pytorch_adapt.hooks.domain.FeaturesForDomainLossHook","text":"Bases: FeaturesChainHook A FeaturesChainHook that has options specific to DomainLossHook . Source code in pytorch_adapt\\hooks\\domain.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class FeaturesForDomainLossHook ( FeaturesChainHook ): \"\"\" A [```FeaturesChainHook```][pytorch_adapt.hooks.features.FeaturesChainHook] that has options specific to [```DomainLossHook```][pytorch_adapt.hooks.DomainLossHook]. \"\"\" def __init__ ( self , f_hook = None , l_hook = None , use_logits = False , domains = None , detach = False , ** kwargs , ): \"\"\" Arguments: f_hook: hook for computing features l_hook: hook for computing logits. This will be used only if ```use_logits``` is ```True```. use_logits: If ```True```, the logits hook is executed after the features hook. domains: the domains for which features will be computed. detach: If ```True```, all outputs will be detached from the autograd graph. \"\"\" hooks = [ c_f . default ( f_hook , FeaturesHook ( detach = detach , domains = domains ), ) ] if use_logits : hooks . append ( c_f . default ( l_hook , LogitsHook ( detach = detach , domains = domains )) ) super () . __init__ ( * hooks , ** kwargs )","title":"FeaturesForDomainLossHook"},{"location":"docs/hooks/domain_confusion/","text":"DomainConfusionHook \u00b6 Bases: GANHook Implementation of Simultaneous Deep Transfer Across Domains and Tasks Source code in pytorch_adapt\\hooks\\domain_confusion.py 7 8 9 10 11 12 13 14 15 16 17 18 class DomainConfusionHook ( GANHook ): \"\"\" Implementation of [Simultaneous Deep Transfer Across Domains and Tasks](https://arxiv.org/abs/1510.02192) \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( disc_domain_loss_fn = torch . nn . CrossEntropyLoss ( reduction = \"none\" ), gen_domain_loss_fn = UniformDistributionLoss (), ** kwargs , )","title":"domain_confusion"},{"location":"docs/hooks/domain_confusion/#pytorch_adapt.hooks.domain_confusion.DomainConfusionHook","text":"Bases: GANHook Implementation of Simultaneous Deep Transfer Across Domains and Tasks Source code in pytorch_adapt\\hooks\\domain_confusion.py 7 8 9 10 11 12 13 14 15 16 17 18 class DomainConfusionHook ( GANHook ): \"\"\" Implementation of [Simultaneous Deep Transfer Across Domains and Tasks](https://arxiv.org/abs/1510.02192) \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( disc_domain_loss_fn = torch . nn . CrossEntropyLoss ( reduction = \"none\" ), gen_domain_loss_fn = UniformDistributionLoss (), ** kwargs , )","title":"DomainConfusionHook"},{"location":"docs/hooks/features/","text":"BaseFeaturesHook \u00b6 Bases: BaseHook This hook: Checks to see if specific tensors are in the context Exits if the tensors are already in the context Otherwise computes those tensors using the appropriate inputs and models, and adds them to the context. Source code in pytorch_adapt\\hooks\\features.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 class BaseFeaturesHook ( BaseHook ): \"\"\" This hook: 1. Checks to see if specific tensors are in the context 2. Exits if the tensors are already in the context 3. Otherwise computes those tensors using the appropriate inputs and models, and adds them to the context. \"\"\" def __init__ ( self , model_name : str , in_suffixes : List [ str ] = None , out_suffixes : List [ str ] = None , domains : List [ str ] = None , detach : bool = False , ** kwargs , ): \"\"\" Arguments: model_name: The name of the model that will be used to compute any missing tensors. in_suffixes: The suffixes of the names of the inputs to the model. For example if: - ```domains = [\"src\", \"target\"]``` - ```in_suffixes = [\"_imgs_features\"]``` then the model will be given - ```[\"src_imgs_features\", \"target_imgs_features\"]```. out_suffixes: The suffixes of the names of the outputs of the model. Output suffixes are appended to the input name. For example, if - ```domains = [\"src\", \"target\"]``` - ```in_suffixes = [\"_imgs_features\"]``` - ```out_suffixes = [\"_logits\"]``` then the output keys will be - ```[\"src_imgs_features_logits\", \"target_imgs_features_logits\"]``` domains: The names of the domains to use. If ```None```, this defaults to ```[\"src\", \"target\"]```. detach: If ```True```, then the output will be detached from the autograd graph. Any output that is detached will have ```\"_detached\"``` appended to its name in the context. \"\"\" super () . __init__ ( ** kwargs ) self . model_name = model_name self . domains = c_f . default ( domains , [ \"src\" , \"target\" ]) self . init_detach_mode ( detach ) self . init_suffixes ( in_suffixes , out_suffixes ) def call ( self , inputs , losses ): \"\"\"\"\"\" outputs = {} for domain in self . domains : self . logger ( f \"Getting { domain } \" ) detach = self . check_grad_mode ( domain ) func = self . mode_detached if detach else self . mode_with_grad in_keys = c_f . filter ( self . in_keys , f \"^ { domain } \" ) func ( inputs , outputs , domain , in_keys ) self . check_outputs_requires_grad ( outputs ) return outputs , {} def check_grad_mode ( self , domain ): detach = self . detach [ domain ] if not torch . is_grad_enabled (): if not detach : raise ValueError ( f \"detach[ { domain } ] == { detach } but grad is not enabled\" ) return detach def check_outputs_requires_grad ( self , outputs ): for k , v in outputs . items (): if k . endswith ( \"detached\" ) and c_f . requires_grad ( v , does = True ): raise TypeError ( f \" { k } ends with 'detached' but tensor requires grad\" ) if not k . endswith ( \"detached\" ) and c_f . requires_grad ( v , does = False ): raise TypeError ( f \" { k } doesn't end in 'detached' but tensor doesn't require grad\" ) def mode_with_grad ( self , inputs , outputs , domain , in_keys ): output_keys = c_f . filter ( self . _out_keys (), f \"^ { domain } \" ) output_vals = self . get_kwargs ( inputs , output_keys ) self . add_if_new ( outputs , output_keys , output_vals , inputs , self . model_name , in_keys , domain ) return output_keys , output_vals def mode_detached ( self , inputs , outputs , domain , in_keys ): curr_out_keys = c_f . filter ( self . _out_keys (), f \"^ { domain } \" ) self . try_existing_detachable ( inputs , outputs , curr_out_keys ) remaining_out_keys = [ k for k in curr_out_keys if k not in set () . union ( inputs , outputs ) ] if len ( remaining_out_keys ) > 0 : output_vals = self . get_kwargs ( inputs , remaining_out_keys ) with torch . no_grad (): self . add_if_new ( outputs , remaining_out_keys , output_vals , inputs , self . model_name , in_keys , domain , ) def add_if_new ( self , outputs , full_key , output_vals , inputs , model_name , in_keys , domain ): c_f . add_if_new ( outputs , full_key , output_vals , inputs , model_name , in_keys , logger = self . logger , ) def create_keys ( self , domain , suffix , starting_keys = None , detach = False ): if starting_keys is None : full_keys = [ f \" { domain }{ x } \" for x in suffix ] else : if len ( starting_keys ) > 1 : starting_keys = self . join_keys ( starting_keys ) if len ( suffix ) > 1 : starting_keys = starting_keys * len ( suffix ) full_keys = [ f \" { k }{ x } \" for k , x in zip ( starting_keys , suffix )] if detach : full_keys = self . add_detached_string ( full_keys ) return full_keys def get_kwargs ( self , inputs , keys ): return [ inputs . get ( k ) for k in keys ] def try_existing_detachable ( self , inputs , outputs , curr_out_keys ): for k in curr_out_keys : if k in inputs or k in outputs : continue curr_regex = self . detachable_regex [ k ] success = self . try_existing_detachable_in_dict ( curr_regex , inputs , outputs , k ) if not success : self . try_existing_detachable_in_dict ( curr_regex , outputs , outputs , k ) def try_existing_detachable_in_dict ( self , regex , in_dict , outputs , new_k ): for k , v in in_dict . items (): if regex . search ( k ) and v is not None : outputs [ new_k ] = v . detach () return True return False def add_detached_string ( self , keys ): # delete existing detached string, then append to the very end # for example, if computing detached logits for: src_imgs_features_detached # 1. src_imgs_features_detached_logits --> src_imgs_features_logits # 2. src_imgs_features_logits --> src_imgs_features_logits_detached keys = [ k . replace ( \"_detached\" , \"\" ) for k in keys ] return [ f \" { k } _detached\" for k in keys ] def join_keys ( self , keys ): return [ \"_AND_\" . join ( keys )] def init_detach_mode ( self , detach ): if isinstance ( detach , dict ): if any ( not isinstance ( v , bool ) for v in detach . values ()): raise TypeError ( \"if detach is a dict, values must be bools\" ) self . detach = detach elif isinstance ( detach , bool ): self . detach = { k : detach for k in self . domains } else : raise TypeError ( \"detach must be a bool or a dict of bools\" ) def init_suffixes ( self , in_suffixes , out_suffixes ): self . in_suffixes = in_suffixes self . out_suffixes = out_suffixes in_keys = [] for domain in self . domains : in_keys . extend ( self . create_keys ( domain , in_suffixes )) self . set_in_keys ( in_keys ) def set_in_keys ( self , in_keys ): super () . set_in_keys ( in_keys ) self . all_out_keys = [] for domain in self . domains : curr_in_keys = c_f . filter ( self . in_keys , f \"^ { domain } \" ) curr_out_keys = self . create_keys ( domain , self . out_suffixes , curr_in_keys , detach = self . detach [ domain ] ) self . all_out_keys . extend ( curr_out_keys ) # strings with '_detached' optional and anywhere self . detachable_regex = { k : re . compile ( f \"^ { k . replace ( '_detached' , '' ) . replace ( '_' , '(_detached)?_' ) } $\" ) for k in self . all_out_keys } def _loss_keys ( self ): \"\"\"\"\"\" return [] def _out_keys ( self ): \"\"\"\"\"\" return self . all_out_keys def extra_repr ( self ): return c_f . extra_repr ( self , [ \"model_name\" , \"domains\" , \"detach\" ]) __init__ ( model_name , in_suffixes = None , out_suffixes = None , domains = None , detach = False , ** kwargs ) \u00b6 Parameters: Name Type Description Default model_name str The name of the model that will be used to compute any missing tensors. required in_suffixes List [ str ] The suffixes of the names of the inputs to the model. For example if: domains = [\"src\", \"target\"] in_suffixes = [\"_imgs_features\"] then the model will be given [\"src_imgs_features\", \"target_imgs_features\"] . None out_suffixes List [ str ] The suffixes of the names of the outputs of the model. Output suffixes are appended to the input name. For example, if domains = [\"src\", \"target\"] in_suffixes = [\"_imgs_features\"] out_suffixes = [\"_logits\"] then the output keys will be [\"src_imgs_features_logits\", \"target_imgs_features_logits\"] None domains List [ str ] The names of the domains to use. If None , this defaults to [\"src\", \"target\"] . None detach bool If True , then the output will be detached from the autograd graph. Any output that is detached will have \"_detached\" appended to its name in the context. False Source code in pytorch_adapt\\hooks\\features.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , model_name : str , in_suffixes : List [ str ] = None , out_suffixes : List [ str ] = None , domains : List [ str ] = None , detach : bool = False , ** kwargs , ): \"\"\" Arguments: model_name: The name of the model that will be used to compute any missing tensors. in_suffixes: The suffixes of the names of the inputs to the model. For example if: - ```domains = [\"src\", \"target\"]``` - ```in_suffixes = [\"_imgs_features\"]``` then the model will be given - ```[\"src_imgs_features\", \"target_imgs_features\"]```. out_suffixes: The suffixes of the names of the outputs of the model. Output suffixes are appended to the input name. For example, if - ```domains = [\"src\", \"target\"]``` - ```in_suffixes = [\"_imgs_features\"]``` - ```out_suffixes = [\"_logits\"]``` then the output keys will be - ```[\"src_imgs_features_logits\", \"target_imgs_features_logits\"]``` domains: The names of the domains to use. If ```None```, this defaults to ```[\"src\", \"target\"]```. detach: If ```True```, then the output will be detached from the autograd graph. Any output that is detached will have ```\"_detached\"``` appended to its name in the context. \"\"\" super () . __init__ ( ** kwargs ) self . model_name = model_name self . domains = c_f . default ( domains , [ \"src\" , \"target\" ]) self . init_detach_mode ( detach ) self . init_suffixes ( in_suffixes , out_suffixes ) CombinedFeaturesHook \u00b6 Bases: BaseFeaturesHook Default input/output context names: Model: \"feature_combiner\" Inputs: [\"src_imgs_features\", \"src_imgs_features_logits\", \"target_imgs_features\", \"target_imgs_features_logits\"] Outputs: [\"src_imgs_features_AND_src_imgs_features_logits_combined\", \"target_imgs_features_AND_target_imgs_features_logits_combined\"] Source code in pytorch_adapt\\hooks\\features.py 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 class CombinedFeaturesHook ( BaseFeaturesHook ): \"\"\" Default input/output context names: - Model: ```\"feature_combiner\"``` - Inputs: ``` [\"src_imgs_features\", \"src_imgs_features_logits\", \"target_imgs_features\", \"target_imgs_features_logits\"] ``` - Outputs: ``` [\"src_imgs_features_AND_src_imgs_features_logits_combined\", \"target_imgs_features_AND_target_imgs_features_logits_combined\"] ``` \"\"\" def __init__ ( self , in_suffixes = None , out_suffixes = None , ** kwargs , ): in_suffixes = c_f . default ( in_suffixes , [ \"_imgs_features\" , \"_imgs_features_logits\" ] ) out_suffixes = c_f . default ( out_suffixes , [ \"_combined\" ]) super () . __init__ ( model_name = \"feature_combiner\" , in_suffixes = in_suffixes , out_suffixes = out_suffixes , ** kwargs , ) DLogitsHook \u00b6 Bases: BaseFeaturesHook Default input/output context names: Model: \"D\" Inputs: [\"src_imgs_features\", \"target_imgs_features\"] Outputs: [\"src_imgs_features_dlogits\", \"target_imgs_features_dlogits\"] Source code in pytorch_adapt\\hooks\\features.py 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 class DLogitsHook ( BaseFeaturesHook ): \"\"\" Default input/output context names: - Model: ```\"D\"``` - Inputs: ``` [\"src_imgs_features\", \"target_imgs_features\"] ``` - Outputs: ``` [\"src_imgs_features_dlogits\", \"target_imgs_features_dlogits\"] ``` \"\"\" def __init__ ( self , model_name = \"D\" , in_suffixes = None , out_suffixes = None , ** kwargs , ): in_suffixes = c_f . default ( in_suffixes , [ \"_imgs_features\" ]) out_suffixes = c_f . default ( out_suffixes , [ \"_dlogits\" ]) super () . __init__ ( model_name = model_name , in_suffixes = in_suffixes , out_suffixes = out_suffixes , ** kwargs , ) FeaturesAndLogitsHook \u00b6 Bases: FeaturesChainHook Chains together FeaturesHook and LogitsHook . Source code in pytorch_adapt\\hooks\\features.py 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 class FeaturesAndLogitsHook ( FeaturesChainHook ): \"\"\" Chains together [```FeaturesHook```][pytorch_adapt.hooks.FeaturesHook] and [```LogitsHook```][pytorch_adapt.hooks.LogitsHook]. \"\"\" def __init__ ( self , domains : List [ str ] = None , detach_features : bool = False , detach_logits : bool = False , other_hooks : List [ BaseHook ] = None , ** kwargs , ): \"\"\" Arguments: domains: The domains used by both the features and logits hooks. If ```None```, it defaults to ```[\"src\", \"target\"]``` detach_features: If ```True```, returns features that are detached from the autograd graph. detach_logits: If ```True```, returns logits that are detached from the autograd graph. other_hooks: A list of hooks that will be called after the features and logits hooks. \"\"\" features_hook = FeaturesHook ( detach = detach_features , domains = domains ) logits_hook = LogitsHook ( detach = detach_logits , domains = domains ) other_hooks = c_f . default ( other_hooks , []) super () . __init__ ( features_hook , logits_hook , * other_hooks , ** kwargs ) __init__ ( domains = None , detach_features = False , detach_logits = False , other_hooks = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default domains List [ str ] The domains used by both the features and logits hooks. If None , it defaults to [\"src\", \"target\"] None detach_features bool If True , returns features that are detached from the autograd graph. False detach_logits bool If True , returns logits that are detached from the autograd graph. False other_hooks List [ BaseHook ] A list of hooks that will be called after the features and logits hooks. None Source code in pytorch_adapt\\hooks\\features.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def __init__ ( self , domains : List [ str ] = None , detach_features : bool = False , detach_logits : bool = False , other_hooks : List [ BaseHook ] = None , ** kwargs , ): \"\"\" Arguments: domains: The domains used by both the features and logits hooks. If ```None```, it defaults to ```[\"src\", \"target\"]``` detach_features: If ```True```, returns features that are detached from the autograd graph. detach_logits: If ```True```, returns logits that are detached from the autograd graph. other_hooks: A list of hooks that will be called after the features and logits hooks. \"\"\" features_hook = FeaturesHook ( detach = detach_features , domains = domains ) logits_hook = LogitsHook ( detach = detach_logits , domains = domains ) other_hooks = c_f . default ( other_hooks , []) super () . __init__ ( features_hook , logits_hook , * other_hooks , ** kwargs ) FeaturesChainHook \u00b6 Bases: ChainHook A special ChainHook for features hooks. It sets each sub-hook's in_keys using the previous sub-hook's out_keys . Source code in pytorch_adapt\\hooks\\features.py 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 class FeaturesChainHook ( ChainHook ): \"\"\" A special [```ChainHook```][pytorch_adapt.hooks.ChainHook] for features hooks. It sets each sub-hook's ```in_keys``` using the previous sub-hook's ```out_keys```. \"\"\" def __init__ ( self , * hooks , ** kwargs , ): for i in range ( len ( hooks ) - 1 ): hooks [ i + 1 ] . set_in_keys ( hooks [ i ] . out_keys ) super () . __init__ ( * hooks , ** kwargs ) FeaturesHook \u00b6 Bases: BaseFeaturesHook Default input/output context names: Model: \"G\" Inputs: [\"src_imgs\", \"target_imgs\"] Outputs: [\"src_imgs_features\", \"target_imgs_features\"] Source code in pytorch_adapt\\hooks\\features.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 class FeaturesHook ( BaseFeaturesHook ): \"\"\" Default input/output context names: - Model: ```\"G\"``` - Inputs: ``` [\"src_imgs\", \"target_imgs\"] ``` - Outputs: ``` [\"src_imgs_features\", \"target_imgs_features\"] ``` \"\"\" def __init__ ( self , model_name = \"G\" , in_suffixes = None , out_suffixes = None , ** kwargs , ): in_suffixes = c_f . default ( in_suffixes , [ \"_imgs\" ]) out_suffixes = c_f . default ( out_suffixes , [ \"_features\" ]) super () . __init__ ( model_name = model_name , in_suffixes = in_suffixes , out_suffixes = out_suffixes , ** kwargs , ) FeaturesWithGradAndDetachedHook \u00b6 Bases: BaseWrapperHook Default input/output context names: Model: \"G\" Inputs: [\"src_imgs\", \"target_imgs\"] Outputs: [\"src_imgs_features\", \"target_imgs_features\", \"src_imgs_features_detached\", \"target_imgs_features_detached\"] Source code in pytorch_adapt\\hooks\\features.py 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 class FeaturesWithGradAndDetachedHook ( BaseWrapperHook ): \"\"\" Default input/output context names: - Model: ```\"G\"``` - Inputs: ``` [\"src_imgs\", \"target_imgs\"] ``` - Outputs: ``` [\"src_imgs_features\", \"target_imgs_features\", \"src_imgs_features_detached\", \"target_imgs_features_detached\"] ``` \"\"\" def __init__ ( self , model_name = \"G\" , in_suffixes = None , out_suffixes = None , domains = None , ** kwargs , ): super () . __init__ ( ** kwargs ) hooks = [] for detach in [ False , True ]: hooks . append ( FeaturesHook ( model_name = model_name , in_suffixes = in_suffixes , out_suffixes = out_suffixes , domains = domains , detach = detach , ** kwargs , ) ) self . hook = ChainHook ( * hooks ) FrozenModelHook \u00b6 Bases: BaseWrapperHook Sets model to eval() mode, and does all computations with gradients turned off. Source code in pytorch_adapt\\hooks\\features.py 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 class FrozenModelHook ( BaseWrapperHook ): \"\"\" Sets model to ```eval()``` mode, and does all computations with gradients turned off. \"\"\" def __init__ ( self , hook : BaseHook , model_name : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook which computes all losses and outputs. model_name: The name of the model that will be set to eval() mode. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . model_name = model_name def call ( self , inputs , losses ): \"\"\"\"\"\" model = inputs [ self . model_name ] model . eval () with torch . no_grad (): return self . hook ( inputs , losses ) __init__ ( hook , model_name , ** kwargs ) \u00b6 Parameters: Name Type Description Default hook BaseHook The wrapped hook which computes all losses and outputs. required model_name str The name of the model that will be set to eval() mode. required Source code in pytorch_adapt\\hooks\\features.py 472 473 474 475 476 477 478 479 480 481 def __init__ ( self , hook : BaseHook , model_name : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook which computes all losses and outputs. model_name: The name of the model that will be set to eval() mode. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . model_name = model_name LogitsHook \u00b6 Bases: BaseFeaturesHook Default input/output context names: Model: \"C\" Inputs: [\"src_imgs_features\", \"target_imgs_features\"] Outputs: [\"src_imgs_features_logits\", \"target_imgs_features_logits\"] Source code in pytorch_adapt\\hooks\\features.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 class LogitsHook ( BaseFeaturesHook ): \"\"\" Default input/output context names: - Model: ```\"C\"``` - Inputs: ``` [\"src_imgs_features\", \"target_imgs_features\"] ``` - Outputs: ``` [\"src_imgs_features_logits\", \"target_imgs_features_logits\"] ``` \"\"\" def __init__ ( self , model_name = \"C\" , in_suffixes = None , out_suffixes = None , ** kwargs , ): in_suffixes = c_f . default ( in_suffixes , [ \"_imgs_features\" ]) out_suffixes = c_f . default ( out_suffixes , [ \"_logits\" ]) super () . __init__ ( model_name = model_name , in_suffixes = in_suffixes , out_suffixes = out_suffixes , ** kwargs , )","title":"features"},{"location":"docs/hooks/features/#pytorch_adapt.hooks.features.BaseFeaturesHook","text":"Bases: BaseHook This hook: Checks to see if specific tensors are in the context Exits if the tensors are already in the context Otherwise computes those tensors using the appropriate inputs and models, and adds them to the context. Source code in pytorch_adapt\\hooks\\features.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 class BaseFeaturesHook ( BaseHook ): \"\"\" This hook: 1. Checks to see if specific tensors are in the context 2. Exits if the tensors are already in the context 3. Otherwise computes those tensors using the appropriate inputs and models, and adds them to the context. \"\"\" def __init__ ( self , model_name : str , in_suffixes : List [ str ] = None , out_suffixes : List [ str ] = None , domains : List [ str ] = None , detach : bool = False , ** kwargs , ): \"\"\" Arguments: model_name: The name of the model that will be used to compute any missing tensors. in_suffixes: The suffixes of the names of the inputs to the model. For example if: - ```domains = [\"src\", \"target\"]``` - ```in_suffixes = [\"_imgs_features\"]``` then the model will be given - ```[\"src_imgs_features\", \"target_imgs_features\"]```. out_suffixes: The suffixes of the names of the outputs of the model. Output suffixes are appended to the input name. For example, if - ```domains = [\"src\", \"target\"]``` - ```in_suffixes = [\"_imgs_features\"]``` - ```out_suffixes = [\"_logits\"]``` then the output keys will be - ```[\"src_imgs_features_logits\", \"target_imgs_features_logits\"]``` domains: The names of the domains to use. If ```None```, this defaults to ```[\"src\", \"target\"]```. detach: If ```True```, then the output will be detached from the autograd graph. Any output that is detached will have ```\"_detached\"``` appended to its name in the context. \"\"\" super () . __init__ ( ** kwargs ) self . model_name = model_name self . domains = c_f . default ( domains , [ \"src\" , \"target\" ]) self . init_detach_mode ( detach ) self . init_suffixes ( in_suffixes , out_suffixes ) def call ( self , inputs , losses ): \"\"\"\"\"\" outputs = {} for domain in self . domains : self . logger ( f \"Getting { domain } \" ) detach = self . check_grad_mode ( domain ) func = self . mode_detached if detach else self . mode_with_grad in_keys = c_f . filter ( self . in_keys , f \"^ { domain } \" ) func ( inputs , outputs , domain , in_keys ) self . check_outputs_requires_grad ( outputs ) return outputs , {} def check_grad_mode ( self , domain ): detach = self . detach [ domain ] if not torch . is_grad_enabled (): if not detach : raise ValueError ( f \"detach[ { domain } ] == { detach } but grad is not enabled\" ) return detach def check_outputs_requires_grad ( self , outputs ): for k , v in outputs . items (): if k . endswith ( \"detached\" ) and c_f . requires_grad ( v , does = True ): raise TypeError ( f \" { k } ends with 'detached' but tensor requires grad\" ) if not k . endswith ( \"detached\" ) and c_f . requires_grad ( v , does = False ): raise TypeError ( f \" { k } doesn't end in 'detached' but tensor doesn't require grad\" ) def mode_with_grad ( self , inputs , outputs , domain , in_keys ): output_keys = c_f . filter ( self . _out_keys (), f \"^ { domain } \" ) output_vals = self . get_kwargs ( inputs , output_keys ) self . add_if_new ( outputs , output_keys , output_vals , inputs , self . model_name , in_keys , domain ) return output_keys , output_vals def mode_detached ( self , inputs , outputs , domain , in_keys ): curr_out_keys = c_f . filter ( self . _out_keys (), f \"^ { domain } \" ) self . try_existing_detachable ( inputs , outputs , curr_out_keys ) remaining_out_keys = [ k for k in curr_out_keys if k not in set () . union ( inputs , outputs ) ] if len ( remaining_out_keys ) > 0 : output_vals = self . get_kwargs ( inputs , remaining_out_keys ) with torch . no_grad (): self . add_if_new ( outputs , remaining_out_keys , output_vals , inputs , self . model_name , in_keys , domain , ) def add_if_new ( self , outputs , full_key , output_vals , inputs , model_name , in_keys , domain ): c_f . add_if_new ( outputs , full_key , output_vals , inputs , model_name , in_keys , logger = self . logger , ) def create_keys ( self , domain , suffix , starting_keys = None , detach = False ): if starting_keys is None : full_keys = [ f \" { domain }{ x } \" for x in suffix ] else : if len ( starting_keys ) > 1 : starting_keys = self . join_keys ( starting_keys ) if len ( suffix ) > 1 : starting_keys = starting_keys * len ( suffix ) full_keys = [ f \" { k }{ x } \" for k , x in zip ( starting_keys , suffix )] if detach : full_keys = self . add_detached_string ( full_keys ) return full_keys def get_kwargs ( self , inputs , keys ): return [ inputs . get ( k ) for k in keys ] def try_existing_detachable ( self , inputs , outputs , curr_out_keys ): for k in curr_out_keys : if k in inputs or k in outputs : continue curr_regex = self . detachable_regex [ k ] success = self . try_existing_detachable_in_dict ( curr_regex , inputs , outputs , k ) if not success : self . try_existing_detachable_in_dict ( curr_regex , outputs , outputs , k ) def try_existing_detachable_in_dict ( self , regex , in_dict , outputs , new_k ): for k , v in in_dict . items (): if regex . search ( k ) and v is not None : outputs [ new_k ] = v . detach () return True return False def add_detached_string ( self , keys ): # delete existing detached string, then append to the very end # for example, if computing detached logits for: src_imgs_features_detached # 1. src_imgs_features_detached_logits --> src_imgs_features_logits # 2. src_imgs_features_logits --> src_imgs_features_logits_detached keys = [ k . replace ( \"_detached\" , \"\" ) for k in keys ] return [ f \" { k } _detached\" for k in keys ] def join_keys ( self , keys ): return [ \"_AND_\" . join ( keys )] def init_detach_mode ( self , detach ): if isinstance ( detach , dict ): if any ( not isinstance ( v , bool ) for v in detach . values ()): raise TypeError ( \"if detach is a dict, values must be bools\" ) self . detach = detach elif isinstance ( detach , bool ): self . detach = { k : detach for k in self . domains } else : raise TypeError ( \"detach must be a bool or a dict of bools\" ) def init_suffixes ( self , in_suffixes , out_suffixes ): self . in_suffixes = in_suffixes self . out_suffixes = out_suffixes in_keys = [] for domain in self . domains : in_keys . extend ( self . create_keys ( domain , in_suffixes )) self . set_in_keys ( in_keys ) def set_in_keys ( self , in_keys ): super () . set_in_keys ( in_keys ) self . all_out_keys = [] for domain in self . domains : curr_in_keys = c_f . filter ( self . in_keys , f \"^ { domain } \" ) curr_out_keys = self . create_keys ( domain , self . out_suffixes , curr_in_keys , detach = self . detach [ domain ] ) self . all_out_keys . extend ( curr_out_keys ) # strings with '_detached' optional and anywhere self . detachable_regex = { k : re . compile ( f \"^ { k . replace ( '_detached' , '' ) . replace ( '_' , '(_detached)?_' ) } $\" ) for k in self . all_out_keys } def _loss_keys ( self ): \"\"\"\"\"\" return [] def _out_keys ( self ): \"\"\"\"\"\" return self . all_out_keys def extra_repr ( self ): return c_f . extra_repr ( self , [ \"model_name\" , \"domains\" , \"detach\" ])","title":"BaseFeaturesHook"},{"location":"docs/hooks/features/#pytorch_adapt.hooks.features.CombinedFeaturesHook","text":"Bases: BaseFeaturesHook Default input/output context names: Model: \"feature_combiner\" Inputs: [\"src_imgs_features\", \"src_imgs_features_logits\", \"target_imgs_features\", \"target_imgs_features_logits\"] Outputs: [\"src_imgs_features_AND_src_imgs_features_logits_combined\", \"target_imgs_features_AND_target_imgs_features_logits_combined\"] Source code in pytorch_adapt\\hooks\\features.py 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 class CombinedFeaturesHook ( BaseFeaturesHook ): \"\"\" Default input/output context names: - Model: ```\"feature_combiner\"``` - Inputs: ``` [\"src_imgs_features\", \"src_imgs_features_logits\", \"target_imgs_features\", \"target_imgs_features_logits\"] ``` - Outputs: ``` [\"src_imgs_features_AND_src_imgs_features_logits_combined\", \"target_imgs_features_AND_target_imgs_features_logits_combined\"] ``` \"\"\" def __init__ ( self , in_suffixes = None , out_suffixes = None , ** kwargs , ): in_suffixes = c_f . default ( in_suffixes , [ \"_imgs_features\" , \"_imgs_features_logits\" ] ) out_suffixes = c_f . default ( out_suffixes , [ \"_combined\" ]) super () . __init__ ( model_name = \"feature_combiner\" , in_suffixes = in_suffixes , out_suffixes = out_suffixes , ** kwargs , )","title":"CombinedFeaturesHook"},{"location":"docs/hooks/features/#pytorch_adapt.hooks.features.DLogitsHook","text":"Bases: BaseFeaturesHook Default input/output context names: Model: \"D\" Inputs: [\"src_imgs_features\", \"target_imgs_features\"] Outputs: [\"src_imgs_features_dlogits\", \"target_imgs_features_dlogits\"] Source code in pytorch_adapt\\hooks\\features.py 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 class DLogitsHook ( BaseFeaturesHook ): \"\"\" Default input/output context names: - Model: ```\"D\"``` - Inputs: ``` [\"src_imgs_features\", \"target_imgs_features\"] ``` - Outputs: ``` [\"src_imgs_features_dlogits\", \"target_imgs_features_dlogits\"] ``` \"\"\" def __init__ ( self , model_name = \"D\" , in_suffixes = None , out_suffixes = None , ** kwargs , ): in_suffixes = c_f . default ( in_suffixes , [ \"_imgs_features\" ]) out_suffixes = c_f . default ( out_suffixes , [ \"_dlogits\" ]) super () . __init__ ( model_name = model_name , in_suffixes = in_suffixes , out_suffixes = out_suffixes , ** kwargs , )","title":"DLogitsHook"},{"location":"docs/hooks/features/#pytorch_adapt.hooks.features.FeaturesAndLogitsHook","text":"Bases: FeaturesChainHook Chains together FeaturesHook and LogitsHook . Source code in pytorch_adapt\\hooks\\features.py 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 class FeaturesAndLogitsHook ( FeaturesChainHook ): \"\"\" Chains together [```FeaturesHook```][pytorch_adapt.hooks.FeaturesHook] and [```LogitsHook```][pytorch_adapt.hooks.LogitsHook]. \"\"\" def __init__ ( self , domains : List [ str ] = None , detach_features : bool = False , detach_logits : bool = False , other_hooks : List [ BaseHook ] = None , ** kwargs , ): \"\"\" Arguments: domains: The domains used by both the features and logits hooks. If ```None```, it defaults to ```[\"src\", \"target\"]``` detach_features: If ```True```, returns features that are detached from the autograd graph. detach_logits: If ```True```, returns logits that are detached from the autograd graph. other_hooks: A list of hooks that will be called after the features and logits hooks. \"\"\" features_hook = FeaturesHook ( detach = detach_features , domains = domains ) logits_hook = LogitsHook ( detach = detach_logits , domains = domains ) other_hooks = c_f . default ( other_hooks , []) super () . __init__ ( features_hook , logits_hook , * other_hooks , ** kwargs )","title":"FeaturesAndLogitsHook"},{"location":"docs/hooks/features/#pytorch_adapt.hooks.features.FeaturesChainHook","text":"Bases: ChainHook A special ChainHook for features hooks. It sets each sub-hook's in_keys using the previous sub-hook's out_keys . Source code in pytorch_adapt\\hooks\\features.py 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 class FeaturesChainHook ( ChainHook ): \"\"\" A special [```ChainHook```][pytorch_adapt.hooks.ChainHook] for features hooks. It sets each sub-hook's ```in_keys``` using the previous sub-hook's ```out_keys```. \"\"\" def __init__ ( self , * hooks , ** kwargs , ): for i in range ( len ( hooks ) - 1 ): hooks [ i + 1 ] . set_in_keys ( hooks [ i ] . out_keys ) super () . __init__ ( * hooks , ** kwargs )","title":"FeaturesChainHook"},{"location":"docs/hooks/features/#pytorch_adapt.hooks.features.FeaturesHook","text":"Bases: BaseFeaturesHook Default input/output context names: Model: \"G\" Inputs: [\"src_imgs\", \"target_imgs\"] Outputs: [\"src_imgs_features\", \"target_imgs_features\"] Source code in pytorch_adapt\\hooks\\features.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 class FeaturesHook ( BaseFeaturesHook ): \"\"\" Default input/output context names: - Model: ```\"G\"``` - Inputs: ``` [\"src_imgs\", \"target_imgs\"] ``` - Outputs: ``` [\"src_imgs_features\", \"target_imgs_features\"] ``` \"\"\" def __init__ ( self , model_name = \"G\" , in_suffixes = None , out_suffixes = None , ** kwargs , ): in_suffixes = c_f . default ( in_suffixes , [ \"_imgs\" ]) out_suffixes = c_f . default ( out_suffixes , [ \"_features\" ]) super () . __init__ ( model_name = model_name , in_suffixes = in_suffixes , out_suffixes = out_suffixes , ** kwargs , )","title":"FeaturesHook"},{"location":"docs/hooks/features/#pytorch_adapt.hooks.features.FeaturesWithGradAndDetachedHook","text":"Bases: BaseWrapperHook Default input/output context names: Model: \"G\" Inputs: [\"src_imgs\", \"target_imgs\"] Outputs: [\"src_imgs_features\", \"target_imgs_features\", \"src_imgs_features_detached\", \"target_imgs_features_detached\"] Source code in pytorch_adapt\\hooks\\features.py 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 class FeaturesWithGradAndDetachedHook ( BaseWrapperHook ): \"\"\" Default input/output context names: - Model: ```\"G\"``` - Inputs: ``` [\"src_imgs\", \"target_imgs\"] ``` - Outputs: ``` [\"src_imgs_features\", \"target_imgs_features\", \"src_imgs_features_detached\", \"target_imgs_features_detached\"] ``` \"\"\" def __init__ ( self , model_name = \"G\" , in_suffixes = None , out_suffixes = None , domains = None , ** kwargs , ): super () . __init__ ( ** kwargs ) hooks = [] for detach in [ False , True ]: hooks . append ( FeaturesHook ( model_name = model_name , in_suffixes = in_suffixes , out_suffixes = out_suffixes , domains = domains , detach = detach , ** kwargs , ) ) self . hook = ChainHook ( * hooks )","title":"FeaturesWithGradAndDetachedHook"},{"location":"docs/hooks/features/#pytorch_adapt.hooks.features.FrozenModelHook","text":"Bases: BaseWrapperHook Sets model to eval() mode, and does all computations with gradients turned off. Source code in pytorch_adapt\\hooks\\features.py 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 class FrozenModelHook ( BaseWrapperHook ): \"\"\" Sets model to ```eval()``` mode, and does all computations with gradients turned off. \"\"\" def __init__ ( self , hook : BaseHook , model_name : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook which computes all losses and outputs. model_name: The name of the model that will be set to eval() mode. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . model_name = model_name def call ( self , inputs , losses ): \"\"\"\"\"\" model = inputs [ self . model_name ] model . eval () with torch . no_grad (): return self . hook ( inputs , losses )","title":"FrozenModelHook"},{"location":"docs/hooks/features/#pytorch_adapt.hooks.features.LogitsHook","text":"Bases: BaseFeaturesHook Default input/output context names: Model: \"C\" Inputs: [\"src_imgs_features\", \"target_imgs_features\"] Outputs: [\"src_imgs_features_logits\", \"target_imgs_features_logits\"] Source code in pytorch_adapt\\hooks\\features.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 class LogitsHook ( BaseFeaturesHook ): \"\"\" Default input/output context names: - Model: ```\"C\"``` - Inputs: ``` [\"src_imgs_features\", \"target_imgs_features\"] ``` - Outputs: ``` [\"src_imgs_features_logits\", \"target_imgs_features_logits\"] ``` \"\"\" def __init__ ( self , model_name = \"C\" , in_suffixes = None , out_suffixes = None , ** kwargs , ): in_suffixes = c_f . default ( in_suffixes , [ \"_imgs_features\" ]) out_suffixes = c_f . default ( out_suffixes , [ \"_logits\" ]) super () . __init__ ( model_name = model_name , in_suffixes = in_suffixes , out_suffixes = out_suffixes , ** kwargs , )","title":"LogitsHook"},{"location":"docs/hooks/gan/","text":"GANHook \u00b6 Bases: BaseWrapperHook A generic GAN architecture for domain adaptation. This includes the model optimization steps. Source code in pytorch_adapt\\hooks\\gan.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 class GANHook ( BaseWrapperHook ): \"\"\" A generic GAN architecture for domain adaptation. This includes the model optimization steps. \"\"\" def __init__ ( self , d_opts , g_opts , d_weighter = None , d_reducer = None , g_weighter = None , g_reducer = None , pre_d = None , post_d = None , pre_g = None , post_g = None , use_logits = False , disc_hook = None , gen_hook = None , disc_f_hook = None , gen_f_hook = None , disc_d_hook = None , gen_d_hook = None , c_hook = None , disc_conditions = None , disc_alts = None , gen_conditions = None , gen_alts = None , disc_domains = None , gen_domains = None , disc_domain_loss_fn = None , gen_domain_loss_fn = None , ** kwargs ): \"\"\" Arguments: d_opts: List of optimizers for the D phase. g_opts: List of optimizers for the G phase. d_weighter: A loss weighter for the D phase. If ```None``` then [```MeanWeighter```][pytorch_adapt.weighters.MeanWeighter] is used. d_reducer: A loss reducer for the D phase. If ```None``` then [```MeanReducer```][pytorch_adapt.hooks.MeanReducer] is used. g_weighter: A loss weighter for the G phase. If ```None``` then [```MeanWeighter```][pytorch_adapt.weighters.MeanWeighter] is used. g_reducer: A loss reducer for the G phase. If ```None``` then [```MeanReducer```][pytorch_adapt.hooks.MeanReducer] is used. pre_d: List of hooks that will be executed at the very beginning of the D phase. post_d: List of hooks that will be executed at the end of the D phase, but before the optimizers are called. pre_g: List of hooks that will be executed at the very beginning of the G phase. post_g: List of hooks that will be executed at the end of the G phase, but before the optimizers are called. use_logits: If ```True```, then D receives the output of C instead of the output of G. disc_hook: The hook used for computing the discriminator's domain loss. If ```None``` then [```DomainLossHook```][pytorch_adapt.hooks.DomainLossHook] is used. gen_hook: The hook used for computing the generator's domain loss. If ```None``` then [```DomainLossHook```][pytorch_adapt.hooks.DomainLossHook] is used. c_hook: The hook used for computing the classifiers's loss. If ```None``` then [```CLossHook```][pytorch_adapt.hooks.CLossHook] is used. disc_conditions: The condition hooks used in the [```ChainHook```][pytorch_adapt.hooks.ChainHook] for the D phase. disc_alts: The alt hooks used in the [```ChainHook```][pytorch_adapt.hooks.ChainHook] for the D phase. gen_conditions: The condition hooks used in the [```ChainHook```][pytorch_adapt.hooks.ChainHook] for the G phase. gen_alts: The alt hooks used in the [```ChainHook```][pytorch_adapt.hooks.ChainHook] for the G phase. disc_domains: The domains used to compute the discriminator's domain loss. If ```None```, then ```[\"src\", \"target\"]``` is used. gen_domains: The domains used to compute the generators's domain loss. If ```None```, then ```[\"src\", \"target\"]``` is used. disc_domain_loss_fn: The loss function used to compute the discriminator's domain loss. If ```None``` then ```torch.nn.BCEWithLogitsLoss``` is used. gen_domain_loss_fn: The loss function used to compute the generator's domain loss. If ```None``` then ```torch.nn.BCEWithLogitsLoss``` is used. \"\"\" super () . __init__ ( ** kwargs ) [ pre_d , post_d , pre_g , post_g ] = c_f . many_default ( [ pre_d , post_d , pre_g , post_g ], [[], [], [], []] ) disc_f_hook = c_f . default ( disc_f_hook , FeaturesForDomainLossHook , { \"detach\" : True , \"use_logits\" : use_logits , \"domains\" : disc_domains }, ) gen_f_hook = c_f . default ( gen_f_hook , FeaturesForDomainLossHook , { \"use_logits\" : use_logits , \"domains\" : gen_domains }, ) c_hook = c_f . default ( c_hook , CLossHook , {}) disc_hook = c_f . default ( disc_hook , DomainLossHook , { \"d_loss_fn\" : disc_domain_loss_fn , \"loss_prefix\" : \"d_\" , \"detach_features\" : True , \"f_hook\" : disc_f_hook , \"d_hook\" : disc_d_hook , \"domains\" : disc_domains , }, ) gen_hook = c_f . default ( gen_hook , DomainLossHook , { \"d_loss_fn\" : gen_domain_loss_fn , \"loss_prefix\" : \"g_\" , \"reverse_labels\" : True , \"f_hook\" : gen_f_hook , \"d_hook\" : gen_d_hook , \"domains\" : gen_domains , }, ) # use gen_f_hook to get undetached features first disc_hook = ChainHook ( * pre_d , gen_f_hook , disc_hook , * post_d , conditions = disc_conditions , alts = disc_alts ) gen_hook = ChainHook ( * pre_g , gen_hook , c_hook , * post_g , conditions = gen_conditions , alts = gen_alts ) disc_hook = OptimizerHook ( disc_hook , d_opts , d_weighter , d_reducer ) gen_hook = OptimizerHook ( gen_hook , g_opts , g_weighter , g_reducer ) s_hook = SummaryHook ({ \"d_loss\" : disc_hook , \"g_loss\" : gen_hook }) self . hook = ChainHook ( disc_hook , gen_hook , s_hook ) __init__ ( d_opts , g_opts , d_weighter = None , d_reducer = None , g_weighter = None , g_reducer = None , pre_d = None , post_d = None , pre_g = None , post_g = None , use_logits = False , disc_hook = None , gen_hook = None , disc_f_hook = None , gen_f_hook = None , disc_d_hook = None , gen_d_hook = None , c_hook = None , disc_conditions = None , disc_alts = None , gen_conditions = None , gen_alts = None , disc_domains = None , gen_domains = None , disc_domain_loss_fn = None , gen_domain_loss_fn = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default d_opts List of optimizers for the D phase. required g_opts List of optimizers for the G phase. required d_weighter A loss weighter for the D phase. If None then MeanWeighter is used. None d_reducer A loss reducer for the D phase. If None then MeanReducer is used. None g_weighter A loss weighter for the G phase. If None then MeanWeighter is used. None g_reducer A loss reducer for the G phase. If None then MeanReducer is used. None pre_d List of hooks that will be executed at the very beginning of the D phase. None post_d List of hooks that will be executed at the end of the D phase, but before the optimizers are called. None pre_g List of hooks that will be executed at the very beginning of the G phase. None post_g List of hooks that will be executed at the end of the G phase, but before the optimizers are called. None use_logits If True , then D receives the output of C instead of the output of G. False disc_hook The hook used for computing the discriminator's domain loss. If None then DomainLossHook is used. None gen_hook The hook used for computing the generator's domain loss. If None then DomainLossHook is used. None c_hook The hook used for computing the classifiers's loss. If None then CLossHook is used. None disc_conditions The condition hooks used in the ChainHook for the D phase. None disc_alts The alt hooks used in the ChainHook for the D phase. None gen_conditions The condition hooks used in the ChainHook for the G phase. None gen_alts The alt hooks used in the ChainHook for the G phase. None disc_domains The domains used to compute the discriminator's domain loss. If None , then [\"src\", \"target\"] is used. None gen_domains The domains used to compute the generators's domain loss. If None , then [\"src\", \"target\"] is used. None disc_domain_loss_fn The loss function used to compute the discriminator's domain loss. If None then torch.nn.BCEWithLogitsLoss is used. None gen_domain_loss_fn The loss function used to compute the generator's domain loss. If None then torch.nn.BCEWithLogitsLoss is used. None Source code in pytorch_adapt\\hooks\\gan.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def __init__ ( self , d_opts , g_opts , d_weighter = None , d_reducer = None , g_weighter = None , g_reducer = None , pre_d = None , post_d = None , pre_g = None , post_g = None , use_logits = False , disc_hook = None , gen_hook = None , disc_f_hook = None , gen_f_hook = None , disc_d_hook = None , gen_d_hook = None , c_hook = None , disc_conditions = None , disc_alts = None , gen_conditions = None , gen_alts = None , disc_domains = None , gen_domains = None , disc_domain_loss_fn = None , gen_domain_loss_fn = None , ** kwargs ): \"\"\" Arguments: d_opts: List of optimizers for the D phase. g_opts: List of optimizers for the G phase. d_weighter: A loss weighter for the D phase. If ```None``` then [```MeanWeighter```][pytorch_adapt.weighters.MeanWeighter] is used. d_reducer: A loss reducer for the D phase. If ```None``` then [```MeanReducer```][pytorch_adapt.hooks.MeanReducer] is used. g_weighter: A loss weighter for the G phase. If ```None``` then [```MeanWeighter```][pytorch_adapt.weighters.MeanWeighter] is used. g_reducer: A loss reducer for the G phase. If ```None``` then [```MeanReducer```][pytorch_adapt.hooks.MeanReducer] is used. pre_d: List of hooks that will be executed at the very beginning of the D phase. post_d: List of hooks that will be executed at the end of the D phase, but before the optimizers are called. pre_g: List of hooks that will be executed at the very beginning of the G phase. post_g: List of hooks that will be executed at the end of the G phase, but before the optimizers are called. use_logits: If ```True```, then D receives the output of C instead of the output of G. disc_hook: The hook used for computing the discriminator's domain loss. If ```None``` then [```DomainLossHook```][pytorch_adapt.hooks.DomainLossHook] is used. gen_hook: The hook used for computing the generator's domain loss. If ```None``` then [```DomainLossHook```][pytorch_adapt.hooks.DomainLossHook] is used. c_hook: The hook used for computing the classifiers's loss. If ```None``` then [```CLossHook```][pytorch_adapt.hooks.CLossHook] is used. disc_conditions: The condition hooks used in the [```ChainHook```][pytorch_adapt.hooks.ChainHook] for the D phase. disc_alts: The alt hooks used in the [```ChainHook```][pytorch_adapt.hooks.ChainHook] for the D phase. gen_conditions: The condition hooks used in the [```ChainHook```][pytorch_adapt.hooks.ChainHook] for the G phase. gen_alts: The alt hooks used in the [```ChainHook```][pytorch_adapt.hooks.ChainHook] for the G phase. disc_domains: The domains used to compute the discriminator's domain loss. If ```None```, then ```[\"src\", \"target\"]``` is used. gen_domains: The domains used to compute the generators's domain loss. If ```None```, then ```[\"src\", \"target\"]``` is used. disc_domain_loss_fn: The loss function used to compute the discriminator's domain loss. If ```None``` then ```torch.nn.BCEWithLogitsLoss``` is used. gen_domain_loss_fn: The loss function used to compute the generator's domain loss. If ```None``` then ```torch.nn.BCEWithLogitsLoss``` is used. \"\"\" super () . __init__ ( ** kwargs ) [ pre_d , post_d , pre_g , post_g ] = c_f . many_default ( [ pre_d , post_d , pre_g , post_g ], [[], [], [], []] ) disc_f_hook = c_f . default ( disc_f_hook , FeaturesForDomainLossHook , { \"detach\" : True , \"use_logits\" : use_logits , \"domains\" : disc_domains }, ) gen_f_hook = c_f . default ( gen_f_hook , FeaturesForDomainLossHook , { \"use_logits\" : use_logits , \"domains\" : gen_domains }, ) c_hook = c_f . default ( c_hook , CLossHook , {}) disc_hook = c_f . default ( disc_hook , DomainLossHook , { \"d_loss_fn\" : disc_domain_loss_fn , \"loss_prefix\" : \"d_\" , \"detach_features\" : True , \"f_hook\" : disc_f_hook , \"d_hook\" : disc_d_hook , \"domains\" : disc_domains , }, ) gen_hook = c_f . default ( gen_hook , DomainLossHook , { \"d_loss_fn\" : gen_domain_loss_fn , \"loss_prefix\" : \"g_\" , \"reverse_labels\" : True , \"f_hook\" : gen_f_hook , \"d_hook\" : gen_d_hook , \"domains\" : gen_domains , }, ) # use gen_f_hook to get undetached features first disc_hook = ChainHook ( * pre_d , gen_f_hook , disc_hook , * post_d , conditions = disc_conditions , alts = disc_alts ) gen_hook = ChainHook ( * pre_g , gen_hook , c_hook , * post_g , conditions = gen_conditions , alts = gen_alts ) disc_hook = OptimizerHook ( disc_hook , d_opts , d_weighter , d_reducer ) gen_hook = OptimizerHook ( gen_hook , g_opts , g_weighter , g_reducer ) s_hook = SummaryHook ({ \"d_loss\" : disc_hook , \"g_loss\" : gen_hook }) self . hook = ChainHook ( disc_hook , gen_hook , s_hook )","title":"gan"},{"location":"docs/hooks/gan/#pytorch_adapt.hooks.gan.GANHook","text":"Bases: BaseWrapperHook A generic GAN architecture for domain adaptation. This includes the model optimization steps. Source code in pytorch_adapt\\hooks\\gan.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 class GANHook ( BaseWrapperHook ): \"\"\" A generic GAN architecture for domain adaptation. This includes the model optimization steps. \"\"\" def __init__ ( self , d_opts , g_opts , d_weighter = None , d_reducer = None , g_weighter = None , g_reducer = None , pre_d = None , post_d = None , pre_g = None , post_g = None , use_logits = False , disc_hook = None , gen_hook = None , disc_f_hook = None , gen_f_hook = None , disc_d_hook = None , gen_d_hook = None , c_hook = None , disc_conditions = None , disc_alts = None , gen_conditions = None , gen_alts = None , disc_domains = None , gen_domains = None , disc_domain_loss_fn = None , gen_domain_loss_fn = None , ** kwargs ): \"\"\" Arguments: d_opts: List of optimizers for the D phase. g_opts: List of optimizers for the G phase. d_weighter: A loss weighter for the D phase. If ```None``` then [```MeanWeighter```][pytorch_adapt.weighters.MeanWeighter] is used. d_reducer: A loss reducer for the D phase. If ```None``` then [```MeanReducer```][pytorch_adapt.hooks.MeanReducer] is used. g_weighter: A loss weighter for the G phase. If ```None``` then [```MeanWeighter```][pytorch_adapt.weighters.MeanWeighter] is used. g_reducer: A loss reducer for the G phase. If ```None``` then [```MeanReducer```][pytorch_adapt.hooks.MeanReducer] is used. pre_d: List of hooks that will be executed at the very beginning of the D phase. post_d: List of hooks that will be executed at the end of the D phase, but before the optimizers are called. pre_g: List of hooks that will be executed at the very beginning of the G phase. post_g: List of hooks that will be executed at the end of the G phase, but before the optimizers are called. use_logits: If ```True```, then D receives the output of C instead of the output of G. disc_hook: The hook used for computing the discriminator's domain loss. If ```None``` then [```DomainLossHook```][pytorch_adapt.hooks.DomainLossHook] is used. gen_hook: The hook used for computing the generator's domain loss. If ```None``` then [```DomainLossHook```][pytorch_adapt.hooks.DomainLossHook] is used. c_hook: The hook used for computing the classifiers's loss. If ```None``` then [```CLossHook```][pytorch_adapt.hooks.CLossHook] is used. disc_conditions: The condition hooks used in the [```ChainHook```][pytorch_adapt.hooks.ChainHook] for the D phase. disc_alts: The alt hooks used in the [```ChainHook```][pytorch_adapt.hooks.ChainHook] for the D phase. gen_conditions: The condition hooks used in the [```ChainHook```][pytorch_adapt.hooks.ChainHook] for the G phase. gen_alts: The alt hooks used in the [```ChainHook```][pytorch_adapt.hooks.ChainHook] for the G phase. disc_domains: The domains used to compute the discriminator's domain loss. If ```None```, then ```[\"src\", \"target\"]``` is used. gen_domains: The domains used to compute the generators's domain loss. If ```None```, then ```[\"src\", \"target\"]``` is used. disc_domain_loss_fn: The loss function used to compute the discriminator's domain loss. If ```None``` then ```torch.nn.BCEWithLogitsLoss``` is used. gen_domain_loss_fn: The loss function used to compute the generator's domain loss. If ```None``` then ```torch.nn.BCEWithLogitsLoss``` is used. \"\"\" super () . __init__ ( ** kwargs ) [ pre_d , post_d , pre_g , post_g ] = c_f . many_default ( [ pre_d , post_d , pre_g , post_g ], [[], [], [], []] ) disc_f_hook = c_f . default ( disc_f_hook , FeaturesForDomainLossHook , { \"detach\" : True , \"use_logits\" : use_logits , \"domains\" : disc_domains }, ) gen_f_hook = c_f . default ( gen_f_hook , FeaturesForDomainLossHook , { \"use_logits\" : use_logits , \"domains\" : gen_domains }, ) c_hook = c_f . default ( c_hook , CLossHook , {}) disc_hook = c_f . default ( disc_hook , DomainLossHook , { \"d_loss_fn\" : disc_domain_loss_fn , \"loss_prefix\" : \"d_\" , \"detach_features\" : True , \"f_hook\" : disc_f_hook , \"d_hook\" : disc_d_hook , \"domains\" : disc_domains , }, ) gen_hook = c_f . default ( gen_hook , DomainLossHook , { \"d_loss_fn\" : gen_domain_loss_fn , \"loss_prefix\" : \"g_\" , \"reverse_labels\" : True , \"f_hook\" : gen_f_hook , \"d_hook\" : gen_d_hook , \"domains\" : gen_domains , }, ) # use gen_f_hook to get undetached features first disc_hook = ChainHook ( * pre_d , gen_f_hook , disc_hook , * post_d , conditions = disc_conditions , alts = disc_alts ) gen_hook = ChainHook ( * pre_g , gen_hook , c_hook , * post_g , conditions = gen_conditions , alts = gen_alts ) disc_hook = OptimizerHook ( disc_hook , d_opts , d_weighter , d_reducer ) gen_hook = OptimizerHook ( gen_hook , g_opts , g_weighter , g_reducer ) s_hook = SummaryHook ({ \"d_loss\" : disc_hook , \"g_loss\" : gen_hook }) self . hook = ChainHook ( disc_hook , gen_hook , s_hook )","title":"GANHook"},{"location":"docs/hooks/gvb/","text":"GVBHook \u00b6 Bases: DANNHook Implementation of Gradually Vanishing Bridge for Adversarial Domain Adaptation Source code in pytorch_adapt\\hooks\\gvb.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 class GVBHook ( DANNHook ): \"\"\" Implementation of [Gradually Vanishing Bridge for Adversarial Domain Adaptation](https://arxiv.org/abs/2003.13183) \"\"\" def __init__ ( self , gradient_reversal_weight = 1 , pre = None , pre_d = None , pre_g = None , ** kwargs ): # f_hook and d_hook are used inside DomainLossHook f_hook = FeaturesForDomainLossHook ( use_logits = True ) d_hook = DBridgeAndLogitsHook () apply_to = c_f . filter ( f_hook . out_keys , \"_logits$\" ) gradient_reversal = SoftmaxGradientReversalHook ( weight = gradient_reversal_weight , apply_to = apply_to ) [ pre , pre_d , pre_g ] = c_f . many_default ([ pre , pre_d , pre_g ], [[], [], []]) pre += [ FeaturesLogitsAndGBridge ()] pre_d += [ DBridgeLossHook ()] pre_g += [ GBridgeLossHook ()] super () . __init__ ( pre = pre , pre_d = pre_d , pre_g = pre_g , gradient_reversal = gradient_reversal , f_hook = f_hook , d_hook = d_hook , d_hook_allowed = \"_dlogits$|_dbridge$\" , ** kwargs , )","title":"gvb"},{"location":"docs/hooks/gvb/#pytorch_adapt.hooks.gvb.GVBHook","text":"Bases: DANNHook Implementation of Gradually Vanishing Bridge for Adversarial Domain Adaptation Source code in pytorch_adapt\\hooks\\gvb.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 class GVBHook ( DANNHook ): \"\"\" Implementation of [Gradually Vanishing Bridge for Adversarial Domain Adaptation](https://arxiv.org/abs/2003.13183) \"\"\" def __init__ ( self , gradient_reversal_weight = 1 , pre = None , pre_d = None , pre_g = None , ** kwargs ): # f_hook and d_hook are used inside DomainLossHook f_hook = FeaturesForDomainLossHook ( use_logits = True ) d_hook = DBridgeAndLogitsHook () apply_to = c_f . filter ( f_hook . out_keys , \"_logits$\" ) gradient_reversal = SoftmaxGradientReversalHook ( weight = gradient_reversal_weight , apply_to = apply_to ) [ pre , pre_d , pre_g ] = c_f . many_default ([ pre , pre_d , pre_g ], [[], [], []]) pre += [ FeaturesLogitsAndGBridge ()] pre_d += [ DBridgeLossHook ()] pre_g += [ GBridgeLossHook ()] super () . __init__ ( pre = pre , pre_d = pre_d , pre_g = pre_g , gradient_reversal = gradient_reversal , f_hook = f_hook , d_hook = d_hook , d_hook_allowed = \"_dlogits$|_dbridge$\" , ** kwargs , )","title":"GVBHook"},{"location":"docs/hooks/mcd/","text":"MCDHook \u00b6 Bases: BaseWrapperHook Implementation of Maximum Classifier Discrepancy for Unsupervised Domain Adaptation . Source code in pytorch_adapt\\hooks\\mcd.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class MCDHook ( BaseWrapperHook ): \"\"\" Implementation of [Maximum Classifier Discrepancy for Unsupervised Domain Adaptation](https://arxiv.org/abs/1712.02560). \"\"\" def __init__ ( self , g_opts , c_opts , discrepancy_loss_fn = None , x_weighter = None , x_reducer = None , y_weighter = None , y_reducer = None , z_weighter = None , z_reducer = None , pre_x = None , post_x = None , pre_y = None , post_y = None , pre_z = None , post_z = None , repeat = 4 , ** kwargs , ): super () . __init__ ( ** kwargs ) [ pre_x , post_x , pre_y , post_y , pre_z , post_z ] = c_f . many_default ( [ pre_x , post_x , pre_y , post_y , pre_z , post_z ], [[], [], [], [], [], []] ) x = ChainHook ( * pre_x , MultipleCLossHook (), * post_x ) y = ChainHook ( * pre_y , MultipleCLossHook ( detach_features = True ), MCDLossHook ( detach_features = True , minimize = False , loss_fn = discrepancy_loss_fn ), * post_y , ) z = ChainHook ( * pre_z , MCDLossHook ( loss_fn = discrepancy_loss_fn ), * post_z ) x = OptimizerHook ( x , [ * c_opts , * g_opts ], x_weighter , x_reducer ) y = OptimizerHook ( y , c_opts , y_weighter , y_reducer ) z = OptimizerHook ( z , g_opts , z_weighter , z_reducer ) s_hook = SummaryHook ({ \"x_loss\" : x , \"y_loss\" : y , \"z_loss\" : z }) z = RepeatHook ( z , repeat , keep_only_last = True ) self . hook = ChainHook ( ParallelHook ( x , y , z ), s_hook )","title":"mcd"},{"location":"docs/hooks/mcd/#pytorch_adapt.hooks.mcd.MCDHook","text":"Bases: BaseWrapperHook Implementation of Maximum Classifier Discrepancy for Unsupervised Domain Adaptation . Source code in pytorch_adapt\\hooks\\mcd.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class MCDHook ( BaseWrapperHook ): \"\"\" Implementation of [Maximum Classifier Discrepancy for Unsupervised Domain Adaptation](https://arxiv.org/abs/1712.02560). \"\"\" def __init__ ( self , g_opts , c_opts , discrepancy_loss_fn = None , x_weighter = None , x_reducer = None , y_weighter = None , y_reducer = None , z_weighter = None , z_reducer = None , pre_x = None , post_x = None , pre_y = None , post_y = None , pre_z = None , post_z = None , repeat = 4 , ** kwargs , ): super () . __init__ ( ** kwargs ) [ pre_x , post_x , pre_y , post_y , pre_z , post_z ] = c_f . many_default ( [ pre_x , post_x , pre_y , post_y , pre_z , post_z ], [[], [], [], [], [], []] ) x = ChainHook ( * pre_x , MultipleCLossHook (), * post_x ) y = ChainHook ( * pre_y , MultipleCLossHook ( detach_features = True ), MCDLossHook ( detach_features = True , minimize = False , loss_fn = discrepancy_loss_fn ), * post_y , ) z = ChainHook ( * pre_z , MCDLossHook ( loss_fn = discrepancy_loss_fn ), * post_z ) x = OptimizerHook ( x , [ * c_opts , * g_opts ], x_weighter , x_reducer ) y = OptimizerHook ( y , c_opts , y_weighter , y_reducer ) z = OptimizerHook ( z , g_opts , z_weighter , z_reducer ) s_hook = SummaryHook ({ \"x_loss\" : x , \"y_loss\" : y , \"z_loss\" : z }) z = RepeatHook ( z , repeat , keep_only_last = True ) self . hook = ChainHook ( ParallelHook ( x , y , z ), s_hook )","title":"MCDHook"},{"location":"docs/hooks/optimizer/","text":"OptimizerHook \u00b6 Bases: BaseHook Executes the wrapped hook Zeros all gradients Backpropagates the loss Steps the optimizer Source code in pytorch_adapt\\hooks\\optimizer.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class OptimizerHook ( BaseHook ): \"\"\" 1. Executes the wrapped hook 2. Zeros all gradients 3. Backpropagates the loss 4. Steps the optimizer \"\"\" def __init__ ( self , hook : BaseHook , optimizers : Union [ List [ torch . optim . Optimizer ], List [ str ]], weighter : BaseWeighter = None , reducer : BaseReducer = None , ** kwargs ): \"\"\" Arguments: hook: the hook that computes the losses optimizers: either a list of optimizers that will be used to update model weights, or a list of optimizer names. If it's the latter, then the optimizers must be passed into the hook as one of the ```inputs```. weighter: weights the returned losses and outputs a single value on which ```.backward()``` is called. If ```None```, then it defaults to [```MeanWeighter```][pytorch_adapt.weighters.MeanWeighter]. reducer: a hook that reduces any unreduced losses to a single value. If ```None```, then it defaults to [```MeanReducer```][pytorch_adapt.hooks.MeanReducer]. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . optimizers = optimizers self . weighter = c_f . default ( weighter , MeanWeighter , {}) self . reducer = c_f . default ( reducer , MeanReducer , {}) self . loss_components = {} def call ( self , inputs , losses ): \"\"\"\"\"\" outputs , losses = self . hook ( inputs , losses ) combined = c_f . assert_dicts_are_disjoint ( inputs , outputs ) new_outputs , losses = self . reducer ( combined , losses ) outputs . update ( new_outputs ) loss , self . loss_components = self . weighter ( losses ) optimizers = self . optimizers if isinstance ( optimizers [ 0 ], str ): optimizers = c_f . extract ( inputs , optimizers ) c_f . zero_back_step ( loss , optimizers , inputs . get ( \"custom_backward\" )) return outputs , {} def _loss_keys ( self ): \"\"\"\"\"\" return [] def _out_keys ( self ): \"\"\"\"\"\" return c_f . join_lists ([ self . hook . out_keys , self . reducer . out_keys ]) def extra_repr ( self ): return c_f . extra_repr ( self , [ \"optimizers\" , \"weighter\" ]) __init__ ( hook , optimizers , weighter = None , reducer = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default hook BaseHook the hook that computes the losses required optimizers Union [ List [ torch . optim . Optimizer ], List [ str ]] either a list of optimizers that will be used to update model weights, or a list of optimizer names. If it's the latter, then the optimizers must be passed into the hook as one of the inputs . required weighter BaseWeighter weights the returned losses and outputs a single value on which .backward() is called. If None , then it defaults to MeanWeighter . None reducer BaseReducer a hook that reduces any unreduced losses to a single value. If None , then it defaults to MeanReducer . None Source code in pytorch_adapt\\hooks\\optimizer.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , hook : BaseHook , optimizers : Union [ List [ torch . optim . Optimizer ], List [ str ]], weighter : BaseWeighter = None , reducer : BaseReducer = None , ** kwargs ): \"\"\" Arguments: hook: the hook that computes the losses optimizers: either a list of optimizers that will be used to update model weights, or a list of optimizer names. If it's the latter, then the optimizers must be passed into the hook as one of the ```inputs```. weighter: weights the returned losses and outputs a single value on which ```.backward()``` is called. If ```None```, then it defaults to [```MeanWeighter```][pytorch_adapt.weighters.MeanWeighter]. reducer: a hook that reduces any unreduced losses to a single value. If ```None```, then it defaults to [```MeanReducer```][pytorch_adapt.hooks.MeanReducer]. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . optimizers = optimizers self . weighter = c_f . default ( weighter , MeanWeighter , {}) self . reducer = c_f . default ( reducer , MeanReducer , {}) self . loss_components = {} SummaryHook \u00b6 Bases: BaseHook Repackages losses into a dictionary format useful for logging. This should be used only at the very end of each iteration, i.e. it should be the last sub-hook in a ChainHook . Source code in pytorch_adapt\\hooks\\optimizer.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 class SummaryHook ( BaseHook ): \"\"\" Repackages losses into a dictionary format useful for logging. This should be used only at the very end of each iteration, i.e. it should be the last sub-hook in a [ChainHook][pytorch_adapt.hooks.ChainHook]. \"\"\" def __init__ ( self , optimizers : Dict [ str , OptimizerHook ], ** kwargs ): \"\"\" Arguments: optimizers: A dictionary of optimizer hooks. The losses computed inside these hooks will be packaged into nested dictionaries. \"\"\" super () . __init__ ( ** kwargs ) self . optimizers = optimizers def call ( self , inputs , losses ): \"\"\"\"\"\" losses = {} for k , v in self . optimizers . items (): losses [ k ] = v . loss_components return {}, losses def _loss_keys ( self ): \"\"\"\"\"\" return list ( self . optimizers . keys ()) def _out_keys ( self ): \"\"\"\"\"\" return [] __init__ ( optimizers , ** kwargs ) \u00b6 Parameters: Name Type Description Default optimizers Dict [ str , OptimizerHook ] A dictionary of optimizer hooks. The losses computed inside these hooks will be packaged into nested dictionaries. required Source code in pytorch_adapt\\hooks\\optimizer.py 82 83 84 85 86 87 88 89 90 def __init__ ( self , optimizers : Dict [ str , OptimizerHook ], ** kwargs ): \"\"\" Arguments: optimizers: A dictionary of optimizer hooks. The losses computed inside these hooks will be packaged into nested dictionaries. \"\"\" super () . __init__ ( ** kwargs ) self . optimizers = optimizers","title":"optimizer"},{"location":"docs/hooks/optimizer/#pytorch_adapt.hooks.optimizer.OptimizerHook","text":"Bases: BaseHook Executes the wrapped hook Zeros all gradients Backpropagates the loss Steps the optimizer Source code in pytorch_adapt\\hooks\\optimizer.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class OptimizerHook ( BaseHook ): \"\"\" 1. Executes the wrapped hook 2. Zeros all gradients 3. Backpropagates the loss 4. Steps the optimizer \"\"\" def __init__ ( self , hook : BaseHook , optimizers : Union [ List [ torch . optim . Optimizer ], List [ str ]], weighter : BaseWeighter = None , reducer : BaseReducer = None , ** kwargs ): \"\"\" Arguments: hook: the hook that computes the losses optimizers: either a list of optimizers that will be used to update model weights, or a list of optimizer names. If it's the latter, then the optimizers must be passed into the hook as one of the ```inputs```. weighter: weights the returned losses and outputs a single value on which ```.backward()``` is called. If ```None```, then it defaults to [```MeanWeighter```][pytorch_adapt.weighters.MeanWeighter]. reducer: a hook that reduces any unreduced losses to a single value. If ```None```, then it defaults to [```MeanReducer```][pytorch_adapt.hooks.MeanReducer]. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . optimizers = optimizers self . weighter = c_f . default ( weighter , MeanWeighter , {}) self . reducer = c_f . default ( reducer , MeanReducer , {}) self . loss_components = {} def call ( self , inputs , losses ): \"\"\"\"\"\" outputs , losses = self . hook ( inputs , losses ) combined = c_f . assert_dicts_are_disjoint ( inputs , outputs ) new_outputs , losses = self . reducer ( combined , losses ) outputs . update ( new_outputs ) loss , self . loss_components = self . weighter ( losses ) optimizers = self . optimizers if isinstance ( optimizers [ 0 ], str ): optimizers = c_f . extract ( inputs , optimizers ) c_f . zero_back_step ( loss , optimizers , inputs . get ( \"custom_backward\" )) return outputs , {} def _loss_keys ( self ): \"\"\"\"\"\" return [] def _out_keys ( self ): \"\"\"\"\"\" return c_f . join_lists ([ self . hook . out_keys , self . reducer . out_keys ]) def extra_repr ( self ): return c_f . extra_repr ( self , [ \"optimizers\" , \"weighter\" ])","title":"OptimizerHook"},{"location":"docs/hooks/optimizer/#pytorch_adapt.hooks.optimizer.SummaryHook","text":"Bases: BaseHook Repackages losses into a dictionary format useful for logging. This should be used only at the very end of each iteration, i.e. it should be the last sub-hook in a ChainHook . Source code in pytorch_adapt\\hooks\\optimizer.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 class SummaryHook ( BaseHook ): \"\"\" Repackages losses into a dictionary format useful for logging. This should be used only at the very end of each iteration, i.e. it should be the last sub-hook in a [ChainHook][pytorch_adapt.hooks.ChainHook]. \"\"\" def __init__ ( self , optimizers : Dict [ str , OptimizerHook ], ** kwargs ): \"\"\" Arguments: optimizers: A dictionary of optimizer hooks. The losses computed inside these hooks will be packaged into nested dictionaries. \"\"\" super () . __init__ ( ** kwargs ) self . optimizers = optimizers def call ( self , inputs , losses ): \"\"\"\"\"\" losses = {} for k , v in self . optimizers . items (): losses [ k ] = v . loss_components return {}, losses def _loss_keys ( self ): \"\"\"\"\"\" return list ( self . optimizers . keys ()) def _out_keys ( self ): \"\"\"\"\"\" return []","title":"SummaryHook"},{"location":"docs/hooks/reducers/","text":"BaseReducer \u00b6 Bases: BaseHook , ABC Converts an unreduced loss tensor into a single number. In other words, if the loss tensor has shape (N,) , the reducer converts it to shape (1,) . Source code in pytorch_adapt\\hooks\\reducers.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class BaseReducer ( BaseHook , ABC ): \"\"\" Converts an unreduced loss tensor into a single number. In other words, if the loss tensor has shape ```(N,)```, the reducer converts it to shape ```(1,)```. \"\"\" def __init__ ( self , apply_to : List [ str ] = None , default_reducer : \"BaseReducer\" = None , ** kwargs , ): \"\"\" Arguments: apply_to: list of loss names to apply reduction to default_reducer: a reducer to use for losses that are not already reduced and are also not specified in ```apply_to```. If ```None```, then no action is taken. \"\"\" super () . __init__ ( ** kwargs ) self . apply_to = apply_to self . default_reducer = default_reducer self . curr_loss_keys = [] def call ( self , inputs , losses ): \"\"\"\"\"\" self . curr_loss_keys = list ( losses . keys ()) apply_to = self . get_keys_to_apply_to ( losses ) outputs , losses = self . call_reducer ( inputs , losses , apply_to ) if self . default_reducer : combined = c_f . assert_dicts_are_disjoint ( inputs , outputs ) new_outputs , losses = self . default_reducer ( combined , losses ) outputs . update ( new_outputs ) if losses . keys () != set ( self . curr_loss_keys ): raise ValueError ( \"Loss dict returned by reducer should have same keys as input loss dict\" ) return outputs , losses @abstractmethod def call_reducer ( self , inputs , losses , apply_to ): pass def _loss_keys ( self ): \"\"\"\"\"\" return self . curr_loss_keys def get_keys_to_apply_to ( self , losses ): apply_to = self . apply_to if apply_to is None : apply_to = [ k for k , v in losses . items () if not c_f . len_one_tensor ( v )] elif len ( set ( apply_to ) - set ( self . curr_loss_keys )) > 0 : raise ValueError ( f \"self.apply_to ( { self . apply_to } ) must be a subset of losses.keys() ( { losses . keys () } )\" ) return apply_to def extra_repr ( self ): return c_f . extra_repr ( self , [ \"apply_to\" ]) __init__ ( apply_to = None , default_reducer = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default apply_to List [ str ] list of loss names to apply reduction to None default_reducer 'BaseReducer' a reducer to use for losses that are not already reduced and are also not specified in apply_to . If None , then no action is taken. None Source code in pytorch_adapt\\hooks\\reducers.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , apply_to : List [ str ] = None , default_reducer : \"BaseReducer\" = None , ** kwargs , ): \"\"\" Arguments: apply_to: list of loss names to apply reduction to default_reducer: a reducer to use for losses that are not already reduced and are also not specified in ```apply_to```. If ```None```, then no action is taken. \"\"\" super () . __init__ ( ** kwargs ) self . apply_to = apply_to self . default_reducer = default_reducer self . curr_loss_keys = [] EntropyReducer \u00b6 Bases: BaseReducer Implementation of \"entropy conditioning\" from Conditional Adversarial Domain Adaptation . It weights loss elements using EntropyWeights . The entropy weights are derived from classifier logits. Source code in pytorch_adapt\\hooks\\reducers.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 class EntropyReducer ( BaseReducer ): \"\"\" Implementation of \"entropy conditioning\" from [Conditional Adversarial Domain Adaptation](https://arxiv.org/abs/1705.10667). It weights loss elements using [```EntropyWeights```][pytorch_adapt.layers.EntropyWeights]. The entropy weights are derived from classifier logits. \"\"\" def __init__ ( self , f_hook : BaseHook = None , domains : List [ str ] = None , entropy_weights_fn : Callable [[ torch . Tensor ], torch . Tensor ] = None , detach_weights : bool = True , ** kwargs , ): \"\"\" Arguments: f_hook: the hook for computing logits from which entropy weights are derived domains: the domains that ```f_hook``` should compute for entropy_weights_fn: the function for computing the weights that will be multiplied with the unreduced losses. detach_weights: If ```True```, the entropy weights are detached from the autograd graph \"\"\" super () . __init__ ( ** kwargs ) src_regex = \"^ {0} _|_ {0} $|_ {0} _|^ {0} $\" . format ( \"src\" ) target_regex = \"^ {0} _|_ {0} $|_ {0} _|^ {0} $\" . format ( \"target\" ) self . src_regex = re . compile ( src_regex ) self . target_regex = re . compile ( target_regex ) self . entropy_weights_fn = c_f . default ( entropy_weights_fn , EntropyWeights , {}) self . f_hook = c_f . default ( f_hook , FeaturesAndLogitsHook , { \"detach_features\" : detach_weights , \"detach_logits\" : detach_weights , \"domains\" : domains , }, ) self . context = torch . no_grad () if detach_weights else nullcontext () def call_reducer ( self , inputs , losses , apply_to ): outputs = self . f_hook ( inputs , losses )[ 0 ] for k in apply_to : if self . src_regex . search ( k ): domain = \"src\" elif self . target_regex . search ( k ): domain = \"target\" else : raise ValueError with self . context : search_str = c_f . filter ( self . f_hook . out_keys , \"_logits\" , [ f \"^ { domain } \" ]) [ logits ] = c_f . extract ([ outputs , inputs ], search_str ) weights = self . entropy_weights_fn ( logits ) losses [ k ] = torch . mean ( weights * losses [ k ]) return outputs , losses def _out_keys ( self ): \"\"\"\"\"\" return self . f_hook . out_keys __init__ ( f_hook = None , domains = None , entropy_weights_fn = None , detach_weights = True , ** kwargs ) \u00b6 Parameters: Name Type Description Default f_hook BaseHook the hook for computing logits from which entropy weights are derived None domains List [ str ] the domains that f_hook should compute for None entropy_weights_fn Callable [[ torch . Tensor ], torch . Tensor ] the function for computing the weights that will be multiplied with the unreduced losses. None detach_weights bool If True , the entropy weights are detached from the autograd graph True Source code in pytorch_adapt\\hooks\\reducers.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def __init__ ( self , f_hook : BaseHook = None , domains : List [ str ] = None , entropy_weights_fn : Callable [[ torch . Tensor ], torch . Tensor ] = None , detach_weights : bool = True , ** kwargs , ): \"\"\" Arguments: f_hook: the hook for computing logits from which entropy weights are derived domains: the domains that ```f_hook``` should compute for entropy_weights_fn: the function for computing the weights that will be multiplied with the unreduced losses. detach_weights: If ```True```, the entropy weights are detached from the autograd graph \"\"\" super () . __init__ ( ** kwargs ) src_regex = \"^ {0} _|_ {0} $|_ {0} _|^ {0} $\" . format ( \"src\" ) target_regex = \"^ {0} _|_ {0} $|_ {0} _|^ {0} $\" . format ( \"target\" ) self . src_regex = re . compile ( src_regex ) self . target_regex = re . compile ( target_regex ) self . entropy_weights_fn = c_f . default ( entropy_weights_fn , EntropyWeights , {}) self . f_hook = c_f . default ( f_hook , FeaturesAndLogitsHook , { \"detach_features\" : detach_weights , \"detach_logits\" : detach_weights , \"domains\" : domains , }, ) self . context = torch . no_grad () if detach_weights else nullcontext () MeanReducer \u00b6 Bases: BaseReducer Reduces loss elements by taking the mean. Source code in pytorch_adapt\\hooks\\reducers.py 160 161 162 163 164 165 166 167 168 169 170 171 172 class MeanReducer ( BaseReducer ): \"\"\" Reduces loss elements by taking the mean. \"\"\" def call_reducer ( self , inputs , losses , apply_to ): for k in apply_to : losses [ k ] = torch . mean ( losses [ k ]) return {}, losses def _out_keys ( self ): \"\"\"\"\"\" return []","title":"reducers"},{"location":"docs/hooks/reducers/#pytorch_adapt.hooks.reducers.BaseReducer","text":"Bases: BaseHook , ABC Converts an unreduced loss tensor into a single number. In other words, if the loss tensor has shape (N,) , the reducer converts it to shape (1,) . Source code in pytorch_adapt\\hooks\\reducers.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class BaseReducer ( BaseHook , ABC ): \"\"\" Converts an unreduced loss tensor into a single number. In other words, if the loss tensor has shape ```(N,)```, the reducer converts it to shape ```(1,)```. \"\"\" def __init__ ( self , apply_to : List [ str ] = None , default_reducer : \"BaseReducer\" = None , ** kwargs , ): \"\"\" Arguments: apply_to: list of loss names to apply reduction to default_reducer: a reducer to use for losses that are not already reduced and are also not specified in ```apply_to```. If ```None```, then no action is taken. \"\"\" super () . __init__ ( ** kwargs ) self . apply_to = apply_to self . default_reducer = default_reducer self . curr_loss_keys = [] def call ( self , inputs , losses ): \"\"\"\"\"\" self . curr_loss_keys = list ( losses . keys ()) apply_to = self . get_keys_to_apply_to ( losses ) outputs , losses = self . call_reducer ( inputs , losses , apply_to ) if self . default_reducer : combined = c_f . assert_dicts_are_disjoint ( inputs , outputs ) new_outputs , losses = self . default_reducer ( combined , losses ) outputs . update ( new_outputs ) if losses . keys () != set ( self . curr_loss_keys ): raise ValueError ( \"Loss dict returned by reducer should have same keys as input loss dict\" ) return outputs , losses @abstractmethod def call_reducer ( self , inputs , losses , apply_to ): pass def _loss_keys ( self ): \"\"\"\"\"\" return self . curr_loss_keys def get_keys_to_apply_to ( self , losses ): apply_to = self . apply_to if apply_to is None : apply_to = [ k for k , v in losses . items () if not c_f . len_one_tensor ( v )] elif len ( set ( apply_to ) - set ( self . curr_loss_keys )) > 0 : raise ValueError ( f \"self.apply_to ( { self . apply_to } ) must be a subset of losses.keys() ( { losses . keys () } )\" ) return apply_to def extra_repr ( self ): return c_f . extra_repr ( self , [ \"apply_to\" ])","title":"BaseReducer"},{"location":"docs/hooks/reducers/#pytorch_adapt.hooks.reducers.EntropyReducer","text":"Bases: BaseReducer Implementation of \"entropy conditioning\" from Conditional Adversarial Domain Adaptation . It weights loss elements using EntropyWeights . The entropy weights are derived from classifier logits. Source code in pytorch_adapt\\hooks\\reducers.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 class EntropyReducer ( BaseReducer ): \"\"\" Implementation of \"entropy conditioning\" from [Conditional Adversarial Domain Adaptation](https://arxiv.org/abs/1705.10667). It weights loss elements using [```EntropyWeights```][pytorch_adapt.layers.EntropyWeights]. The entropy weights are derived from classifier logits. \"\"\" def __init__ ( self , f_hook : BaseHook = None , domains : List [ str ] = None , entropy_weights_fn : Callable [[ torch . Tensor ], torch . Tensor ] = None , detach_weights : bool = True , ** kwargs , ): \"\"\" Arguments: f_hook: the hook for computing logits from which entropy weights are derived domains: the domains that ```f_hook``` should compute for entropy_weights_fn: the function for computing the weights that will be multiplied with the unreduced losses. detach_weights: If ```True```, the entropy weights are detached from the autograd graph \"\"\" super () . __init__ ( ** kwargs ) src_regex = \"^ {0} _|_ {0} $|_ {0} _|^ {0} $\" . format ( \"src\" ) target_regex = \"^ {0} _|_ {0} $|_ {0} _|^ {0} $\" . format ( \"target\" ) self . src_regex = re . compile ( src_regex ) self . target_regex = re . compile ( target_regex ) self . entropy_weights_fn = c_f . default ( entropy_weights_fn , EntropyWeights , {}) self . f_hook = c_f . default ( f_hook , FeaturesAndLogitsHook , { \"detach_features\" : detach_weights , \"detach_logits\" : detach_weights , \"domains\" : domains , }, ) self . context = torch . no_grad () if detach_weights else nullcontext () def call_reducer ( self , inputs , losses , apply_to ): outputs = self . f_hook ( inputs , losses )[ 0 ] for k in apply_to : if self . src_regex . search ( k ): domain = \"src\" elif self . target_regex . search ( k ): domain = \"target\" else : raise ValueError with self . context : search_str = c_f . filter ( self . f_hook . out_keys , \"_logits\" , [ f \"^ { domain } \" ]) [ logits ] = c_f . extract ([ outputs , inputs ], search_str ) weights = self . entropy_weights_fn ( logits ) losses [ k ] = torch . mean ( weights * losses [ k ]) return outputs , losses def _out_keys ( self ): \"\"\"\"\"\" return self . f_hook . out_keys","title":"EntropyReducer"},{"location":"docs/hooks/reducers/#pytorch_adapt.hooks.reducers.MeanReducer","text":"Bases: BaseReducer Reduces loss elements by taking the mean. Source code in pytorch_adapt\\hooks\\reducers.py 160 161 162 163 164 165 166 167 168 169 170 171 172 class MeanReducer ( BaseReducer ): \"\"\" Reduces loss elements by taking the mean. \"\"\" def call_reducer ( self , inputs , losses , apply_to ): for k in apply_to : losses [ k ] = torch . mean ( losses [ k ]) return {}, losses def _out_keys ( self ): \"\"\"\"\"\" return []","title":"MeanReducer"},{"location":"docs/hooks/rtn/","text":"RTNHook \u00b6 Bases: BaseWrapperHook Implementation of Unsupervised Domain Adaptation with Residual Transfer Networks . Source code in pytorch_adapt\\hooks\\rtn.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class RTNHook ( BaseWrapperHook ): \"\"\" Implementation of [Unsupervised Domain Adaptation with Residual Transfer Networks](https://arxiv.org/abs/1602.04433). \"\"\" def __init__ ( self , opts , weighter = None , reducer = None , pre = None , post = None , aligner_loss_fn = None , ** kwargs , ): super () . __init__ ( ** kwargs ) [ pre , post ] = c_f . many_default ([ pre , post ], [[], []]) hook = ChainHook ( * pre , RTNAlignerHook ( aligner_loss_fn ), RTNLogitsHook (), * post ) hook = OptimizerHook ( hook , opts , weighter , reducer ) s_hook = SummaryHook ({ \"total_loss\" : hook }) self . hook = ChainHook ( hook , s_hook )","title":"rtn"},{"location":"docs/hooks/rtn/#pytorch_adapt.hooks.rtn.RTNHook","text":"Bases: BaseWrapperHook Implementation of Unsupervised Domain Adaptation with Residual Transfer Networks . Source code in pytorch_adapt\\hooks\\rtn.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class RTNHook ( BaseWrapperHook ): \"\"\" Implementation of [Unsupervised Domain Adaptation with Residual Transfer Networks](https://arxiv.org/abs/1602.04433). \"\"\" def __init__ ( self , opts , weighter = None , reducer = None , pre = None , post = None , aligner_loss_fn = None , ** kwargs , ): super () . __init__ ( ** kwargs ) [ pre , post ] = c_f . many_default ([ pre , post ], [[], []]) hook = ChainHook ( * pre , RTNAlignerHook ( aligner_loss_fn ), RTNLogitsHook (), * post ) hook = OptimizerHook ( hook , opts , weighter , reducer ) s_hook = SummaryHook ({ \"total_loss\" : hook }) self . hook = ChainHook ( hook , s_hook )","title":"RTNHook"},{"location":"docs/hooks/symnets/","text":"SymNetsHook \u00b6 Bases: BaseWrapperHook Implementation of Domain-Symmetric Networks for Adversarial Domain Adaptation . Source code in pytorch_adapt\\hooks\\symnets.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class SymNetsHook ( BaseWrapperHook ): \"\"\" Implementation of [Domain-Symmetric Networks for Adversarial Domain Adaptation](https://arxiv.org/abs/1904.04663). \"\"\" def __init__ ( self , c_opts , g_opts , c_weighter = None , c_reducer = None , g_weighter = None , g_reducer = None , ** kwargs , ): super () . __init__ ( ** kwargs ) f_hook = FeaturesHook () c_hook = OptimizerHook ( SymNetsCHook (), c_opts , c_weighter , c_reducer ) g_hook = OptimizerHook ( SymNetsGHook (), g_opts , g_weighter , g_reducer ) s_hook = SummaryHook ({ \"c_loss\" : c_hook , \"g_loss\" : g_hook }) self . hook = ChainHook ( f_hook , c_hook , g_hook , s_hook )","title":"symnets"},{"location":"docs/hooks/symnets/#pytorch_adapt.hooks.symnets.SymNetsHook","text":"Bases: BaseWrapperHook Implementation of Domain-Symmetric Networks for Adversarial Domain Adaptation . Source code in pytorch_adapt\\hooks\\symnets.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class SymNetsHook ( BaseWrapperHook ): \"\"\" Implementation of [Domain-Symmetric Networks for Adversarial Domain Adaptation](https://arxiv.org/abs/1904.04663). \"\"\" def __init__ ( self , c_opts , g_opts , c_weighter = None , c_reducer = None , g_weighter = None , g_reducer = None , ** kwargs , ): super () . __init__ ( ** kwargs ) f_hook = FeaturesHook () c_hook = OptimizerHook ( SymNetsCHook (), c_opts , c_weighter , c_reducer ) g_hook = OptimizerHook ( SymNetsGHook (), g_opts , g_weighter , g_reducer ) s_hook = SummaryHook ({ \"c_loss\" : c_hook , \"g_loss\" : g_hook }) self . hook = ChainHook ( f_hook , c_hook , g_hook , s_hook )","title":"SymNetsHook"},{"location":"docs/hooks/utils/","text":"ApplyFnHook \u00b6 Bases: BaseHook Applies a function to specific values of the context. Source code in pytorch_adapt\\hooks\\utils.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 class ApplyFnHook ( BaseHook ): \"\"\" Applies a function to specific values of the context. \"\"\" def __init__ ( self , fn : Callable , apply_to : List [ str ], is_loss : bool = False , ** kwargs ): \"\"\" Arguments: fn: The function that will be applied to the inputs. apply_to: fn will be applied to ```inputs[k]``` for k in apply_to is_loss: If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. \"\"\" super () . __init__ ( ** kwargs ) self . fn = fn self . apply_to = apply_to self . is_loss = is_loss def call ( self , inputs , losses ): \"\"\"\"\"\" x = c_f . extract ( inputs , self . apply_to ) outputs = { k : self . fn ( v ) for k , v in zip ( self . apply_to , x )} if self . is_loss : return outputs , {} return outputs , {} def _loss_keys ( self ): \"\"\"\"\"\" return self . apply_to if self . is_loss else [] def _out_keys ( self ): \"\"\"\"\"\" return [] if self . is_loss else self . apply_to def extra_repr ( self ): return c_f . extra_repr ( self , [ \"apply_to\" ]) __init__ ( fn , apply_to , is_loss = False , ** kwargs ) \u00b6 Parameters: Name Type Description Default fn Callable The function that will be applied to the inputs. required apply_to List [ str ] fn will be applied to inputs[k] for k in apply_to required is_loss bool If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. False Source code in pytorch_adapt\\hooks\\utils.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 def __init__ ( self , fn : Callable , apply_to : List [ str ], is_loss : bool = False , ** kwargs ): \"\"\" Arguments: fn: The function that will be applied to the inputs. apply_to: fn will be applied to ```inputs[k]``` for k in apply_to is_loss: If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. \"\"\" super () . __init__ ( ** kwargs ) self . fn = fn self . apply_to = apply_to self . is_loss = is_loss AssertHook \u00b6 Bases: BaseWrapperHook Asserts that the output keys of a hook match a specified regex string Source code in pytorch_adapt\\hooks\\utils.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 class AssertHook ( BaseWrapperHook ): \"\"\" Asserts that the output keys of a hook match a specified regex string \"\"\" def __init__ ( self , hook : BaseHook , allowed : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook allowed: The output dictionary of ```hook``` must have keys that match the ```allowed``` regex. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook if not isinstance ( allowed , str ): raise TypeError ( \"allowed must be a str\" ) self . allowed = allowed def call ( self , inputs , losses ): \"\"\"\"\"\" outputs , losses = self . hook ( inputs , losses ) self . assert_fn ( outputs ) return outputs , losses def assert_fn ( self , outputs ): filtered = c_f . filter ( outputs , self . allowed ) if len ( filtered ) != len ( outputs ): error_str = f \" { c_f . cls_name ( self . hook ) } is producing outputs that don't match the allowed regex in { c_f . cls_name ( self ) } \\n \" error_str += f \"output keys = { outputs . keys () } \\n \" error_str += f \"regex filter = { self . allowed } \" raise ValueError ( error_str ) def extra_repr ( self ): return c_f . extra_repr ( self , [ \"allowed\" ]) __init__ ( hook , allowed , ** kwargs ) \u00b6 Parameters: Name Type Description Default hook BaseHook The wrapped hook required allowed str The output dictionary of hook must have keys that match the allowed regex. required Source code in pytorch_adapt\\hooks\\utils.py 303 304 305 306 307 308 309 310 311 312 313 314 def __init__ ( self , hook : BaseHook , allowed : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook allowed: The output dictionary of ```hook``` must have keys that match the ```allowed``` regex. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook if not isinstance ( allowed , str ): raise TypeError ( \"allowed must be a str\" ) self . allowed = allowed ChainHook \u00b6 Bases: BaseHook Calls multiple hooks sequentially. The Nth hook receives the context accumulated through hooks 0 to N-1. Source code in pytorch_adapt\\hooks\\utils.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class ChainHook ( BaseHook ): \"\"\" Calls multiple hooks sequentially. The Nth hook receives the context accumulated through hooks 0 to N-1. \"\"\" def __init__ ( self , * hooks : BaseHook , conditions : List [ BaseConditionHook ] = None , alts : List [ BaseHook ] = None , overwrite : Union [ bool , List [ int ]] = False , ** kwargs , ): \"\"\" Arguments: *hooks: a sequence of hooks that will be called sequentially. conditions: an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. alts: an optional list of hooks that will be executed when the corresponding condition hook returns False overwrite: If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . conditions = c_f . default ( conditions , [ TrueHook () for _ in range ( len ( hooks ))] ) self . alts = c_f . default ( alts , [ ZeroLossHook ( h . loss_keys , h . out_keys ) for h in self . hooks ] ) self . check_alt_keys_match_hook_keys () if not isinstance ( overwrite , ( list , bool )): raise TypeError ( \"overwrite must be a list or bool\" ) self . overwrite = overwrite self . in_keys = self . hooks [ 0 ] . in_keys def call ( self , inputs , losses ): \"\"\"\"\"\" outputs , out_losses = {}, {} all_inputs , all_losses = inputs , losses prev_outputs , prev_losses = {}, {} for i , h in enumerate ( self . hooks ): self . check_overwrite ( i , all_inputs , prev_outputs , self . overwrite ) self . check_overwrite ( i , all_losses , prev_losses , False ) all_inputs = { ** all_inputs , ** prev_outputs } all_losses = { ** all_losses , ** prev_losses } if self . conditions [ i ]( all_inputs , all_losses ): x = h ( all_inputs , all_losses ) else : x = self . alts [ i ]( all_inputs , all_losses ) prev_outputs , prev_losses = x out_losses . update ( prev_losses ) outputs . update ( prev_outputs ) return outputs , out_losses def check_overlap ( self , x , y , names ): is_overlap , overlap = c_f . dicts_are_overlapping ( x , y , return_overlap = True ) if is_overlap : raise KeyError ( f \"overwrite is false, but { names [ 0 ] } and { names [ 1 ] } have overlapping keys: { overlap } \" ) def check_overwrite ( self , i , kwargs , prev_outputs , overwrite ): if not overwrite or ( isinstance ( overwrite , list ) and i not in overwrite ): self . check_overlap ( kwargs , prev_outputs , [ \"kwargs\" , \"prev_outputs\" ]) def _loss_keys ( self ): \"\"\"\"\"\" return c_f . join_lists ([ h . loss_keys for h in self . hooks ]) def _out_keys ( self ): \"\"\"\"\"\" return c_f . join_lists ([ h . out_keys for h in self . hooks ]) @property def last_hook_out_keys ( self ): return self . hooks [ - 1 ] . out_keys def check_alt_keys_match_hook_keys ( self ): for i in range ( len ( self . hooks )): h = self . hooks [ i ] a = self . alts [ i ] if ( sorted ( h . loss_keys ) != sorted ( a . loss_keys )) or ( sorted ( h . out_keys ) != sorted ( a . out_keys ) ): raise ValueError ( \"alt loss/out keys must be equal to hook loss/out keys\" ) def children_repr ( self ): x = super () . children_repr () x [ \"hooks\" ] = self . hooks if any ( not isinstance ( c , TrueHook ) for c in self . conditions ): x . update ({ \"conditions\" : self . conditions , \"alts\" : self . alts }) return x __init__ ( * hooks , conditions = None , alts = None , overwrite = False , ** kwargs ) \u00b6 Parameters: Name Type Description Default *hooks BaseHook a sequence of hooks that will be called sequentially. () conditions List [ BaseConditionHook ] an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. None alts List [ BaseHook ] an optional list of hooks that will be executed when the corresponding condition hook returns False None overwrite Union [ bool , List [ int ]] If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. False Source code in pytorch_adapt\\hooks\\utils.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , * hooks : BaseHook , conditions : List [ BaseConditionHook ] = None , alts : List [ BaseHook ] = None , overwrite : Union [ bool , List [ int ]] = False , ** kwargs , ): \"\"\" Arguments: *hooks: a sequence of hooks that will be called sequentially. conditions: an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. alts: an optional list of hooks that will be executed when the corresponding condition hook returns False overwrite: If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . conditions = c_f . default ( conditions , [ TrueHook () for _ in range ( len ( hooks ))] ) self . alts = c_f . default ( alts , [ ZeroLossHook ( h . loss_keys , h . out_keys ) for h in self . hooks ] ) self . check_alt_keys_match_hook_keys () if not isinstance ( overwrite , ( list , bool )): raise TypeError ( \"overwrite must be a list or bool\" ) self . overwrite = overwrite self . in_keys = self . hooks [ 0 ] . in_keys EmptyHook \u00b6 Bases: BaseHook Returns two empty dictionaries. Source code in pytorch_adapt\\hooks\\utils.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class EmptyHook ( BaseHook ): \"\"\"Returns two empty dictionaries.\"\"\" def call ( self , inputs , losses ): \"\"\"\"\"\" return {}, {} def _loss_keys ( self ): \"\"\"\"\"\" return [] def _out_keys ( self ): \"\"\"\"\"\" return [] FalseHook \u00b6 Bases: BaseConditionHook Returns False Source code in pytorch_adapt\\hooks\\utils.py 274 275 276 277 278 279 class FalseHook ( BaseConditionHook ): \"\"\"Returns ```False```\"\"\" def call ( self , inputs , losses ): \"\"\"\"\"\" return False MultiplierHook \u00b6 Bases: BaseWrapperHook Multiplies every loss by a scalar Source code in pytorch_adapt\\hooks\\utils.py 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 class MultiplierHook ( BaseWrapperHook ): \"\"\" Multiplies every loss by a scalar \"\"\" def __init__ ( self , hook : BaseHook , m : float , ** kwargs ): \"\"\" Arguments: hook: The losses of this hook will be multiplied by ```m``` m: The scalar \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . m = m def call ( self , inputs , losses ): \"\"\"\"\"\" outputs , losses = self . hook ( inputs , losses ) losses = { k : v * self . m for k , v in losses . items ()} return outputs , losses def extra_repr ( self ): return c_f . extra_repr ( self , [ \"m\" ]) __init__ ( hook , m , ** kwargs ) \u00b6 Parameters: Name Type Description Default hook BaseHook The losses of this hook will be multiplied by m required m float The scalar required Source code in pytorch_adapt\\hooks\\utils.py 339 340 341 342 343 344 345 346 347 def __init__ ( self , hook : BaseHook , m : float , ** kwargs ): \"\"\" Arguments: hook: The losses of this hook will be multiplied by ```m``` m: The scalar \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . m = m NotHook \u00b6 Bases: BaseConditionHook Returns the boolean negation of the wrapped hook. Source code in pytorch_adapt\\hooks\\utils.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 class NotHook ( BaseConditionHook ): \"\"\"Returns the boolean negation of the wrapped hook.\"\"\" def __init__ ( self , hook : BaseConditionHook , ** kwargs ): \"\"\" Arguments: hook: The condition hook that will be negated. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook def call ( self , inputs , losses ): \"\"\"\"\"\" return not self . hook ( inputs , losses ) __init__ ( hook , ** kwargs ) \u00b6 Parameters: Name Type Description Default hook BaseConditionHook The condition hook that will be negated. required Source code in pytorch_adapt\\hooks\\utils.py 285 286 287 288 289 290 291 def __init__ ( self , hook : BaseConditionHook , ** kwargs ): \"\"\" Arguments: hook: The condition hook that will be negated. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook OnlyNewOutputsHook \u00b6 Bases: BaseWrapperHook Returns only outputs that are not present in the input context. You should use this if you want to change the value of a key passed to self.hook, but not propagate that change to the outside. Source code in pytorch_adapt\\hooks\\utils.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 class OnlyNewOutputsHook ( BaseWrapperHook ): \"\"\" Returns only outputs that are not present in the input context. You should use this if you want to change the value of a key passed to self.hook, but not propagate that change to the outside. \"\"\" def __init__ ( self , hook : BaseHook , ** kwargs ): \"\"\" Arguments: hook: The hook inside which changes to the context will be allowed. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook def call ( self , inputs , losses ): \"\"\"\"\"\" outputs , losses = self . hook ( inputs , losses ) outputs = { k : outputs [ k ] for k in ( outputs . keys () - inputs . keys ())} c_f . assert_dicts_are_disjoint ( inputs , outputs ) return outputs , losses __init__ ( hook , ** kwargs ) \u00b6 Parameters: Name Type Description Default hook BaseHook The hook inside which changes to the context will be allowed. required Source code in pytorch_adapt\\hooks\\utils.py 210 211 212 213 214 215 216 def __init__ ( self , hook : BaseHook , ** kwargs ): \"\"\" Arguments: hook: The hook inside which changes to the context will be allowed. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook ParallelHook \u00b6 Bases: BaseHook Calls multiple hooks while keeping contexts separate. The Nth hook receives the same context as hooks 0 to N-1. All the output contexts are merged at the end. Source code in pytorch_adapt\\hooks\\utils.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class ParallelHook ( BaseHook ): \"\"\" Calls multiple hooks while keeping contexts separate. The Nth hook receives the same context as hooks 0 to N-1. All the output contexts are merged at the end. \"\"\" def __init__ ( self , * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: *hooks: a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . in_keys = c_f . join_lists ([ h . in_keys for h in self . hooks ]) def call ( self , inputs , losses ): \"\"\"\"\"\" outputs , out_losses = {}, {} for h in self . hooks : x = h ( inputs , losses ) outputs . update ( x [ 0 ]) out_losses . update ( x [ 1 ]) return outputs , out_losses def children_repr ( self ): x = super () . children_repr () x . update ({ \"hooks\" : self . hooks }) return x def _loss_keys ( self ): \"\"\"\"\"\" return c_f . join_lists ([ h . loss_keys for h in self . hooks ]) def _out_keys ( self ): \"\"\"\"\"\" return c_f . join_lists ([ h . out_keys for h in self . hooks ]) __init__ ( * hooks , ** kwargs ) \u00b6 Parameters: Name Type Description Default *hooks BaseHook a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. () Source code in pytorch_adapt\\hooks\\utils.py 168 169 170 171 172 173 174 175 176 def __init__ ( self , * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: *hooks: a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . in_keys = c_f . join_lists ([ h . in_keys for h in self . hooks ]) RepeatHook \u00b6 Bases: BaseHook Executes the wrapped hook n times. Source code in pytorch_adapt\\hooks\\utils.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 class RepeatHook ( BaseHook ): \"\"\" Executes the wrapped hook ```n``` times. \"\"\" def __init__ ( self , hook : BaseHook , n : int , keep_only_last : bool = False , ** kwargs ): \"\"\" Arguments: hook: The hook that will be executed ```n``` times n: The number of times the hook will be executed. keep_only_last: If ```False```, the (outputs, losses) from each execution will be accumulated, and the keys will have the iteration number appended. If ```True```, then only the (outputs, losses) of the final execution will be kept. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . n = n self . keep_only_last = keep_only_last def call ( self , inputs , losses ): \"\"\"\"\"\" outputs , losses = {}, {} for i in range ( self . n ): x = self . hook ( inputs , losses ) if self . keep_only_last and i == self . n - 1 : outputs , losses = x else : outputs . update ({ f \" { k }{ i } \" : v for k , v in x [ 0 ] . items ()}) losses . update ({ f \" { k }{ i } \" : v for k , v in x [ 1 ] . items ()}) return outputs , losses def _loss_keys ( self ): \"\"\"\"\"\" if self . keep_only_last : return self . hook . loss_keys else : return [ f \" { k }{ i } \" for k in self . hook . loss_keys for i in range ( self . n )] def _out_keys ( self ): \"\"\"\"\"\" if self . keep_only_last : return self . hook . out_keys else : return [ f \" { k }{ i } \" for k in self . hook . out_keys for i in range ( self . n )] def extra_repr ( self ): return c_f . extra_repr ( self , [ \"n\" , \"keep_only_last\" ]) __init__ ( hook , n , keep_only_last = False , ** kwargs ) \u00b6 Parameters: Name Type Description Default hook BaseHook The hook that will be executed n times required n int The number of times the hook will be executed. required keep_only_last bool If False , the (outputs, losses) from each execution will be accumulated, and the keys will have the iteration number appended. If True , then only the (outputs, losses) of the final execution will be kept. False Source code in pytorch_adapt\\hooks\\utils.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 def __init__ ( self , hook : BaseHook , n : int , keep_only_last : bool = False , ** kwargs ): \"\"\" Arguments: hook: The hook that will be executed ```n``` times n: The number of times the hook will be executed. keep_only_last: If ```False```, the (outputs, losses) from each execution will be accumulated, and the keys will have the iteration number appended. If ```True```, then only the (outputs, losses) of the final execution will be kept. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . n = n self . keep_only_last = keep_only_last TrueHook \u00b6 Bases: BaseConditionHook Returns True Source code in pytorch_adapt\\hooks\\utils.py 266 267 268 269 270 271 class TrueHook ( BaseConditionHook ): \"\"\"Returns ```True```\"\"\" def call ( self , inputs , losses ): \"\"\"\"\"\" return True ZeroLossHook \u00b6 Bases: BaseHook Returns only 0 losses and None outputs. Source code in pytorch_adapt\\hooks\\utils.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class ZeroLossHook ( BaseHook ): \"\"\" Returns only 0 losses and ```None``` outputs. \"\"\" def __init__ ( self , loss_names : List [ str ], out_names : List [ str ], ** kwargs ): \"\"\" Arguments: loss_names: The keys of the loss dictionary which will have ```tensor(0.)``` as its values. out_names: The keys of the output dictionary which will have ```None``` as its values. \"\"\" super () . __init__ ( ** kwargs ) self . loss_names = loss_names self . out_names = out_names def call ( self , inputs , losses ): \"\"\"\"\"\" out_keys = set ( self . out_names ) - inputs . keys () return ( { k : None for k in out_keys }, { k : c_f . zero_loss () for k in self . loss_names }, ) def _loss_keys ( self ): \"\"\"\"\"\" return self . loss_names def _out_keys ( self ): \"\"\"\"\"\" return self . out_names __init__ ( loss_names , out_names , ** kwargs ) \u00b6 Parameters: Name Type Description Default loss_names List [ str ] The keys of the loss dictionary which will have tensor(0.) as its values. required out_names List [ str ] The keys of the output dictionary which will have None as its values. required Source code in pytorch_adapt\\hooks\\utils.py 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , loss_names : List [ str ], out_names : List [ str ], ** kwargs ): \"\"\" Arguments: loss_names: The keys of the loss dictionary which will have ```tensor(0.)``` as its values. out_names: The keys of the output dictionary which will have ```None``` as its values. \"\"\" super () . __init__ ( ** kwargs ) self . loss_names = loss_names self . out_names = out_names","title":"utils"},{"location":"docs/hooks/utils/#pytorch_adapt.hooks.utils.ApplyFnHook","text":"Bases: BaseHook Applies a function to specific values of the context. Source code in pytorch_adapt\\hooks\\utils.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 class ApplyFnHook ( BaseHook ): \"\"\" Applies a function to specific values of the context. \"\"\" def __init__ ( self , fn : Callable , apply_to : List [ str ], is_loss : bool = False , ** kwargs ): \"\"\" Arguments: fn: The function that will be applied to the inputs. apply_to: fn will be applied to ```inputs[k]``` for k in apply_to is_loss: If False, then the returned loss dictionary will be empty. Otherwise, the returned output dictionary will be empty. \"\"\" super () . __init__ ( ** kwargs ) self . fn = fn self . apply_to = apply_to self . is_loss = is_loss def call ( self , inputs , losses ): \"\"\"\"\"\" x = c_f . extract ( inputs , self . apply_to ) outputs = { k : self . fn ( v ) for k , v in zip ( self . apply_to , x )} if self . is_loss : return outputs , {} return outputs , {} def _loss_keys ( self ): \"\"\"\"\"\" return self . apply_to if self . is_loss else [] def _out_keys ( self ): \"\"\"\"\"\" return [] if self . is_loss else self . apply_to def extra_repr ( self ): return c_f . extra_repr ( self , [ \"apply_to\" ])","title":"ApplyFnHook"},{"location":"docs/hooks/utils/#pytorch_adapt.hooks.utils.AssertHook","text":"Bases: BaseWrapperHook Asserts that the output keys of a hook match a specified regex string Source code in pytorch_adapt\\hooks\\utils.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 class AssertHook ( BaseWrapperHook ): \"\"\" Asserts that the output keys of a hook match a specified regex string \"\"\" def __init__ ( self , hook : BaseHook , allowed : str , ** kwargs ): \"\"\" Arguments: hook: The wrapped hook allowed: The output dictionary of ```hook``` must have keys that match the ```allowed``` regex. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook if not isinstance ( allowed , str ): raise TypeError ( \"allowed must be a str\" ) self . allowed = allowed def call ( self , inputs , losses ): \"\"\"\"\"\" outputs , losses = self . hook ( inputs , losses ) self . assert_fn ( outputs ) return outputs , losses def assert_fn ( self , outputs ): filtered = c_f . filter ( outputs , self . allowed ) if len ( filtered ) != len ( outputs ): error_str = f \" { c_f . cls_name ( self . hook ) } is producing outputs that don't match the allowed regex in { c_f . cls_name ( self ) } \\n \" error_str += f \"output keys = { outputs . keys () } \\n \" error_str += f \"regex filter = { self . allowed } \" raise ValueError ( error_str ) def extra_repr ( self ): return c_f . extra_repr ( self , [ \"allowed\" ])","title":"AssertHook"},{"location":"docs/hooks/utils/#pytorch_adapt.hooks.utils.ChainHook","text":"Bases: BaseHook Calls multiple hooks sequentially. The Nth hook receives the context accumulated through hooks 0 to N-1. Source code in pytorch_adapt\\hooks\\utils.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class ChainHook ( BaseHook ): \"\"\" Calls multiple hooks sequentially. The Nth hook receives the context accumulated through hooks 0 to N-1. \"\"\" def __init__ ( self , * hooks : BaseHook , conditions : List [ BaseConditionHook ] = None , alts : List [ BaseHook ] = None , overwrite : Union [ bool , List [ int ]] = False , ** kwargs , ): \"\"\" Arguments: *hooks: a sequence of hooks that will be called sequentially. conditions: an optional list of condition hooks. If conditions[i] returns False, then alts[i] is called. Otherwise hooks[i] is called. alts: an optional list of hooks that will be executed when the corresponding condition hook returns False overwrite: If True, then hooks will be allowed to overwrite keys in the context. If a list of integers, then the hooks at the specified indices will be allowed to overwrite keys in the context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . conditions = c_f . default ( conditions , [ TrueHook () for _ in range ( len ( hooks ))] ) self . alts = c_f . default ( alts , [ ZeroLossHook ( h . loss_keys , h . out_keys ) for h in self . hooks ] ) self . check_alt_keys_match_hook_keys () if not isinstance ( overwrite , ( list , bool )): raise TypeError ( \"overwrite must be a list or bool\" ) self . overwrite = overwrite self . in_keys = self . hooks [ 0 ] . in_keys def call ( self , inputs , losses ): \"\"\"\"\"\" outputs , out_losses = {}, {} all_inputs , all_losses = inputs , losses prev_outputs , prev_losses = {}, {} for i , h in enumerate ( self . hooks ): self . check_overwrite ( i , all_inputs , prev_outputs , self . overwrite ) self . check_overwrite ( i , all_losses , prev_losses , False ) all_inputs = { ** all_inputs , ** prev_outputs } all_losses = { ** all_losses , ** prev_losses } if self . conditions [ i ]( all_inputs , all_losses ): x = h ( all_inputs , all_losses ) else : x = self . alts [ i ]( all_inputs , all_losses ) prev_outputs , prev_losses = x out_losses . update ( prev_losses ) outputs . update ( prev_outputs ) return outputs , out_losses def check_overlap ( self , x , y , names ): is_overlap , overlap = c_f . dicts_are_overlapping ( x , y , return_overlap = True ) if is_overlap : raise KeyError ( f \"overwrite is false, but { names [ 0 ] } and { names [ 1 ] } have overlapping keys: { overlap } \" ) def check_overwrite ( self , i , kwargs , prev_outputs , overwrite ): if not overwrite or ( isinstance ( overwrite , list ) and i not in overwrite ): self . check_overlap ( kwargs , prev_outputs , [ \"kwargs\" , \"prev_outputs\" ]) def _loss_keys ( self ): \"\"\"\"\"\" return c_f . join_lists ([ h . loss_keys for h in self . hooks ]) def _out_keys ( self ): \"\"\"\"\"\" return c_f . join_lists ([ h . out_keys for h in self . hooks ]) @property def last_hook_out_keys ( self ): return self . hooks [ - 1 ] . out_keys def check_alt_keys_match_hook_keys ( self ): for i in range ( len ( self . hooks )): h = self . hooks [ i ] a = self . alts [ i ] if ( sorted ( h . loss_keys ) != sorted ( a . loss_keys )) or ( sorted ( h . out_keys ) != sorted ( a . out_keys ) ): raise ValueError ( \"alt loss/out keys must be equal to hook loss/out keys\" ) def children_repr ( self ): x = super () . children_repr () x [ \"hooks\" ] = self . hooks if any ( not isinstance ( c , TrueHook ) for c in self . conditions ): x . update ({ \"conditions\" : self . conditions , \"alts\" : self . alts }) return x","title":"ChainHook"},{"location":"docs/hooks/utils/#pytorch_adapt.hooks.utils.EmptyHook","text":"Bases: BaseHook Returns two empty dictionaries. Source code in pytorch_adapt\\hooks\\utils.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class EmptyHook ( BaseHook ): \"\"\"Returns two empty dictionaries.\"\"\" def call ( self , inputs , losses ): \"\"\"\"\"\" return {}, {} def _loss_keys ( self ): \"\"\"\"\"\" return [] def _out_keys ( self ): \"\"\"\"\"\" return []","title":"EmptyHook"},{"location":"docs/hooks/utils/#pytorch_adapt.hooks.utils.FalseHook","text":"Bases: BaseConditionHook Returns False Source code in pytorch_adapt\\hooks\\utils.py 274 275 276 277 278 279 class FalseHook ( BaseConditionHook ): \"\"\"Returns ```False```\"\"\" def call ( self , inputs , losses ): \"\"\"\"\"\" return False","title":"FalseHook"},{"location":"docs/hooks/utils/#pytorch_adapt.hooks.utils.MultiplierHook","text":"Bases: BaseWrapperHook Multiplies every loss by a scalar Source code in pytorch_adapt\\hooks\\utils.py 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 class MultiplierHook ( BaseWrapperHook ): \"\"\" Multiplies every loss by a scalar \"\"\" def __init__ ( self , hook : BaseHook , m : float , ** kwargs ): \"\"\" Arguments: hook: The losses of this hook will be multiplied by ```m``` m: The scalar \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . m = m def call ( self , inputs , losses ): \"\"\"\"\"\" outputs , losses = self . hook ( inputs , losses ) losses = { k : v * self . m for k , v in losses . items ()} return outputs , losses def extra_repr ( self ): return c_f . extra_repr ( self , [ \"m\" ])","title":"MultiplierHook"},{"location":"docs/hooks/utils/#pytorch_adapt.hooks.utils.NotHook","text":"Bases: BaseConditionHook Returns the boolean negation of the wrapped hook. Source code in pytorch_adapt\\hooks\\utils.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 class NotHook ( BaseConditionHook ): \"\"\"Returns the boolean negation of the wrapped hook.\"\"\" def __init__ ( self , hook : BaseConditionHook , ** kwargs ): \"\"\" Arguments: hook: The condition hook that will be negated. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook def call ( self , inputs , losses ): \"\"\"\"\"\" return not self . hook ( inputs , losses )","title":"NotHook"},{"location":"docs/hooks/utils/#pytorch_adapt.hooks.utils.OnlyNewOutputsHook","text":"Bases: BaseWrapperHook Returns only outputs that are not present in the input context. You should use this if you want to change the value of a key passed to self.hook, but not propagate that change to the outside. Source code in pytorch_adapt\\hooks\\utils.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 class OnlyNewOutputsHook ( BaseWrapperHook ): \"\"\" Returns only outputs that are not present in the input context. You should use this if you want to change the value of a key passed to self.hook, but not propagate that change to the outside. \"\"\" def __init__ ( self , hook : BaseHook , ** kwargs ): \"\"\" Arguments: hook: The hook inside which changes to the context will be allowed. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook def call ( self , inputs , losses ): \"\"\"\"\"\" outputs , losses = self . hook ( inputs , losses ) outputs = { k : outputs [ k ] for k in ( outputs . keys () - inputs . keys ())} c_f . assert_dicts_are_disjoint ( inputs , outputs ) return outputs , losses","title":"OnlyNewOutputsHook"},{"location":"docs/hooks/utils/#pytorch_adapt.hooks.utils.ParallelHook","text":"Bases: BaseHook Calls multiple hooks while keeping contexts separate. The Nth hook receives the same context as hooks 0 to N-1. All the output contexts are merged at the end. Source code in pytorch_adapt\\hooks\\utils.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class ParallelHook ( BaseHook ): \"\"\" Calls multiple hooks while keeping contexts separate. The Nth hook receives the same context as hooks 0 to N-1. All the output contexts are merged at the end. \"\"\" def __init__ ( self , * hooks : BaseHook , ** kwargs ): \"\"\" Arguments: *hooks: a sequence of hooks that will be called sequentially, with each hook receiving the same initial context. \"\"\" super () . __init__ ( ** kwargs ) self . hooks = hooks self . in_keys = c_f . join_lists ([ h . in_keys for h in self . hooks ]) def call ( self , inputs , losses ): \"\"\"\"\"\" outputs , out_losses = {}, {} for h in self . hooks : x = h ( inputs , losses ) outputs . update ( x [ 0 ]) out_losses . update ( x [ 1 ]) return outputs , out_losses def children_repr ( self ): x = super () . children_repr () x . update ({ \"hooks\" : self . hooks }) return x def _loss_keys ( self ): \"\"\"\"\"\" return c_f . join_lists ([ h . loss_keys for h in self . hooks ]) def _out_keys ( self ): \"\"\"\"\"\" return c_f . join_lists ([ h . out_keys for h in self . hooks ])","title":"ParallelHook"},{"location":"docs/hooks/utils/#pytorch_adapt.hooks.utils.RepeatHook","text":"Bases: BaseHook Executes the wrapped hook n times. Source code in pytorch_adapt\\hooks\\utils.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 class RepeatHook ( BaseHook ): \"\"\" Executes the wrapped hook ```n``` times. \"\"\" def __init__ ( self , hook : BaseHook , n : int , keep_only_last : bool = False , ** kwargs ): \"\"\" Arguments: hook: The hook that will be executed ```n``` times n: The number of times the hook will be executed. keep_only_last: If ```False```, the (outputs, losses) from each execution will be accumulated, and the keys will have the iteration number appended. If ```True```, then only the (outputs, losses) of the final execution will be kept. \"\"\" super () . __init__ ( ** kwargs ) self . hook = hook self . n = n self . keep_only_last = keep_only_last def call ( self , inputs , losses ): \"\"\"\"\"\" outputs , losses = {}, {} for i in range ( self . n ): x = self . hook ( inputs , losses ) if self . keep_only_last and i == self . n - 1 : outputs , losses = x else : outputs . update ({ f \" { k }{ i } \" : v for k , v in x [ 0 ] . items ()}) losses . update ({ f \" { k }{ i } \" : v for k , v in x [ 1 ] . items ()}) return outputs , losses def _loss_keys ( self ): \"\"\"\"\"\" if self . keep_only_last : return self . hook . loss_keys else : return [ f \" { k }{ i } \" for k in self . hook . loss_keys for i in range ( self . n )] def _out_keys ( self ): \"\"\"\"\"\" if self . keep_only_last : return self . hook . out_keys else : return [ f \" { k }{ i } \" for k in self . hook . out_keys for i in range ( self . n )] def extra_repr ( self ): return c_f . extra_repr ( self , [ \"n\" , \"keep_only_last\" ])","title":"RepeatHook"},{"location":"docs/hooks/utils/#pytorch_adapt.hooks.utils.TrueHook","text":"Bases: BaseConditionHook Returns True Source code in pytorch_adapt\\hooks\\utils.py 266 267 268 269 270 271 class TrueHook ( BaseConditionHook ): \"\"\"Returns ```True```\"\"\" def call ( self , inputs , losses ): \"\"\"\"\"\" return True","title":"TrueHook"},{"location":"docs/hooks/utils/#pytorch_adapt.hooks.utils.ZeroLossHook","text":"Bases: BaseHook Returns only 0 losses and None outputs. Source code in pytorch_adapt\\hooks\\utils.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class ZeroLossHook ( BaseHook ): \"\"\" Returns only 0 losses and ```None``` outputs. \"\"\" def __init__ ( self , loss_names : List [ str ], out_names : List [ str ], ** kwargs ): \"\"\" Arguments: loss_names: The keys of the loss dictionary which will have ```tensor(0.)``` as its values. out_names: The keys of the output dictionary which will have ```None``` as its values. \"\"\" super () . __init__ ( ** kwargs ) self . loss_names = loss_names self . out_names = out_names def call ( self , inputs , losses ): \"\"\"\"\"\" out_keys = set ( self . out_names ) - inputs . keys () return ( { k : None for k in out_keys }, { k : c_f . zero_loss () for k in self . loss_names }, ) def _loss_keys ( self ): \"\"\"\"\"\" return self . loss_names def _out_keys ( self ): \"\"\"\"\"\" return self . out_names","title":"ZeroLossHook"},{"location":"docs/hooks/vada/","text":"VADAHook \u00b6 Bases: GANHook Implementation of VADA from A DIRT-T Approach to Unsupervised Domain Adaptation . Source code in pytorch_adapt\\hooks\\vada.py 51 52 53 54 55 56 57 58 59 60 class VADAHook ( GANHook ): \"\"\" Implementation of VADA from [A DIRT-T Approach to Unsupervised Domain Adaptation](https://arxiv.org/abs/1802.08735). \"\"\" def __init__ ( self , vat_loss_fn = None , entropy_loss_fn = None , post_g = None , ** kwargs ): post_g = c_f . default ( post_g , []) post_g += [ VATPlusEntropyHook ( vat_loss_fn , entropy_loss_fn )] super () . __init__ ( post_g = post_g , ** kwargs ) VATHook \u00b6 Bases: BaseWrapperHook Applies the VATLoss . Source code in pytorch_adapt\\hooks\\vada.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class VATHook ( BaseWrapperHook ): \"\"\" Applies the [```VATLoss```][pytorch_adapt.layers.VATLoss]. \"\"\" def __init__ ( self , loss_fn = None , ** kwargs ): super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , VATLoss , {}) self . hook = FeaturesAndLogitsHook () def call ( self , inputs , losses ): outputs = self . hook ( inputs , losses )[ 0 ] [ src_imgs , target_imgs , combined_model ] = c_f . extract ( inputs , [ \"src_imgs\" , \"target_imgs\" , \"combined_model\" ] ) [ src_logits , target_logits ] = c_f . extract ( [ outputs , inputs ], c_f . filter ( self . hook . out_keys , \"_logits$\" , [ \"^src\" , \"^target\" ]), ) src_vat_loss = self . loss_fn ( src_imgs , src_logits , combined_model ) target_vat_loss = self . loss_fn ( target_imgs , target_logits , combined_model ) return ( outputs , { \"src_vat_loss\" : src_vat_loss , \"target_vat_loss\" : target_vat_loss , }, ) def _loss_keys ( self ): return [ \"src_vat_loss\" , \"target_vat_loss\" ]","title":"vada"},{"location":"docs/hooks/vada/#pytorch_adapt.hooks.vada.VADAHook","text":"Bases: GANHook Implementation of VADA from A DIRT-T Approach to Unsupervised Domain Adaptation . Source code in pytorch_adapt\\hooks\\vada.py 51 52 53 54 55 56 57 58 59 60 class VADAHook ( GANHook ): \"\"\" Implementation of VADA from [A DIRT-T Approach to Unsupervised Domain Adaptation](https://arxiv.org/abs/1802.08735). \"\"\" def __init__ ( self , vat_loss_fn = None , entropy_loss_fn = None , post_g = None , ** kwargs ): post_g = c_f . default ( post_g , []) post_g += [ VATPlusEntropyHook ( vat_loss_fn , entropy_loss_fn )] super () . __init__ ( post_g = post_g , ** kwargs )","title":"VADAHook"},{"location":"docs/hooks/vada/#pytorch_adapt.hooks.vada.VATHook","text":"Bases: BaseWrapperHook Applies the VATLoss . Source code in pytorch_adapt\\hooks\\vada.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class VATHook ( BaseWrapperHook ): \"\"\" Applies the [```VATLoss```][pytorch_adapt.layers.VATLoss]. \"\"\" def __init__ ( self , loss_fn = None , ** kwargs ): super () . __init__ ( ** kwargs ) self . loss_fn = c_f . default ( loss_fn , VATLoss , {}) self . hook = FeaturesAndLogitsHook () def call ( self , inputs , losses ): outputs = self . hook ( inputs , losses )[ 0 ] [ src_imgs , target_imgs , combined_model ] = c_f . extract ( inputs , [ \"src_imgs\" , \"target_imgs\" , \"combined_model\" ] ) [ src_logits , target_logits ] = c_f . extract ( [ outputs , inputs ], c_f . filter ( self . hook . out_keys , \"_logits$\" , [ \"^src\" , \"^target\" ]), ) src_vat_loss = self . loss_fn ( src_imgs , src_logits , combined_model ) target_vat_loss = self . loss_fn ( target_imgs , target_logits , combined_model ) return ( outputs , { \"src_vat_loss\" : src_vat_loss , \"target_vat_loss\" : target_vat_loss , }, ) def _loss_keys ( self ): return [ \"src_vat_loss\" , \"target_vat_loss\" ]","title":"VATHook"},{"location":"docs/hooks/validate/","text":"validate_hook ( hook , available_keys = None , depth = 0 , model_counts = None ) \u00b6 Parameters: Name Type Description Default hook the hook to validate required available_keys a list of keys that the context will start with. None Returns: Type Description Dict [ str , int ] A dictionary with each model's forward call count. Source code in pytorch_adapt\\hooks\\validate.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def validate_hook ( hook , available_keys = None , depth = 0 , model_counts = None ) -> Dict [ str , int ]: \"\"\" Arguments: hook: the hook to validate available_keys: a list of keys that the context will start with. Returns: A dictionary with each model's ```forward``` call count. \"\"\" c_f . LOGGER . debug ( f \"VALIDATE: { ' ' * depth }{ c_f . cls_name ( hook ) } \" ) available_keys = c_f . default ( available_keys , []) model_counts = c_f . default ( model_counts , defaultdict ( int )) if isinstance ( available_keys , list ): available_keys = set ( available_keys ) if isinstance ( hook , ChainHook ): hooks = hook . hooks for i in range ( 0 , len ( hooks )): validate_hook ( hooks [ i ], available_keys , depth + 1 , model_counts ) elif isinstance ( hook , ParallelHook ): hooks = hook . hooks for i in range ( 0 , len ( hooks )): curr_available_keys = copy . deepcopy ( available_keys ) validate_hook ( hooks [ i ], curr_available_keys , depth + 1 , model_counts ) elif isinstance ( hook , RepeatHook ): for _ in range ( hook . n ): curr_available_keys = copy . deepcopy ( available_keys ) validate_hook ( hook . hook , curr_available_keys , depth + 1 , model_counts ) else : check_keys_are_present ( hook , hook . in_keys , list ( available_keys ), \"in_keys\" , \"available_keys\" ) check_keys_are_present ( hook , list ( hook . key_map . keys ()), list ( available_keys ), \"key_map\" , \"available_keys\" , ) all_hooks = c_f . attrs_of_type ( hook , BaseHook ) for h in all_hooks . values (): validate_hook ( h , available_keys , depth + 1 , model_counts ) update_model_counts ( hook , available_keys , model_counts ) available_keys . update ( set ( hook . out_keys )) return model_counts","title":"validate"},{"location":"docs/hooks/validate/#pytorch_adapt.hooks.validate.validate_hook","text":"Parameters: Name Type Description Default hook the hook to validate required available_keys a list of keys that the context will start with. None Returns: Type Description Dict [ str , int ] A dictionary with each model's forward call count. Source code in pytorch_adapt\\hooks\\validate.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def validate_hook ( hook , available_keys = None , depth = 0 , model_counts = None ) -> Dict [ str , int ]: \"\"\" Arguments: hook: the hook to validate available_keys: a list of keys that the context will start with. Returns: A dictionary with each model's ```forward``` call count. \"\"\" c_f . LOGGER . debug ( f \"VALIDATE: { ' ' * depth }{ c_f . cls_name ( hook ) } \" ) available_keys = c_f . default ( available_keys , []) model_counts = c_f . default ( model_counts , defaultdict ( int )) if isinstance ( available_keys , list ): available_keys = set ( available_keys ) if isinstance ( hook , ChainHook ): hooks = hook . hooks for i in range ( 0 , len ( hooks )): validate_hook ( hooks [ i ], available_keys , depth + 1 , model_counts ) elif isinstance ( hook , ParallelHook ): hooks = hook . hooks for i in range ( 0 , len ( hooks )): curr_available_keys = copy . deepcopy ( available_keys ) validate_hook ( hooks [ i ], curr_available_keys , depth + 1 , model_counts ) elif isinstance ( hook , RepeatHook ): for _ in range ( hook . n ): curr_available_keys = copy . deepcopy ( available_keys ) validate_hook ( hook . hook , curr_available_keys , depth + 1 , model_counts ) else : check_keys_are_present ( hook , hook . in_keys , list ( available_keys ), \"in_keys\" , \"available_keys\" ) check_keys_are_present ( hook , list ( hook . key_map . keys ()), list ( available_keys ), \"key_map\" , \"available_keys\" , ) all_hooks = c_f . attrs_of_type ( hook , BaseHook ) for h in all_hooks . values (): validate_hook ( h , available_keys , depth + 1 , model_counts ) update_model_counts ( hook , available_keys , model_counts ) available_keys . update ( set ( hook . out_keys )) return model_counts","title":"validate_hook()"},{"location":"docs/inference/","text":"The following can be imported like this (using adabn_fn as an example): from pytorch_adapt.inference import adabn_fn Direct module members \u00b6 adabn_fn adda_fn adda_full_fn adda_with_d cdan_full_fn d_bridge_fn default_fn default_with_d default_with_d_logits_layer gvb_full_fn gvb_with_g_bridge mcd_fn mcd_full_fn rtn_fn rtn_full_fn rtn_with_feature_combiner symnets_fn symnets_full_fn with_d with_d_bridge with_feature_combiner","title":"inference"},{"location":"docs/inference/#direct-module-members","text":"adabn_fn adda_fn adda_full_fn adda_with_d cdan_full_fn d_bridge_fn default_fn default_with_d default_with_d_logits_layer gvb_full_fn gvb_with_g_bridge mcd_fn mcd_full_fn rtn_fn rtn_full_fn rtn_with_feature_combiner symnets_fn symnets_full_fn with_d with_d_bridge with_feature_combiner","title":"Direct module members"},{"location":"docs/inference/inference/","text":"adabn_fn ( x , domain , models , ** kwargs ) \u00b6 AdaBN features and logits. Parameters: Name Type Description Default x The input to the model required domain 0 for source domain, 1 for target domain. required Source code in pytorch_adapt\\inference\\inference.py 18 19 20 21 22 23 24 25 26 27 28 def adabn_fn ( x , domain , models , ** kwargs ) -> Dict [ str , torch . Tensor ]: \"\"\" [AdaBN][pytorch_adapt.adapters.AdaBN] features and logits. Arguments: x: The input to the model domain: 0 for source domain, 1 for target domain. \"\"\" domain = check_domain ( domain , keep_len = True ) features = models [ \"G\" ]( x , domain ) logits = models [ \"C\" ]( features , domain ) return { \"features\" : features , \"logits\" : logits } adda_fn ( x , domain , models , get_all = False , ** kwargs ) \u00b6 ADDA features and logits. Parameters: Name Type Description Default x The input to the model required domain If 0, then features = G(x) . Otherwise features = T(x) . required models Dictionary of models with keys [\"G\", \"C\", \"T\"] . required get_all bool If True , then return features and logits using both G and T as the feature extractor. False Returns: Type Description Dict [ str , torch . Tensor ] A dictionary of features and logits. Dict [ str , torch . Tensor ] If get_all is False , then the keys are {\"features\", \"logits\"} . Dict [ str , torch . Tensor ] If get_all is True , Dict [ str , torch . Tensor ] then the keys will be {\"features\", \"logits\", \"other_features\", \"other_logits\"} , Dict [ str , torch . Tensor ] where the other_ prefix represents the features and logits obtained Dict [ str , torch . Tensor ] using G if domain == 1 and T if domain == 0 . Source code in pytorch_adapt\\inference\\inference.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def adda_fn ( x , domain , models , get_all : bool = False , ** kwargs ) -> Dict [ str , torch . Tensor ]: \"\"\" [ADDA][pytorch_adapt.adapters.ADDA] features and logits. Arguments: x: The input to the model domain: If 0, then ```features = G(x)```. Otherwise ```features = T(x)```. models: Dictionary of models with keys ```[\"G\", \"C\", \"T\"]```. get_all: If ```True```, then return features and logits using both ```G``` and ```T``` as the feature extractor. Returns: A dictionary of features and logits. - If ```get_all``` is ```False```, then the keys are ```{\"features\", \"logits\"}```. - If ```get_all``` is ```True```, then the keys will be ```{\"features\", \"logits\", \"other_features\", \"other_logits\"}```, where the ```other_``` prefix represents the features and logits obtained using ```G``` if ```domain == 1``` and ```T``` if ```domain == 0```. \"\"\" domain = check_domain ( domain ) fe = \"G\" if domain == 0 else \"T\" features = models [ fe ]( x ) logits = models [ \"C\" ]( features ) output = { \"features\" : features , \"logits\" : logits } if get_all : fe = \"T\" if fe == \"G\" else \"G\" features = models [ fe ]( x ) logits = models [ \"C\" ]( features ) output . update ({ \"other_features\" : features , \"other_logits\" : logits }) return output adda_full_fn ( x , ** kwargs ) \u00b6 ADDA features, logits, discriminator logits, other features, other logits, other discriminator logits. See adda_fn for the input arguments. Returns: Type Description Dict [ str , torch . Tensor ] discriminator logits ( \"d_logits\" ), \"other\" discriminator logits ( \"other_d_logits\" ) Dict [ str , torch . Tensor ] in addition to everything returned by adda_fn Dict [ str , torch . Tensor ] with get_all = True . Source code in pytorch_adapt\\inference\\inference.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def adda_full_fn ( x , ** kwargs ) -> Dict [ str , torch . Tensor ]: \"\"\" [ADDA][pytorch_adapt.adapters.ADDA] features, logits, discriminator logits, other features, other logits, other discriminator logits. See [adda_fn][pytorch_adapt.inference.adda_fn] for the input arguments. Returns: discriminator logits (```\"d_logits\"```), \"other\" discriminator logits (```\"other_d_logits\"```) in addition to everything returned by [adda_fn][pytorch_adapt.inference.adda_fn] with ```get_all = True```. \"\"\" layer = kwargs . get ( \"layer\" , \"features\" ) output = with_d ( x = x , fn = adda_fn , get_all = True , ** kwargs ) output2 = d_fn ( x = output [ f \"other_ { layer } \" ], ** kwargs ) output [ \"other_d_logits\" ] = output2 [ \"d_logits\" ] return output adda_with_d ( ** kwargs ) \u00b6 ADDA features, logits, and discriminator logits. See adda_fn for the input arguments. Returns: Type Description Dict [ str , torch . Tensor ] discriminator logits as \"d_logits\" , in addition to Dict [ str , torch . Tensor ] everything returned by adda_fn . Source code in pytorch_adapt\\inference\\inference.py 68 69 70 71 72 73 74 75 76 def adda_with_d ( ** kwargs ) -> Dict [ str , torch . Tensor ]: \"\"\" [ADDA][pytorch_adapt.adapters.ADDA] features, logits, and discriminator logits. See [adda_fn][pytorch_adapt.inference.adda_fn] for the input arguments. Returns: discriminator logits as ```\"d_logits\"```, in addition to everything returned by [adda_fn][pytorch_adapt.inference.adda_fn]. \"\"\" return with_d ( fn = adda_fn , ** kwargs ) default_fn ( x , models , ** kwargs ) \u00b6 The default inference function for BaseAdapter . Source code in pytorch_adapt\\inference\\inference.py 8 9 10 11 12 13 14 def default_fn ( x , models , ** kwargs ) -> Dict [ str , torch . Tensor ]: \"\"\" The default inference function for [BaseAdapter][pytorch_adapt.adapters.BaseAdapter]. \"\"\" features = models [ \"G\" ]( x ) logits = models [ \"C\" ]( features ) return { \"features\" : features , \"logits\" : logits } mcd_fn ( x , models , get_all = False , ** kwargs ) \u00b6 Returns: Type Description Features and logits, where logits = sum(C(features)) . Source code in pytorch_adapt\\inference\\inference.py 140 141 142 143 144 145 146 147 148 149 150 151 152 def mcd_fn ( x , models , get_all = False , ** kwargs ): \"\"\" Returns: Features and logits, where ```logits = sum(C(features))```. \"\"\" features = models [ \"G\" ]( x ) logits_list = models [ \"C\" ]( features ) logits = sum ( logits_list ) output = { \"features\" : features , \"logits\" : logits } if get_all : for i , L in enumerate ( logits_list ): output [ f \"logits { i } \" ] = L return output rtn_fn ( x , domain , models , get_all = False , ** kwargs ) \u00b6 RTN features and logits. Parameters: Name Type Description Default x The input to the model required domain If 0, logits = residual_model(C(G(x))) . Otherwise, logits = C(G(x)) . required models Dictionary of models with keys [\"G\", \"C\", \"residual_model\"] . required get_all If True , then in addition to the regular outputs, it will return the residual_model logits when domain == 1 and the C logits when domain == 0 . False Returns: Type Description Dict [ str , torch . Tensor ] A dictionary of features and logits. Dict [ str , torch . Tensor ] If get_all is False , then the keys are {\"features\", \"logits\"} . Dict [ str , torch . Tensor ] If get_all is True , Dict [ str , torch . Tensor ] then the keys will be {\"features\", \"logits\", \"other_logits\"} . Source code in pytorch_adapt\\inference\\inference.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def rtn_fn ( x , domain , models , get_all = False , ** kwargs ) -> Dict [ str , torch . Tensor ]: \"\"\" [RTN][pytorch_adapt.adapters.RTN] features and logits. Arguments: x: The input to the model domain: If 0, ```logits = residual_model(C(G(x)))```. Otherwise, ```logits = C(G(x))```. models: Dictionary of models with keys ```[\"G\", \"C\", \"residual_model\"]```. get_all: If ```True```, then in addition to the regular outputs, it will return the ```residual_model``` logits when ```domain == 1``` and the ```C``` logits when ```domain == 0```. Returns: A dictionary of features and logits. - If ```get_all``` is ```False```, then the keys are ```{\"features\", \"logits\"}```. - If ```get_all``` is ```True```, then the keys will be ```{\"features\", \"logits\", \"other_logits\"}```. \"\"\" domain = check_domain ( domain ) f_dict = default_fn ( x = x , models = models ) target_logits = f_dict [ \"logits\" ] if get_all or domain == 0 : src_logits = models [ \"residual_model\" ]( target_logits ) if domain == 0 : f_dict [ \"logits\" ] = src_logits if get_all : f_dict [ \"other_logits\" ] = target_logits elif get_all and domain == 1 : f_dict [ \"other_logits\" ] = src_logits return f_dict symnets_fn ( x , domain , models , get_all = False , ** kwargs ) \u00b6 Parameters: Name Type Description Default x The input to the model required domain 0 for the source domain, 1 for the target domain. required Returns: Type Description Features and logits, where logits = C(features)[domain] . Source code in pytorch_adapt\\inference\\inference.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def symnets_fn ( x , domain , models , get_all = False , ** kwargs ): \"\"\" Arguments: x: The input to the model domain: 0 for the source domain, 1 for the target domain. Returns: Features and logits, where ```logits = C(features)[domain]```. \"\"\" domain = check_domain ( domain ) features = models [ \"G\" ]( x ) logits = models [ \"C\" ]( features )[ domain ] output = { \"features\" : features , \"logits\" : logits } if get_all : logits = models [ \"C\" ]( features )[ int ( not domain )] output . update ({ \"other_logits\" : logits }) return output","title":"inference"},{"location":"docs/inference/inference/#pytorch_adapt.inference.inference.adabn_fn","text":"AdaBN features and logits. Parameters: Name Type Description Default x The input to the model required domain 0 for source domain, 1 for target domain. required Source code in pytorch_adapt\\inference\\inference.py 18 19 20 21 22 23 24 25 26 27 28 def adabn_fn ( x , domain , models , ** kwargs ) -> Dict [ str , torch . Tensor ]: \"\"\" [AdaBN][pytorch_adapt.adapters.AdaBN] features and logits. Arguments: x: The input to the model domain: 0 for source domain, 1 for target domain. \"\"\" domain = check_domain ( domain , keep_len = True ) features = models [ \"G\" ]( x , domain ) logits = models [ \"C\" ]( features , domain ) return { \"features\" : features , \"logits\" : logits }","title":"adabn_fn()"},{"location":"docs/inference/inference/#pytorch_adapt.inference.inference.adda_fn","text":"ADDA features and logits. Parameters: Name Type Description Default x The input to the model required domain If 0, then features = G(x) . Otherwise features = T(x) . required models Dictionary of models with keys [\"G\", \"C\", \"T\"] . required get_all bool If True , then return features and logits using both G and T as the feature extractor. False Returns: Type Description Dict [ str , torch . Tensor ] A dictionary of features and logits. Dict [ str , torch . Tensor ] If get_all is False , then the keys are {\"features\", \"logits\"} . Dict [ str , torch . Tensor ] If get_all is True , Dict [ str , torch . Tensor ] then the keys will be {\"features\", \"logits\", \"other_features\", \"other_logits\"} , Dict [ str , torch . Tensor ] where the other_ prefix represents the features and logits obtained Dict [ str , torch . Tensor ] using G if domain == 1 and T if domain == 0 . Source code in pytorch_adapt\\inference\\inference.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def adda_fn ( x , domain , models , get_all : bool = False , ** kwargs ) -> Dict [ str , torch . Tensor ]: \"\"\" [ADDA][pytorch_adapt.adapters.ADDA] features and logits. Arguments: x: The input to the model domain: If 0, then ```features = G(x)```. Otherwise ```features = T(x)```. models: Dictionary of models with keys ```[\"G\", \"C\", \"T\"]```. get_all: If ```True```, then return features and logits using both ```G``` and ```T``` as the feature extractor. Returns: A dictionary of features and logits. - If ```get_all``` is ```False```, then the keys are ```{\"features\", \"logits\"}```. - If ```get_all``` is ```True```, then the keys will be ```{\"features\", \"logits\", \"other_features\", \"other_logits\"}```, where the ```other_``` prefix represents the features and logits obtained using ```G``` if ```domain == 1``` and ```T``` if ```domain == 0```. \"\"\" domain = check_domain ( domain ) fe = \"G\" if domain == 0 else \"T\" features = models [ fe ]( x ) logits = models [ \"C\" ]( features ) output = { \"features\" : features , \"logits\" : logits } if get_all : fe = \"T\" if fe == \"G\" else \"G\" features = models [ fe ]( x ) logits = models [ \"C\" ]( features ) output . update ({ \"other_features\" : features , \"other_logits\" : logits }) return output","title":"adda_fn()"},{"location":"docs/inference/inference/#pytorch_adapt.inference.inference.adda_full_fn","text":"ADDA features, logits, discriminator logits, other features, other logits, other discriminator logits. See adda_fn for the input arguments. Returns: Type Description Dict [ str , torch . Tensor ] discriminator logits ( \"d_logits\" ), \"other\" discriminator logits ( \"other_d_logits\" ) Dict [ str , torch . Tensor ] in addition to everything returned by adda_fn Dict [ str , torch . Tensor ] with get_all = True . Source code in pytorch_adapt\\inference\\inference.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def adda_full_fn ( x , ** kwargs ) -> Dict [ str , torch . Tensor ]: \"\"\" [ADDA][pytorch_adapt.adapters.ADDA] features, logits, discriminator logits, other features, other logits, other discriminator logits. See [adda_fn][pytorch_adapt.inference.adda_fn] for the input arguments. Returns: discriminator logits (```\"d_logits\"```), \"other\" discriminator logits (```\"other_d_logits\"```) in addition to everything returned by [adda_fn][pytorch_adapt.inference.adda_fn] with ```get_all = True```. \"\"\" layer = kwargs . get ( \"layer\" , \"features\" ) output = with_d ( x = x , fn = adda_fn , get_all = True , ** kwargs ) output2 = d_fn ( x = output [ f \"other_ { layer } \" ], ** kwargs ) output [ \"other_d_logits\" ] = output2 [ \"d_logits\" ] return output","title":"adda_full_fn()"},{"location":"docs/inference/inference/#pytorch_adapt.inference.inference.adda_with_d","text":"ADDA features, logits, and discriminator logits. See adda_fn for the input arguments. Returns: Type Description Dict [ str , torch . Tensor ] discriminator logits as \"d_logits\" , in addition to Dict [ str , torch . Tensor ] everything returned by adda_fn . Source code in pytorch_adapt\\inference\\inference.py 68 69 70 71 72 73 74 75 76 def adda_with_d ( ** kwargs ) -> Dict [ str , torch . Tensor ]: \"\"\" [ADDA][pytorch_adapt.adapters.ADDA] features, logits, and discriminator logits. See [adda_fn][pytorch_adapt.inference.adda_fn] for the input arguments. Returns: discriminator logits as ```\"d_logits\"```, in addition to everything returned by [adda_fn][pytorch_adapt.inference.adda_fn]. \"\"\" return with_d ( fn = adda_fn , ** kwargs )","title":"adda_with_d()"},{"location":"docs/inference/inference/#pytorch_adapt.inference.inference.default_fn","text":"The default inference function for BaseAdapter . Source code in pytorch_adapt\\inference\\inference.py 8 9 10 11 12 13 14 def default_fn ( x , models , ** kwargs ) -> Dict [ str , torch . Tensor ]: \"\"\" The default inference function for [BaseAdapter][pytorch_adapt.adapters.BaseAdapter]. \"\"\" features = models [ \"G\" ]( x ) logits = models [ \"C\" ]( features ) return { \"features\" : features , \"logits\" : logits }","title":"default_fn()"},{"location":"docs/inference/inference/#pytorch_adapt.inference.inference.mcd_fn","text":"Returns: Type Description Features and logits, where logits = sum(C(features)) . Source code in pytorch_adapt\\inference\\inference.py 140 141 142 143 144 145 146 147 148 149 150 151 152 def mcd_fn ( x , models , get_all = False , ** kwargs ): \"\"\" Returns: Features and logits, where ```logits = sum(C(features))```. \"\"\" features = models [ \"G\" ]( x ) logits_list = models [ \"C\" ]( features ) logits = sum ( logits_list ) output = { \"features\" : features , \"logits\" : logits } if get_all : for i , L in enumerate ( logits_list ): output [ f \"logits { i } \" ] = L return output","title":"mcd_fn()"},{"location":"docs/inference/inference/#pytorch_adapt.inference.inference.rtn_fn","text":"RTN features and logits. Parameters: Name Type Description Default x The input to the model required domain If 0, logits = residual_model(C(G(x))) . Otherwise, logits = C(G(x)) . required models Dictionary of models with keys [\"G\", \"C\", \"residual_model\"] . required get_all If True , then in addition to the regular outputs, it will return the residual_model logits when domain == 1 and the C logits when domain == 0 . False Returns: Type Description Dict [ str , torch . Tensor ] A dictionary of features and logits. Dict [ str , torch . Tensor ] If get_all is False , then the keys are {\"features\", \"logits\"} . Dict [ str , torch . Tensor ] If get_all is True , Dict [ str , torch . Tensor ] then the keys will be {\"features\", \"logits\", \"other_logits\"} . Source code in pytorch_adapt\\inference\\inference.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def rtn_fn ( x , domain , models , get_all = False , ** kwargs ) -> Dict [ str , torch . Tensor ]: \"\"\" [RTN][pytorch_adapt.adapters.RTN] features and logits. Arguments: x: The input to the model domain: If 0, ```logits = residual_model(C(G(x)))```. Otherwise, ```logits = C(G(x))```. models: Dictionary of models with keys ```[\"G\", \"C\", \"residual_model\"]```. get_all: If ```True```, then in addition to the regular outputs, it will return the ```residual_model``` logits when ```domain == 1``` and the ```C``` logits when ```domain == 0```. Returns: A dictionary of features and logits. - If ```get_all``` is ```False```, then the keys are ```{\"features\", \"logits\"}```. - If ```get_all``` is ```True```, then the keys will be ```{\"features\", \"logits\", \"other_logits\"}```. \"\"\" domain = check_domain ( domain ) f_dict = default_fn ( x = x , models = models ) target_logits = f_dict [ \"logits\" ] if get_all or domain == 0 : src_logits = models [ \"residual_model\" ]( target_logits ) if domain == 0 : f_dict [ \"logits\" ] = src_logits if get_all : f_dict [ \"other_logits\" ] = target_logits elif get_all and domain == 1 : f_dict [ \"other_logits\" ] = src_logits return f_dict","title":"rtn_fn()"},{"location":"docs/inference/inference/#pytorch_adapt.inference.inference.symnets_fn","text":"Parameters: Name Type Description Default x The input to the model required domain 0 for the source domain, 1 for the target domain. required Returns: Type Description Features and logits, where logits = C(features)[domain] . Source code in pytorch_adapt\\inference\\inference.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def symnets_fn ( x , domain , models , get_all = False , ** kwargs ): \"\"\" Arguments: x: The input to the model domain: 0 for the source domain, 1 for the target domain. Returns: Features and logits, where ```logits = C(features)[domain]```. \"\"\" domain = check_domain ( domain ) features = models [ \"G\" ]( x ) logits = models [ \"C\" ]( features )[ domain ] output = { \"features\" : features , \"logits\" : logits } if get_all : logits = models [ \"C\" ]( features )[ int ( not domain )] output . update ({ \"other_logits\" : logits }) return output","title":"symnets_fn()"},{"location":"docs/layers/","text":"The following can be imported like this (using AbsLoss as an example): from pytorch_adapt.layers import AbsLoss Direct module members \u00b6 AbsLoss AdaBNModel AdaptiveBatchNorm2d AdaptiveFeatureNorm BNMLoss BatchSpectralLoss CORALLoss ConcatSoftmax ConfidenceWeights DiversityLoss DoNothingOptimizer EntropyLoss EntropyWeights GeneralMCDLoss GradientReversal ISTLoss L2PreservedDropout MCCLoss MCDLoss MMDBatchedLoss MMDLoss MaxNormalizer MeanDistLoss MinMaxNormalizer ModelWithBridge MultipleModels NLLLoss NeighborhoodAggregation NoNormalizer PlusResidual PopulationBatchNorm2d RandomizedDotProduct SilhouetteScore SlicedWasserstein StochasticLinear SufficientAccuracy SumNormalizer SymNetsCategoryLoss SymNetsCategoryLossListInput SymNetsDomainLoss SymNetsEntropyLoss SymNetsEntropyLossListInput UniformDistributionLoss VATLoss","title":"layers"},{"location":"docs/layers/#direct-module-members","text":"AbsLoss AdaBNModel AdaptiveBatchNorm2d AdaptiveFeatureNorm BNMLoss BatchSpectralLoss CORALLoss ConcatSoftmax ConfidenceWeights DiversityLoss DoNothingOptimizer EntropyLoss EntropyWeights GeneralMCDLoss GradientReversal ISTLoss L2PreservedDropout MCCLoss MCDLoss MMDBatchedLoss MMDLoss MaxNormalizer MeanDistLoss MinMaxNormalizer ModelWithBridge MultipleModels NLLLoss NeighborhoodAggregation NoNormalizer PlusResidual PopulationBatchNorm2d RandomizedDotProduct SilhouetteScore SlicedWasserstein StochasticLinear SufficientAccuracy SumNormalizer SymNetsCategoryLoss SymNetsCategoryLossListInput SymNetsDomainLoss SymNetsEntropyLoss SymNetsEntropyLossListInput UniformDistributionLoss VATLoss","title":"Direct module members"},{"location":"docs/layers/abs_loss/","text":"AbsLoss \u00b6 Bases: torch . nn . Module The mean absolute value. Source code in pytorch_adapt\\layers\\abs_loss.py 4 5 6 7 8 9 10 11 class AbsLoss ( torch . nn . Module ): \"\"\" The mean absolute value. \"\"\" def forward ( self , x ): \"\"\"\"\"\" return torch . mean ( torch . abs ( x ))","title":"abs_loss"},{"location":"docs/layers/abs_loss/#pytorch_adapt.layers.abs_loss.AbsLoss","text":"Bases: torch . nn . Module The mean absolute value. Source code in pytorch_adapt\\layers\\abs_loss.py 4 5 6 7 8 9 10 11 class AbsLoss ( torch . nn . Module ): \"\"\" The mean absolute value. \"\"\" def forward ( self , x ): \"\"\"\"\"\" return torch . mean ( torch . abs ( x ))","title":"AbsLoss"},{"location":"docs/layers/adaptive_feature_norm/","text":"AdaptiveFeatureNorm \u00b6 Bases: torch . nn . Module Implementation of the loss in Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation . Encourages features to gradually have larger and larger L2 norms. Source code in pytorch_adapt\\layers\\adaptive_feature_norm.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class AdaptiveFeatureNorm ( torch . nn . Module ): \"\"\" Implementation of the loss in [Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation](https://arxiv.org/abs/1811.07456). Encourages features to gradually have larger and larger L2 norms. \"\"\" def __init__ ( self , step_size : float = 1 ): \"\"\" Arguments: step_size: The desired increase in L2 norm at each iteration. Note that the loss will always be equal to ```step_size``` because the goal is always to make the L2 norm ```step_size``` larger than whatever the current L2 norm is. \"\"\" super () . __init__ () self . step_size = step_size def forward ( self , x ): \"\"\"\"\"\" l2_norm = x . norm ( p = 2 , dim = 1 ) radius = l2_norm . detach () + self . step_size return torch . mean (( l2_norm - radius ) ** 2 ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"step_size\" ]) __init__ ( step_size = 1 ) \u00b6 Parameters: Name Type Description Default step_size float The desired increase in L2 norm at each iteration. Note that the loss will always be equal to step_size because the goal is always to make the L2 norm step_size larger than whatever the current L2 norm is. 1 Source code in pytorch_adapt\\layers\\adaptive_feature_norm.py 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , step_size : float = 1 ): \"\"\" Arguments: step_size: The desired increase in L2 norm at each iteration. Note that the loss will always be equal to ```step_size``` because the goal is always to make the L2 norm ```step_size``` larger than whatever the current L2 norm is. \"\"\" super () . __init__ () self . step_size = step_size L2PreservedDropout \u00b6 Bases: torch . nn . Module Implementation of the dropout layer described in Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation . Regular dropout preserves the L1 norm of features, whereas this layer preserves the L2 norm. Source code in pytorch_adapt\\layers\\adaptive_feature_norm.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class L2PreservedDropout ( torch . nn . Module ): \"\"\" Implementation of the dropout layer described in [Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation](https://arxiv.org/abs/1811.07456). Regular dropout preserves the L1 norm of features, whereas this layer preserves the L2 norm. \"\"\" def __init__ ( self , p : float = 0.5 , inplace : bool = False ): \"\"\" Arguments: p: probability of an element to be zeroed inplace: if set to True, will do this operation in-place \"\"\" super () . __init__ () self . dropout = torch . nn . Dropout ( p = p , inplace = inplace ) self . scale = math . sqrt ( 1 - p ) def forward ( self , x ): \"\"\"\"\"\" x = self . dropout ( x ) if self . training : return x * self . scale return x __init__ ( p = 0.5 , inplace = False ) \u00b6 Parameters: Name Type Description Default p float probability of an element to be zeroed 0.5 inplace bool if set to True, will do this operation in-place False Source code in pytorch_adapt\\layers\\adaptive_feature_norm.py 50 51 52 53 54 55 56 57 58 def __init__ ( self , p : float = 0.5 , inplace : bool = False ): \"\"\" Arguments: p: probability of an element to be zeroed inplace: if set to True, will do this operation in-place \"\"\" super () . __init__ () self . dropout = torch . nn . Dropout ( p = p , inplace = inplace ) self . scale = math . sqrt ( 1 - p )","title":"adaptive_feature_norm"},{"location":"docs/layers/adaptive_feature_norm/#pytorch_adapt.layers.adaptive_feature_norm.AdaptiveFeatureNorm","text":"Bases: torch . nn . Module Implementation of the loss in Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation . Encourages features to gradually have larger and larger L2 norms. Source code in pytorch_adapt\\layers\\adaptive_feature_norm.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class AdaptiveFeatureNorm ( torch . nn . Module ): \"\"\" Implementation of the loss in [Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation](https://arxiv.org/abs/1811.07456). Encourages features to gradually have larger and larger L2 norms. \"\"\" def __init__ ( self , step_size : float = 1 ): \"\"\" Arguments: step_size: The desired increase in L2 norm at each iteration. Note that the loss will always be equal to ```step_size``` because the goal is always to make the L2 norm ```step_size``` larger than whatever the current L2 norm is. \"\"\" super () . __init__ () self . step_size = step_size def forward ( self , x ): \"\"\"\"\"\" l2_norm = x . norm ( p = 2 , dim = 1 ) radius = l2_norm . detach () + self . step_size return torch . mean (( l2_norm - radius ) ** 2 ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"step_size\" ])","title":"AdaptiveFeatureNorm"},{"location":"docs/layers/adaptive_feature_norm/#pytorch_adapt.layers.adaptive_feature_norm.L2PreservedDropout","text":"Bases: torch . nn . Module Implementation of the dropout layer described in Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation . Regular dropout preserves the L1 norm of features, whereas this layer preserves the L2 norm. Source code in pytorch_adapt\\layers\\adaptive_feature_norm.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class L2PreservedDropout ( torch . nn . Module ): \"\"\" Implementation of the dropout layer described in [Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation](https://arxiv.org/abs/1811.07456). Regular dropout preserves the L1 norm of features, whereas this layer preserves the L2 norm. \"\"\" def __init__ ( self , p : float = 0.5 , inplace : bool = False ): \"\"\" Arguments: p: probability of an element to be zeroed inplace: if set to True, will do this operation in-place \"\"\" super () . __init__ () self . dropout = torch . nn . Dropout ( p = p , inplace = inplace ) self . scale = math . sqrt ( 1 - p ) def forward ( self , x ): \"\"\"\"\"\" x = self . dropout ( x ) if self . training : return x * self . scale return x","title":"L2PreservedDropout"},{"location":"docs/layers/batch_spectral_loss/","text":"BatchSpectralLoss \u00b6 Bases: torch . nn . Module Implementation of the loss in Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation . The loss is the sum of the squares of the first k singular values. Source code in pytorch_adapt\\layers\\batch_spectral_loss.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class BatchSpectralLoss ( torch . nn . Module ): \"\"\" Implementation of the loss in [Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation](http://proceedings.mlr.press/v97/chen19i.html). The loss is the sum of the squares of the first k singular values. \"\"\" def __init__ ( self , k : int = 1 ): \"\"\" Arguments: k: the number of singular values to include in the loss \"\"\" super () . __init__ () self . k = k def forward ( self , x ): \"\"\"\"\"\" return batch_spectral_loss ( x , self . k ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"k\" ]) __init__ ( k = 1 ) \u00b6 Parameters: Name Type Description Default k int the number of singular values to include in the loss 1 Source code in pytorch_adapt\\layers\\batch_spectral_loss.py 19 20 21 22 23 24 25 def __init__ ( self , k : int = 1 ): \"\"\" Arguments: k: the number of singular values to include in the loss \"\"\" super () . __init__ () self . k = k","title":"batch_spectral_loss"},{"location":"docs/layers/batch_spectral_loss/#pytorch_adapt.layers.batch_spectral_loss.BatchSpectralLoss","text":"Bases: torch . nn . Module Implementation of the loss in Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation . The loss is the sum of the squares of the first k singular values. Source code in pytorch_adapt\\layers\\batch_spectral_loss.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class BatchSpectralLoss ( torch . nn . Module ): \"\"\" Implementation of the loss in [Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation](http://proceedings.mlr.press/v97/chen19i.html). The loss is the sum of the squares of the first k singular values. \"\"\" def __init__ ( self , k : int = 1 ): \"\"\" Arguments: k: the number of singular values to include in the loss \"\"\" super () . __init__ () self . k = k def forward ( self , x ): \"\"\"\"\"\" return batch_spectral_loss ( x , self . k ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"k\" ])","title":"BatchSpectralLoss"},{"location":"docs/layers/bnm_loss/","text":"BNMLoss \u00b6 Bases: torch . nn . Module Implementation of the loss in Towards Discriminability and Diversity: Batch Nuclear-norm Maximization under Label Insufficient Situations . Source code in pytorch_adapt\\layers\\bnm_loss.py 4 5 6 7 8 9 10 11 12 13 14 15 class BNMLoss ( torch . nn . Module ): \"\"\" Implementation of the loss in [Towards Discriminability and Diversity: Batch Nuclear-norm Maximization under Label Insufficient Situations](https://arxiv.org/abs/2003.12237). \"\"\" def forward ( self , x ): \"\"\"\"\"\" x = torch . nn . functional . softmax ( x , dim = 1 ) return - torch . linalg . norm ( x , \"nuc\" ) / x . shape [ 0 ]","title":"bnm_loss"},{"location":"docs/layers/bnm_loss/#pytorch_adapt.layers.bnm_loss.BNMLoss","text":"Bases: torch . nn . Module Implementation of the loss in Towards Discriminability and Diversity: Batch Nuclear-norm Maximization under Label Insufficient Situations . Source code in pytorch_adapt\\layers\\bnm_loss.py 4 5 6 7 8 9 10 11 12 13 14 15 class BNMLoss ( torch . nn . Module ): \"\"\" Implementation of the loss in [Towards Discriminability and Diversity: Batch Nuclear-norm Maximization under Label Insufficient Situations](https://arxiv.org/abs/2003.12237). \"\"\" def forward ( self , x ): \"\"\"\"\"\" x = torch . nn . functional . softmax ( x , dim = 1 ) return - torch . linalg . norm ( x , \"nuc\" ) / x . shape [ 0 ]","title":"BNMLoss"},{"location":"docs/layers/concat_softmax/","text":"ConcatSoftmax \u00b6 Bases: torch . nn . Module Applies softmax to the concatenation of a list of tensors. Source code in pytorch_adapt\\layers\\concat_softmax.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class ConcatSoftmax ( torch . nn . Module ): \"\"\" Applies softmax to the concatenation of a list of tensors. \"\"\" def __init__ ( self , dim : int = 1 ): \"\"\" Arguments: dim: a dimension along which softmax will be computed \"\"\" super () . __init__ () self . dim = dim def forward ( self , * x : torch . Tensor ): \"\"\" Arguments: *x: A sequence of tensors to be concatenated \"\"\" all_logits = torch . cat ( x , dim = self . dim ) return torch . nn . functional . softmax ( all_logits , dim = self . dim ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"dim\" ]) __init__ ( dim = 1 ) \u00b6 Parameters: Name Type Description Default dim int a dimension along which softmax will be computed 1 Source code in pytorch_adapt\\layers\\concat_softmax.py 11 12 13 14 15 16 17 def __init__ ( self , dim : int = 1 ): \"\"\" Arguments: dim: a dimension along which softmax will be computed \"\"\" super () . __init__ () self . dim = dim forward ( * x ) \u00b6 Parameters: Name Type Description Default *x torch . Tensor A sequence of tensors to be concatenated () Source code in pytorch_adapt\\layers\\concat_softmax.py 19 20 21 22 23 24 25 def forward ( self , * x : torch . Tensor ): \"\"\" Arguments: *x: A sequence of tensors to be concatenated \"\"\" all_logits = torch . cat ( x , dim = self . dim ) return torch . nn . functional . softmax ( all_logits , dim = self . dim )","title":"concat_softmax"},{"location":"docs/layers/concat_softmax/#pytorch_adapt.layers.concat_softmax.ConcatSoftmax","text":"Bases: torch . nn . Module Applies softmax to the concatenation of a list of tensors. Source code in pytorch_adapt\\layers\\concat_softmax.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class ConcatSoftmax ( torch . nn . Module ): \"\"\" Applies softmax to the concatenation of a list of tensors. \"\"\" def __init__ ( self , dim : int = 1 ): \"\"\" Arguments: dim: a dimension along which softmax will be computed \"\"\" super () . __init__ () self . dim = dim def forward ( self , * x : torch . Tensor ): \"\"\" Arguments: *x: A sequence of tensors to be concatenated \"\"\" all_logits = torch . cat ( x , dim = self . dim ) return torch . nn . functional . softmax ( all_logits , dim = self . dim ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"dim\" ])","title":"ConcatSoftmax"},{"location":"docs/layers/confidence_weights/","text":"ConfidenceWeights \u00b6 Bases: torch . nn . Module Returns the max value along each row of the input, followed by an optional normalization function. The output of this can be used to weight classification losses by the \"confidence\" of the predictions. Source code in pytorch_adapt\\layers\\confidence_weights.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class ConfidenceWeights ( torch . nn . Module ): \"\"\" Returns the max value along each row of the input, followed by an optional normalization function. The output of this can be used to weight classification losses by the \"confidence\" of the predictions. \"\"\" def __init__ ( self , normalizer : Callable [[ torch . Tensor ], torch . Tensor ] = None ): \"\"\" Arguments: normalizer: A callable for normalizing (e.g. min-max normalization) the weights. If ```None```, then no normalization is used. \"\"\" super () . __init__ () self . normalizer = c_f . default ( normalizer , NoNormalizer ()) def forward ( self , preds ): \"\"\"\"\"\" return self . normalizer ( torch . max ( preds , dim = 1 )[ 0 ]) __init__ ( normalizer = None ) \u00b6 Parameters: Name Type Description Default normalizer Callable [[ torch . Tensor ], torch . Tensor ] A callable for normalizing (e.g. min-max normalization) the weights. If None , then no normalization is used. None Source code in pytorch_adapt\\layers\\confidence_weights.py 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , normalizer : Callable [[ torch . Tensor ], torch . Tensor ] = None ): \"\"\" Arguments: normalizer: A callable for normalizing (e.g. min-max normalization) the weights. If ```None```, then no normalization is used. \"\"\" super () . __init__ () self . normalizer = c_f . default ( normalizer , NoNormalizer ())","title":"confidence_weights"},{"location":"docs/layers/confidence_weights/#pytorch_adapt.layers.confidence_weights.ConfidenceWeights","text":"Bases: torch . nn . Module Returns the max value along each row of the input, followed by an optional normalization function. The output of this can be used to weight classification losses by the \"confidence\" of the predictions. Source code in pytorch_adapt\\layers\\confidence_weights.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class ConfidenceWeights ( torch . nn . Module ): \"\"\" Returns the max value along each row of the input, followed by an optional normalization function. The output of this can be used to weight classification losses by the \"confidence\" of the predictions. \"\"\" def __init__ ( self , normalizer : Callable [[ torch . Tensor ], torch . Tensor ] = None ): \"\"\" Arguments: normalizer: A callable for normalizing (e.g. min-max normalization) the weights. If ```None```, then no normalization is used. \"\"\" super () . __init__ () self . normalizer = c_f . default ( normalizer , NoNormalizer ()) def forward ( self , preds ): \"\"\"\"\"\" return self . normalizer ( torch . max ( preds , dim = 1 )[ 0 ])","title":"ConfidenceWeights"},{"location":"docs/layers/coral_loss/","text":"CORALLoss \u00b6 Bases: torch . nn . Module Implementation of Deep CORAL: Correlation Alignment for Deep Domain Adaptation Source code in pytorch_adapt\\layers\\coral_loss.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class CORALLoss ( torch . nn . Module ): \"\"\" Implementation of [Deep CORAL: Correlation Alignment for Deep Domain Adaptation](https://arxiv.org/abs/1607.01719) \"\"\" def forward ( self , x : torch . Tensor , y : torch . Tensor ): \"\"\" Arguments: x: features from one domain y: features from the other domain \"\"\" embedding_size = x . shape [ 1 ] cx = covariance ( x ) cy = covariance ( y ) squared_fro_norm = torch . linalg . norm ( cx - cy , ord = \"fro\" ) ** 2 return squared_fro_norm / ( 4 * ( embedding_size ** 2 )) forward ( x , y ) \u00b6 Parameters: Name Type Description Default x torch . Tensor features from one domain required y torch . Tensor features from the other domain required Source code in pytorch_adapt\\layers\\coral_loss.py 19 20 21 22 23 24 25 26 27 28 29 def forward ( self , x : torch . Tensor , y : torch . Tensor ): \"\"\" Arguments: x: features from one domain y: features from the other domain \"\"\" embedding_size = x . shape [ 1 ] cx = covariance ( x ) cy = covariance ( y ) squared_fro_norm = torch . linalg . norm ( cx - cy , ord = \"fro\" ) ** 2 return squared_fro_norm / ( 4 * ( embedding_size ** 2 ))","title":"coral_loss"},{"location":"docs/layers/coral_loss/#pytorch_adapt.layers.coral_loss.CORALLoss","text":"Bases: torch . nn . Module Implementation of Deep CORAL: Correlation Alignment for Deep Domain Adaptation Source code in pytorch_adapt\\layers\\coral_loss.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class CORALLoss ( torch . nn . Module ): \"\"\" Implementation of [Deep CORAL: Correlation Alignment for Deep Domain Adaptation](https://arxiv.org/abs/1607.01719) \"\"\" def forward ( self , x : torch . Tensor , y : torch . Tensor ): \"\"\" Arguments: x: features from one domain y: features from the other domain \"\"\" embedding_size = x . shape [ 1 ] cx = covariance ( x ) cy = covariance ( y ) squared_fro_norm = torch . linalg . norm ( cx - cy , ord = \"fro\" ) ** 2 return squared_fro_norm / ( 4 * ( embedding_size ** 2 ))","title":"CORALLoss"},{"location":"docs/layers/diversity_loss/","text":"DiversityLoss \u00b6 Bases: torch . nn . Module Encourages predictions to be uniform, batch wise. Takes logits (before softmax) as input. For example: A tensor with a large loss: torch.tensor([[1e4, 0, 0], [1e4, 0, 0], [1e4, 0, 0]]) A tensor with a small loss: torch.tensor([[1e4, 0, 0], [0, 1e4, 0], [0, 0, 1e4]]) Source code in pytorch_adapt\\layers\\diversity_loss.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class DiversityLoss ( torch . nn . Module ): \"\"\" Encourages predictions to be uniform, batch wise. Takes logits (before softmax) as input. For example: - A tensor with a large loss: ```torch.tensor([[1e4, 0, 0], [1e4, 0, 0], [1e4, 0, 0]])``` - A tensor with a small loss: ```torch.tensor([[1e4, 0, 0], [0, 1e4, 0], [0, 0, 1e4]])``` \"\"\" def __init__ ( self , after_softmax : bool = False ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. \"\"\" super () . __init__ () self . after_softmax = after_softmax def forward ( self , logits ): \"\"\"\"\"\" if not self . after_softmax : logits = torch . softmax ( logits , dim = 1 ) logits = torch . mean ( logits , dim = 0 , keepdim = True ) return - torch . mean ( get_entropy ( logits , after_softmax = True )) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"after_softmax\" ]) __init__ ( after_softmax = False ) \u00b6 Parameters: Name Type Description Default after_softmax bool If True , then the rows of the input are assumed to already have softmax applied to them. False Source code in pytorch_adapt\\layers\\diversity_loss.py 19 20 21 22 23 24 25 26 def __init__ ( self , after_softmax : bool = False ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. \"\"\" super () . __init__ () self . after_softmax = after_softmax","title":"diversity_loss"},{"location":"docs/layers/diversity_loss/#pytorch_adapt.layers.diversity_loss.DiversityLoss","text":"Bases: torch . nn . Module Encourages predictions to be uniform, batch wise. Takes logits (before softmax) as input. For example: A tensor with a large loss: torch.tensor([[1e4, 0, 0], [1e4, 0, 0], [1e4, 0, 0]]) A tensor with a small loss: torch.tensor([[1e4, 0, 0], [0, 1e4, 0], [0, 0, 1e4]]) Source code in pytorch_adapt\\layers\\diversity_loss.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class DiversityLoss ( torch . nn . Module ): \"\"\" Encourages predictions to be uniform, batch wise. Takes logits (before softmax) as input. For example: - A tensor with a large loss: ```torch.tensor([[1e4, 0, 0], [1e4, 0, 0], [1e4, 0, 0]])``` - A tensor with a small loss: ```torch.tensor([[1e4, 0, 0], [0, 1e4, 0], [0, 0, 1e4]])``` \"\"\" def __init__ ( self , after_softmax : bool = False ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. \"\"\" super () . __init__ () self . after_softmax = after_softmax def forward ( self , logits ): \"\"\"\"\"\" if not self . after_softmax : logits = torch . softmax ( logits , dim = 1 ) logits = torch . mean ( logits , dim = 0 , keepdim = True ) return - torch . mean ( get_entropy ( logits , after_softmax = True )) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"after_softmax\" ])","title":"DiversityLoss"},{"location":"docs/layers/do_nothing_optimizer/","text":"DoNothingOptimizer \u00b6 Bases: torch . optim . Optimizer An optimizer that doesn't do anything, i.e. step and zero_grad are empty functions. Source code in pytorch_adapt\\layers\\do_nothing_optimizer.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class DoNothingOptimizer ( torch . optim . Optimizer ): \"\"\" An optimizer that doesn't do anything, i.e. ```step``` and ```zero_grad``` are empty functions. \"\"\" def __init__ ( self , * arg , ** kwargs ): self . param_groups = [{ \"lr\" : 0 }] def step ( self ): \"\"\"\"\"\" pass def zero_grad ( self ): \"\"\"\"\"\" pass def state_dict ( self ): \"\"\"\"\"\" return {} def load_state_dict ( self , state_dict ): \"\"\"\"\"\" pass def __repr__ ( self ): return \"DoNothingOptimizer()\"","title":"do_nothing_optimizer"},{"location":"docs/layers/do_nothing_optimizer/#pytorch_adapt.layers.do_nothing_optimizer.DoNothingOptimizer","text":"Bases: torch . optim . Optimizer An optimizer that doesn't do anything, i.e. step and zero_grad are empty functions. Source code in pytorch_adapt\\layers\\do_nothing_optimizer.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class DoNothingOptimizer ( torch . optim . Optimizer ): \"\"\" An optimizer that doesn't do anything, i.e. ```step``` and ```zero_grad``` are empty functions. \"\"\" def __init__ ( self , * arg , ** kwargs ): self . param_groups = [{ \"lr\" : 0 }] def step ( self ): \"\"\"\"\"\" pass def zero_grad ( self ): \"\"\"\"\"\" pass def state_dict ( self ): \"\"\"\"\"\" return {} def load_state_dict ( self , state_dict ): \"\"\"\"\"\" pass def __repr__ ( self ): return \"DoNothingOptimizer()\"","title":"DoNothingOptimizer"},{"location":"docs/layers/entropy_loss/","text":"EntropyLoss \u00b6 Bases: torch . nn . Module Encourages low entropy predictions, or in other words, \"confident\" predictions. Source code in pytorch_adapt\\layers\\entropy_loss.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class EntropyLoss ( torch . nn . Module ): \"\"\" Encourages low entropy predictions, or in other words, \"confident\" predictions. \"\"\" def __init__ ( self , after_softmax : bool = False , return_mean : bool = True ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. return_mean: If ```True```, the mean entropy will be returned. If ```False```, the entropy per row of the input will be returned. \"\"\" super () . __init__ () self . after_softmax = after_softmax self . return_mean = return_mean def forward ( self , logits : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: logits: Raw logits if ```self.after_softmax``` is False. Otherwise each row should be predictions that sum up to 1. \"\"\" entropies = get_entropy ( logits , self . after_softmax ) if self . return_mean : return torch . mean ( entropies ) return entropies def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"after_softmax\" ]) __init__ ( after_softmax = False , return_mean = True ) \u00b6 Parameters: Name Type Description Default after_softmax bool If True , then the rows of the input are assumed to already have softmax applied to them. False return_mean bool If True , the mean entropy will be returned. If False , the entropy per row of the input will be returned. True Source code in pytorch_adapt\\layers\\entropy_loss.py 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , after_softmax : bool = False , return_mean : bool = True ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. return_mean: If ```True```, the mean entropy will be returned. If ```False```, the entropy per row of the input will be returned. \"\"\" super () . __init__ () self . after_softmax = after_softmax self . return_mean = return_mean forward ( logits ) \u00b6 Parameters: Name Type Description Default logits torch . Tensor Raw logits if self.after_softmax is False. Otherwise each row should be predictions that sum up to 1. required Source code in pytorch_adapt\\layers\\entropy_loss.py 43 44 45 46 47 48 49 50 51 52 def forward ( self , logits : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: logits: Raw logits if ```self.after_softmax``` is False. Otherwise each row should be predictions that sum up to 1. \"\"\" entropies = get_entropy ( logits , self . after_softmax ) if self . return_mean : return torch . mean ( entropies ) return entropies","title":"entropy_loss"},{"location":"docs/layers/entropy_loss/#pytorch_adapt.layers.entropy_loss.EntropyLoss","text":"Bases: torch . nn . Module Encourages low entropy predictions, or in other words, \"confident\" predictions. Source code in pytorch_adapt\\layers\\entropy_loss.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class EntropyLoss ( torch . nn . Module ): \"\"\" Encourages low entropy predictions, or in other words, \"confident\" predictions. \"\"\" def __init__ ( self , after_softmax : bool = False , return_mean : bool = True ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. return_mean: If ```True```, the mean entropy will be returned. If ```False```, the entropy per row of the input will be returned. \"\"\" super () . __init__ () self . after_softmax = after_softmax self . return_mean = return_mean def forward ( self , logits : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: logits: Raw logits if ```self.after_softmax``` is False. Otherwise each row should be predictions that sum up to 1. \"\"\" entropies = get_entropy ( logits , self . after_softmax ) if self . return_mean : return torch . mean ( entropies ) return entropies def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"after_softmax\" ])","title":"EntropyLoss"},{"location":"docs/layers/entropy_weights/","text":"EntropyWeights \u00b6 Bases: torch . nn . Module Implementation of entropy weighting described in Conditional Adversarial Domain Adaptation . Computes the entropy ( x ) per row of the input, and returns 1+exp(-x) . This can be used to weight losses, such that the most confidently scored samples have a higher weighting. Source code in pytorch_adapt\\layers\\entropy_weights.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class EntropyWeights ( torch . nn . Module ): \"\"\" Implementation of entropy weighting described in [Conditional Adversarial Domain Adaptation](https://arxiv.org/abs/1705.10667). Computes the entropy (```x```) per row of the input, and returns ```1+exp(-x)```. This can be used to weight losses, such that the most confidently scored samples have a higher weighting. \"\"\" def __init__ ( self , after_softmax : bool = False , normalizer : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. normalizer: A callable for normalizing (e.g. min-max normalization) the weights. If ```None```, then sum normalization is used. \"\"\" super () . __init__ () self . after_softmax = after_softmax self . normalizer = c_f . default ( normalizer , SumNormalizer , {}) def forward ( self , logits : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: logits: Raw logits if ```self.after_softmax``` is False. Otherwise each row should be predictions that sum up to 1. \"\"\" return entropy_weights ( logits , self . after_softmax , self . normalizer ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"after_softmax\" ]) __init__ ( after_softmax = False , normalizer = None ) \u00b6 Parameters: Name Type Description Default after_softmax bool If True , then the rows of the input are assumed to already have softmax applied to them. False normalizer Callable [[ torch . Tensor ], torch . Tensor ] A callable for normalizing (e.g. min-max normalization) the weights. If None , then sum normalization is used. None Source code in pytorch_adapt\\layers\\entropy_weights.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , after_softmax : bool = False , normalizer : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. normalizer: A callable for normalizing (e.g. min-max normalization) the weights. If ```None```, then sum normalization is used. \"\"\" super () . __init__ () self . after_softmax = after_softmax self . normalizer = c_f . default ( normalizer , SumNormalizer , {}) forward ( logits ) \u00b6 Parameters: Name Type Description Default logits torch . Tensor Raw logits if self.after_softmax is False. Otherwise each row should be predictions that sum up to 1. required Source code in pytorch_adapt\\layers\\entropy_weights.py 43 44 45 46 47 48 49 def forward ( self , logits : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: logits: Raw logits if ```self.after_softmax``` is False. Otherwise each row should be predictions that sum up to 1. \"\"\" return entropy_weights ( logits , self . after_softmax , self . normalizer )","title":"entropy_weights"},{"location":"docs/layers/entropy_weights/#pytorch_adapt.layers.entropy_weights.EntropyWeights","text":"Bases: torch . nn . Module Implementation of entropy weighting described in Conditional Adversarial Domain Adaptation . Computes the entropy ( x ) per row of the input, and returns 1+exp(-x) . This can be used to weight losses, such that the most confidently scored samples have a higher weighting. Source code in pytorch_adapt\\layers\\entropy_weights.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class EntropyWeights ( torch . nn . Module ): \"\"\" Implementation of entropy weighting described in [Conditional Adversarial Domain Adaptation](https://arxiv.org/abs/1705.10667). Computes the entropy (```x```) per row of the input, and returns ```1+exp(-x)```. This can be used to weight losses, such that the most confidently scored samples have a higher weighting. \"\"\" def __init__ ( self , after_softmax : bool = False , normalizer : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: after_softmax: If ```True```, then the rows of the input are assumed to already have softmax applied to them. normalizer: A callable for normalizing (e.g. min-max normalization) the weights. If ```None```, then sum normalization is used. \"\"\" super () . __init__ () self . after_softmax = after_softmax self . normalizer = c_f . default ( normalizer , SumNormalizer , {}) def forward ( self , logits : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: logits: Raw logits if ```self.after_softmax``` is False. Otherwise each row should be predictions that sum up to 1. \"\"\" return entropy_weights ( logits , self . after_softmax , self . normalizer ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"after_softmax\" ])","title":"EntropyWeights"},{"location":"docs/layers/gradient_reversal/","text":"GradientReversal \u00b6 Bases: torch . nn . Module Implementation of the gradient reversal layer described in Domain-Adversarial Training of Neural Networks , which 'leaves the input unchanged during forward propagation and reverses the gradient by multiplying it by a negative scalar during backpropagation.' Source code in pytorch_adapt\\layers\\gradient_reversal.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class GradientReversal ( torch . nn . Module ): \"\"\" Implementation of the gradient reversal layer described in [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818), which 'leaves the input unchanged during forward propagation and reverses the gradient by multiplying it by a negative scalar during backpropagation.' \"\"\" def __init__ ( self , weight : float = 1.0 ): \"\"\" Arguments: weight: The gradients will be multiplied by ```-weight``` during the backward pass. \"\"\" super () . __init__ () self . register_buffer ( \"weight\" , torch . tensor ([ weight ])) pml_cf . add_to_recordable_attributes ( self , \"weight\" ) def update_weight ( self , new_weight ): self . weight [ 0 ] = new_weight def forward ( self , x ): \"\"\"\"\"\" return _GradientReversal . apply ( x , pml_cf . to_device ( self . weight , x )) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"weight\" ]) __init__ ( weight = 1.0 ) \u00b6 Parameters: Name Type Description Default weight float The gradients will be multiplied by -weight during the backward pass. 1.0 Source code in pytorch_adapt\\layers\\gradient_reversal.py 16 17 18 19 20 21 22 23 24 def __init__ ( self , weight : float = 1.0 ): \"\"\" Arguments: weight: The gradients will be multiplied by ```-weight``` during the backward pass. \"\"\" super () . __init__ () self . register_buffer ( \"weight\" , torch . tensor ([ weight ])) pml_cf . add_to_recordable_attributes ( self , \"weight\" )","title":"gradient_reversal"},{"location":"docs/layers/gradient_reversal/#pytorch_adapt.layers.gradient_reversal.GradientReversal","text":"Bases: torch . nn . Module Implementation of the gradient reversal layer described in Domain-Adversarial Training of Neural Networks , which 'leaves the input unchanged during forward propagation and reverses the gradient by multiplying it by a negative scalar during backpropagation.' Source code in pytorch_adapt\\layers\\gradient_reversal.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class GradientReversal ( torch . nn . Module ): \"\"\" Implementation of the gradient reversal layer described in [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818), which 'leaves the input unchanged during forward propagation and reverses the gradient by multiplying it by a negative scalar during backpropagation.' \"\"\" def __init__ ( self , weight : float = 1.0 ): \"\"\" Arguments: weight: The gradients will be multiplied by ```-weight``` during the backward pass. \"\"\" super () . __init__ () self . register_buffer ( \"weight\" , torch . tensor ([ weight ])) pml_cf . add_to_recordable_attributes ( self , \"weight\" ) def update_weight ( self , new_weight ): self . weight [ 0 ] = new_weight def forward ( self , x ): \"\"\"\"\"\" return _GradientReversal . apply ( x , pml_cf . to_device ( self . weight , x )) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"weight\" ])","title":"GradientReversal"},{"location":"docs/layers/ist_loss/","text":"ISTLoss \u00b6 Bases: torch . nn . Module Implementation of the I_st loss from Information-Theoretical Learning of Discriminative Clusters for Unsupervised Domain Adaptation Source code in pytorch_adapt\\layers\\ist_loss.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class ISTLoss ( torch . nn . Module ): \"\"\" Implementation of the I_st loss from [Information-Theoretical Learning of Discriminative Clusters for Unsupervised Domain Adaptation](https://icml.cc/2012/papers/566.pdf) \"\"\" def __init__ ( self , distance = None , with_ent = True , with_div = True ): super () . __init__ () self . distance = c_f . default ( distance , CosineSimilarity , {}) if not ( with_ent or with_div ): raise ValueError ( \"At least one of with_ent or with_div must be True\" ) self . with_ent = with_ent self . with_div = with_div self . ent_loss_fn = EntropyLoss ( after_softmax = True ) self . div_loss_fn = DiversityLoss ( after_softmax = True ) def forward ( self , x , y ): \"\"\" Arguments: x: source and target features y: domain labels, i.e. 0 for source domain, 1 for target domain \"\"\" n = x . shape [ 0 ] if torch . min ( y ) < 0 or torch . max ( y ) > 1 : raise ValueError ( \"y must be in the range 0 and 1\" ) if y . shape != torch . Size ([ n ]): raise TypeError ( \"y must have shape (N,)\" ) mat = self . distance ( x ) # remove self comparisons mask = ~ torch . eye ( n , dtype = torch . bool ) mat = mat [ mask ] . view ( n , n - 1 ) probs = get_probs ( mat , mask , y , self . distance . is_inverted ) return get_loss ( probs , self . ent_loss_fn , self . div_loss_fn , self . with_ent , self . with_div ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"with_div\" ]) forward ( x , y ) \u00b6 Parameters: Name Type Description Default x source and target features required y domain labels, i.e. 0 for source domain, 1 for target domain required Source code in pytorch_adapt\\layers\\ist_loss.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def forward ( self , x , y ): \"\"\" Arguments: x: source and target features y: domain labels, i.e. 0 for source domain, 1 for target domain \"\"\" n = x . shape [ 0 ] if torch . min ( y ) < 0 or torch . max ( y ) > 1 : raise ValueError ( \"y must be in the range 0 and 1\" ) if y . shape != torch . Size ([ n ]): raise TypeError ( \"y must have shape (N,)\" ) mat = self . distance ( x ) # remove self comparisons mask = ~ torch . eye ( n , dtype = torch . bool ) mat = mat [ mask ] . view ( n , n - 1 ) probs = get_probs ( mat , mask , y , self . distance . is_inverted ) return get_loss ( probs , self . ent_loss_fn , self . div_loss_fn , self . with_ent , self . with_div )","title":"ist_loss"},{"location":"docs/layers/ist_loss/#pytorch_adapt.layers.ist_loss.ISTLoss","text":"Bases: torch . nn . Module Implementation of the I_st loss from Information-Theoretical Learning of Discriminative Clusters for Unsupervised Domain Adaptation Source code in pytorch_adapt\\layers\\ist_loss.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class ISTLoss ( torch . nn . Module ): \"\"\" Implementation of the I_st loss from [Information-Theoretical Learning of Discriminative Clusters for Unsupervised Domain Adaptation](https://icml.cc/2012/papers/566.pdf) \"\"\" def __init__ ( self , distance = None , with_ent = True , with_div = True ): super () . __init__ () self . distance = c_f . default ( distance , CosineSimilarity , {}) if not ( with_ent or with_div ): raise ValueError ( \"At least one of with_ent or with_div must be True\" ) self . with_ent = with_ent self . with_div = with_div self . ent_loss_fn = EntropyLoss ( after_softmax = True ) self . div_loss_fn = DiversityLoss ( after_softmax = True ) def forward ( self , x , y ): \"\"\" Arguments: x: source and target features y: domain labels, i.e. 0 for source domain, 1 for target domain \"\"\" n = x . shape [ 0 ] if torch . min ( y ) < 0 or torch . max ( y ) > 1 : raise ValueError ( \"y must be in the range 0 and 1\" ) if y . shape != torch . Size ([ n ]): raise TypeError ( \"y must have shape (N,)\" ) mat = self . distance ( x ) # remove self comparisons mask = ~ torch . eye ( n , dtype = torch . bool ) mat = mat [ mask ] . view ( n , n - 1 ) probs = get_probs ( mat , mask , y , self . distance . is_inverted ) return get_loss ( probs , self . ent_loss_fn , self . div_loss_fn , self . with_ent , self . with_div ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"with_div\" ])","title":"ISTLoss"},{"location":"docs/layers/mcc_loss/","text":"MCCLoss \u00b6 Bases: torch . nn . Module Implementation of Minimum Class Confusion for Versatile Domain Adaptation . Source code in pytorch_adapt\\layers\\mcc_loss.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class MCCLoss ( torch . nn . Module ): \"\"\" Implementation of [Minimum Class Confusion for Versatile Domain Adaptation](https://arxiv.org/abs/1912.03699). \"\"\" def __init__ ( self , T : float = 1 , entropy_weighter : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: T: softmax temperature applied to the input target logits entropy_weighter: a function that returns a weight for each sample. The weights are used in the process of computing the class confusion tensor as described in the paper. If ```None```, then ```layers.EntropyWeights``` is used. \"\"\" super () . __init__ () self . T = T self . entropy_weighter = c_f . default ( entropy_weighter , EntropyWeights ( after_softmax = True , normalizer = SumNormalizer ( scale_by_batch_size = True ) ), ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: target logits \"\"\" Y = torch . nn . functional . softmax ( x / self . T , dim = 1 ) H_weights = self . entropy_weighter ( Y . detach ()) C = torch . linalg . multi_dot ([ Y . t (), torch . diag ( H_weights ), Y ]) C = C / torch . sum ( C , dim = 1 ) return ( torch . sum ( C ) - torch . trace ( C )) / C . shape [ 0 ] def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"T\" ]) __init__ ( T = 1 , entropy_weighter = None ) \u00b6 Parameters: Name Type Description Default T float softmax temperature applied to the input target logits 1 entropy_weighter Callable [[ torch . Tensor ], torch . Tensor ] a function that returns a weight for each sample. The weights are used in the process of computing the class confusion tensor as described in the paper. If None , then layers.EntropyWeights is used. None Source code in pytorch_adapt\\layers\\mcc_loss.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , T : float = 1 , entropy_weighter : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: T: softmax temperature applied to the input target logits entropy_weighter: a function that returns a weight for each sample. The weights are used in the process of computing the class confusion tensor as described in the paper. If ```None```, then ```layers.EntropyWeights``` is used. \"\"\" super () . __init__ () self . T = T self . entropy_weighter = c_f . default ( entropy_weighter , EntropyWeights ( after_softmax = True , normalizer = SumNormalizer ( scale_by_batch_size = True ) ), ) forward ( x ) \u00b6 Parameters: Name Type Description Default x torch . Tensor target logits required Source code in pytorch_adapt\\layers\\mcc_loss.py 39 40 41 42 43 44 45 46 47 48 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: target logits \"\"\" Y = torch . nn . functional . softmax ( x / self . T , dim = 1 ) H_weights = self . entropy_weighter ( Y . detach ()) C = torch . linalg . multi_dot ([ Y . t (), torch . diag ( H_weights ), Y ]) C = C / torch . sum ( C , dim = 1 ) return ( torch . sum ( C ) - torch . trace ( C )) / C . shape [ 0 ]","title":"mcc_loss"},{"location":"docs/layers/mcc_loss/#pytorch_adapt.layers.mcc_loss.MCCLoss","text":"Bases: torch . nn . Module Implementation of Minimum Class Confusion for Versatile Domain Adaptation . Source code in pytorch_adapt\\layers\\mcc_loss.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class MCCLoss ( torch . nn . Module ): \"\"\" Implementation of [Minimum Class Confusion for Versatile Domain Adaptation](https://arxiv.org/abs/1912.03699). \"\"\" def __init__ ( self , T : float = 1 , entropy_weighter : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: T: softmax temperature applied to the input target logits entropy_weighter: a function that returns a weight for each sample. The weights are used in the process of computing the class confusion tensor as described in the paper. If ```None```, then ```layers.EntropyWeights``` is used. \"\"\" super () . __init__ () self . T = T self . entropy_weighter = c_f . default ( entropy_weighter , EntropyWeights ( after_softmax = True , normalizer = SumNormalizer ( scale_by_batch_size = True ) ), ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: target logits \"\"\" Y = torch . nn . functional . softmax ( x / self . T , dim = 1 ) H_weights = self . entropy_weighter ( Y . detach ()) C = torch . linalg . multi_dot ([ Y . t (), torch . diag ( H_weights ), Y ]) C = C / torch . sum ( C , dim = 1 ) return ( torch . sum ( C ) - torch . trace ( C )) / C . shape [ 0 ] def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"T\" ])","title":"MCCLoss"},{"location":"docs/layers/mcd_loss/","text":"MCDLoss \u00b6 Bases: torch . nn . Module Implementation of the loss function used in Maximum Classifier Discrepancy for Unsupervised Domain Adaptation . Source code in pytorch_adapt\\layers\\mcd_loss.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class MCDLoss ( torch . nn . Module ): \"\"\" Implementation of the loss function used in [Maximum Classifier Discrepancy for Unsupervised Domain Adaptation](https://arxiv.org/abs/1712.02560). \"\"\" def __init__ ( self , dist_fn : Callable [[ torch . Tensor ], torch . Tensor ] = None ): \"\"\" Arguments: dist_fn: Computes the mean distance between two softmaxed tensors. If ```None```, then ```torch.nn.L1Loss``` is used. \"\"\" super () . __init__ () self . dist_fn = c_f . default ( dist_fn , torch . nn . L1Loss , {}) def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: a batch of class logits y: the other batch of class logits Returns: The discrepancy between the two batches of class logits. \"\"\" return mcd_loss ( x , y , self . dist_fn ) __init__ ( dist_fn = None ) \u00b6 Parameters: Name Type Description Default dist_fn Callable [[ torch . Tensor ], torch . Tensor ] Computes the mean distance between two softmaxed tensors. If None , then torch.nn.L1Loss is used. None Source code in pytorch_adapt\\layers\\mcd_loss.py 19 20 21 22 23 24 25 26 def __init__ ( self , dist_fn : Callable [[ torch . Tensor ], torch . Tensor ] = None ): \"\"\" Arguments: dist_fn: Computes the mean distance between two softmaxed tensors. If ```None```, then ```torch.nn.L1Loss``` is used. \"\"\" super () . __init__ () self . dist_fn = c_f . default ( dist_fn , torch . nn . L1Loss , {}) forward ( x , y ) \u00b6 Parameters: Name Type Description Default x torch . Tensor a batch of class logits required y torch . Tensor the other batch of class logits required Returns: Type Description torch . Tensor The discrepancy between the two batches of class logits. Source code in pytorch_adapt\\layers\\mcd_loss.py 28 29 30 31 32 33 34 35 36 def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: a batch of class logits y: the other batch of class logits Returns: The discrepancy between the two batches of class logits. \"\"\" return mcd_loss ( x , y , self . dist_fn )","title":"mcd_loss"},{"location":"docs/layers/mcd_loss/#pytorch_adapt.layers.mcd_loss.MCDLoss","text":"Bases: torch . nn . Module Implementation of the loss function used in Maximum Classifier Discrepancy for Unsupervised Domain Adaptation . Source code in pytorch_adapt\\layers\\mcd_loss.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class MCDLoss ( torch . nn . Module ): \"\"\" Implementation of the loss function used in [Maximum Classifier Discrepancy for Unsupervised Domain Adaptation](https://arxiv.org/abs/1712.02560). \"\"\" def __init__ ( self , dist_fn : Callable [[ torch . Tensor ], torch . Tensor ] = None ): \"\"\" Arguments: dist_fn: Computes the mean distance between two softmaxed tensors. If ```None```, then ```torch.nn.L1Loss``` is used. \"\"\" super () . __init__ () self . dist_fn = c_f . default ( dist_fn , torch . nn . L1Loss , {}) def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: a batch of class logits y: the other batch of class logits Returns: The discrepancy between the two batches of class logits. \"\"\" return mcd_loss ( x , y , self . dist_fn )","title":"MCDLoss"},{"location":"docs/layers/mmd_loss/","text":"MMDBatchedLoss \u00b6 Bases: MMDLoss Source code in pytorch_adapt\\layers\\mmd_loss.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class MMDBatchedLoss ( MMDLoss ): def __init__ ( self , batch_size = 1024 , ** kwargs ): super () . __init__ ( ** kwargs ) if self . mmd_type != \"quadratic\" : raise ValueError ( \"mmd_type must be 'quadratic'\" ) self . mmd_func = l_u . get_mmd_quadratic_batched self . dist_func = BatchedDistance ( self . dist_func , batch_size = batch_size ) def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: features from one domain. y: features from the other domain. Returns: MMD \"\"\" if c_f . is_list_or_tuple ( x ) or c_f . is_list_or_tuple ( y ): raise TypeError ( \"List of features not yet supported\" ) check_batch_sizes ( x , y , self . mmd_type ) return self . mmd_func ( x , y , self . dist_func , self . kernel_scales , self . bandwidth ) forward ( x , y ) \u00b6 Parameters: Name Type Description Default x torch . Tensor features from one domain. required y torch . Tensor features from the other domain. required Returns: Type Description torch . Tensor MMD Source code in pytorch_adapt\\layers\\mmd_loss.py 99 100 101 102 103 104 105 106 107 108 109 110 def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: features from one domain. y: features from the other domain. Returns: MMD \"\"\" if c_f . is_list_or_tuple ( x ) or c_f . is_list_or_tuple ( y ): raise TypeError ( \"List of features not yet supported\" ) check_batch_sizes ( x , y , self . mmd_type ) return self . mmd_func ( x , y , self . dist_func , self . kernel_scales , self . bandwidth ) MMDLoss \u00b6 Bases: torch . nn . Module Implementation of Learning Transferable Features with Deep Adaptation Networks Deep Transfer Learning with Joint Adaptation Networks . Source code in pytorch_adapt\\layers\\mmd_loss.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class MMDLoss ( torch . nn . Module ): \"\"\" Implementation of - [Learning Transferable Features with Deep Adaptation Networks](https://arxiv.org/abs/1502.02791) - [Deep Transfer Learning with Joint Adaptation Networks](https://arxiv.org/abs/1605.06636). \"\"\" def __init__ ( self , kernel_scales : Union [ float , torch . Tensor ] = 1 , mmd_type : str = \"linear\" , dist_func = None , bandwidth = None , ): \"\"\" Arguments: kernel_scales: The kernel bandwidth is scaled by this amount. If a tensor, then multiple kernel bandwidths are used. mmd_type: 'linear' or 'quadratic'. 'linear' uses the linear estimate of MK-MMD. \"\"\" super () . __init__ () self . kernel_scales = kernel_scales self . dist_func = c_f . default ( dist_func , LpDistance ( normalize_embeddings = False , p = 2 , power = 2 ) ) self . bandwidth = bandwidth self . mmd_type = mmd_type if mmd_type == \"linear\" : self . mmd_func = l_u . get_mmd_linear elif mmd_type == \"quadratic\" : self . mmd_func = l_u . get_mmd_quadratic else : raise ValueError ( \"mmd_type must be either linear or quadratic\" ) # input can be embeddings or list of embeddings def forward ( self , x : Union [ torch . Tensor , List [ torch . Tensor ]], y : Union [ torch . Tensor , List [ torch . Tensor ]], ) -> torch . Tensor : \"\"\" Arguments: x: features or a list of features from one domain. y: features or a list of features from the other domain. Returns: MMD if the inputs are tensors, and Joint MMD (JMMD) if the inputs are lists of tensors. \"\"\" check_batch_sizes ( x , y , self . mmd_type ) xx , yy , zz , scale = l_u . get_mmd_dist_mats ( x , y , self . dist_func , self . bandwidth ) if torch . is_tensor ( self . kernel_scales ): s = scale [ 0 ] if c_f . is_list_or_tuple ( scale ) else scale self . kernel_scales = pml_cf . to_device ( self . kernel_scales , s , dtype = s . dtype ) if c_f . is_list_or_tuple ( scale ): for i in range ( len ( scale )): scale [ i ] = scale [ i ] * self . kernel_scales else : scale = scale * self . kernel_scales return self . mmd_func ( xx , yy , zz , scale ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"mmd_type\" , \"kernel_scales\" ]) __init__ ( kernel_scales = 1 , mmd_type = 'linear' , dist_func = None , bandwidth = None ) \u00b6 Parameters: Name Type Description Default kernel_scales Union [ float , torch . Tensor ] The kernel bandwidth is scaled by this amount. If a tensor, then multiple kernel bandwidths are used. 1 mmd_type str 'linear' or 'quadratic'. 'linear' uses the linear estimate of MK-MMD. 'linear' Source code in pytorch_adapt\\layers\\mmd_loss.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , kernel_scales : Union [ float , torch . Tensor ] = 1 , mmd_type : str = \"linear\" , dist_func = None , bandwidth = None , ): \"\"\" Arguments: kernel_scales: The kernel bandwidth is scaled by this amount. If a tensor, then multiple kernel bandwidths are used. mmd_type: 'linear' or 'quadratic'. 'linear' uses the linear estimate of MK-MMD. \"\"\" super () . __init__ () self . kernel_scales = kernel_scales self . dist_func = c_f . default ( dist_func , LpDistance ( normalize_embeddings = False , p = 2 , power = 2 ) ) self . bandwidth = bandwidth self . mmd_type = mmd_type if mmd_type == \"linear\" : self . mmd_func = l_u . get_mmd_linear elif mmd_type == \"quadratic\" : self . mmd_func = l_u . get_mmd_quadratic else : raise ValueError ( \"mmd_type must be either linear or quadratic\" ) forward ( x , y ) \u00b6 Parameters: Name Type Description Default x Union [ torch . Tensor , List [ torch . Tensor ]] features or a list of features from one domain. required y Union [ torch . Tensor , List [ torch . Tensor ]] features or a list of features from the other domain. required Returns: Type Description torch . Tensor MMD if the inputs are tensors, and Joint MMD (JMMD) if the inputs are lists of tensors. Source code in pytorch_adapt\\layers\\mmd_loss.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def forward ( self , x : Union [ torch . Tensor , List [ torch . Tensor ]], y : Union [ torch . Tensor , List [ torch . Tensor ]], ) -> torch . Tensor : \"\"\" Arguments: x: features or a list of features from one domain. y: features or a list of features from the other domain. Returns: MMD if the inputs are tensors, and Joint MMD (JMMD) if the inputs are lists of tensors. \"\"\" check_batch_sizes ( x , y , self . mmd_type ) xx , yy , zz , scale = l_u . get_mmd_dist_mats ( x , y , self . dist_func , self . bandwidth ) if torch . is_tensor ( self . kernel_scales ): s = scale [ 0 ] if c_f . is_list_or_tuple ( scale ) else scale self . kernel_scales = pml_cf . to_device ( self . kernel_scales , s , dtype = s . dtype ) if c_f . is_list_or_tuple ( scale ): for i in range ( len ( scale )): scale [ i ] = scale [ i ] * self . kernel_scales else : scale = scale * self . kernel_scales return self . mmd_func ( xx , yy , zz , scale )","title":"mmd_loss"},{"location":"docs/layers/mmd_loss/#pytorch_adapt.layers.mmd_loss.MMDBatchedLoss","text":"Bases: MMDLoss Source code in pytorch_adapt\\layers\\mmd_loss.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class MMDBatchedLoss ( MMDLoss ): def __init__ ( self , batch_size = 1024 , ** kwargs ): super () . __init__ ( ** kwargs ) if self . mmd_type != \"quadratic\" : raise ValueError ( \"mmd_type must be 'quadratic'\" ) self . mmd_func = l_u . get_mmd_quadratic_batched self . dist_func = BatchedDistance ( self . dist_func , batch_size = batch_size ) def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: features from one domain. y: features from the other domain. Returns: MMD \"\"\" if c_f . is_list_or_tuple ( x ) or c_f . is_list_or_tuple ( y ): raise TypeError ( \"List of features not yet supported\" ) check_batch_sizes ( x , y , self . mmd_type ) return self . mmd_func ( x , y , self . dist_func , self . kernel_scales , self . bandwidth )","title":"MMDBatchedLoss"},{"location":"docs/layers/mmd_loss/#pytorch_adapt.layers.mmd_loss.MMDLoss","text":"Bases: torch . nn . Module Implementation of Learning Transferable Features with Deep Adaptation Networks Deep Transfer Learning with Joint Adaptation Networks . Source code in pytorch_adapt\\layers\\mmd_loss.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class MMDLoss ( torch . nn . Module ): \"\"\" Implementation of - [Learning Transferable Features with Deep Adaptation Networks](https://arxiv.org/abs/1502.02791) - [Deep Transfer Learning with Joint Adaptation Networks](https://arxiv.org/abs/1605.06636). \"\"\" def __init__ ( self , kernel_scales : Union [ float , torch . Tensor ] = 1 , mmd_type : str = \"linear\" , dist_func = None , bandwidth = None , ): \"\"\" Arguments: kernel_scales: The kernel bandwidth is scaled by this amount. If a tensor, then multiple kernel bandwidths are used. mmd_type: 'linear' or 'quadratic'. 'linear' uses the linear estimate of MK-MMD. \"\"\" super () . __init__ () self . kernel_scales = kernel_scales self . dist_func = c_f . default ( dist_func , LpDistance ( normalize_embeddings = False , p = 2 , power = 2 ) ) self . bandwidth = bandwidth self . mmd_type = mmd_type if mmd_type == \"linear\" : self . mmd_func = l_u . get_mmd_linear elif mmd_type == \"quadratic\" : self . mmd_func = l_u . get_mmd_quadratic else : raise ValueError ( \"mmd_type must be either linear or quadratic\" ) # input can be embeddings or list of embeddings def forward ( self , x : Union [ torch . Tensor , List [ torch . Tensor ]], y : Union [ torch . Tensor , List [ torch . Tensor ]], ) -> torch . Tensor : \"\"\" Arguments: x: features or a list of features from one domain. y: features or a list of features from the other domain. Returns: MMD if the inputs are tensors, and Joint MMD (JMMD) if the inputs are lists of tensors. \"\"\" check_batch_sizes ( x , y , self . mmd_type ) xx , yy , zz , scale = l_u . get_mmd_dist_mats ( x , y , self . dist_func , self . bandwidth ) if torch . is_tensor ( self . kernel_scales ): s = scale [ 0 ] if c_f . is_list_or_tuple ( scale ) else scale self . kernel_scales = pml_cf . to_device ( self . kernel_scales , s , dtype = s . dtype ) if c_f . is_list_or_tuple ( scale ): for i in range ( len ( scale )): scale [ i ] = scale [ i ] * self . kernel_scales else : scale = scale * self . kernel_scales return self . mmd_func ( xx , yy , zz , scale ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"mmd_type\" , \"kernel_scales\" ])","title":"MMDLoss"},{"location":"docs/layers/model_with_bridge/","text":"ModelWithBridge \u00b6 Bases: torch . nn . Module Implementation of the bridge architecture described in Gradually Vanishing Bridge for Adversarial Domain Adaptation . Source code in pytorch_adapt\\layers\\model_with_bridge.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class ModelWithBridge ( torch . nn . Module ): \"\"\" Implementation of the bridge architecture described in [Gradually Vanishing Bridge for Adversarial Domain Adaptation](https://arxiv.org/abs/2003.13183). \"\"\" def __init__ ( self , model : torch . nn . Module , bridge : torch . nn . Module = None ): \"\"\" Arguments: model: Any pytorch model. bridge: A model which has the same input/output sizes as ```model```. If ```None```, then the bridge is formed by copying ```model```, and randomly reinitialization all its parameters. \"\"\" super () . __init__ () self . model = model if bridge is None : bridge = c_f . reinit ( copy . deepcopy ( model )) self . bridge = bridge def forward ( self , x : torch . Tensor , return_bridge : bool = False ) -> Union [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]]: \"\"\" Arguments: x: The input to both ```self.model``` and ```self.bridge```. return_bridge: Whether or not to return the bridge output in addition to the ```model - bridge``` output Returns: If ```return_bridge = False```, then return just ```model - bridge```. If ```return_bridge = True```, then return a tuple of ```(model - bridge), bridge``` \"\"\" y = self . model ( x ) z = self . bridge ( x ) output = y - z if return_bridge : return output , z return output __init__ ( model , bridge = None ) \u00b6 Parameters: Name Type Description Default model torch . nn . Module Any pytorch model. required bridge torch . nn . Module A model which has the same input/output sizes as model . If None , then the bridge is formed by copying model , and randomly reinitialization all its parameters. None Source code in pytorch_adapt\\layers\\model_with_bridge.py 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , model : torch . nn . Module , bridge : torch . nn . Module = None ): \"\"\" Arguments: model: Any pytorch model. bridge: A model which has the same input/output sizes as ```model```. If ```None```, then the bridge is formed by copying ```model```, and randomly reinitialization all its parameters. \"\"\" super () . __init__ () self . model = model if bridge is None : bridge = c_f . reinit ( copy . deepcopy ( model )) self . bridge = bridge forward ( x , return_bridge = False ) \u00b6 Parameters: Name Type Description Default x torch . Tensor The input to both self.model and self.bridge . required return_bridge bool Whether or not to return the bridge output in addition to the model - bridge output False Returns: Type Description Union [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]] If return_bridge = False , then return just model - bridge . Union [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]] If return_bridge = True , then return a tuple of (model - bridge), bridge Source code in pytorch_adapt\\layers\\model_with_bridge.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def forward ( self , x : torch . Tensor , return_bridge : bool = False ) -> Union [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]]: \"\"\" Arguments: x: The input to both ```self.model``` and ```self.bridge```. return_bridge: Whether or not to return the bridge output in addition to the ```model - bridge``` output Returns: If ```return_bridge = False```, then return just ```model - bridge```. If ```return_bridge = True```, then return a tuple of ```(model - bridge), bridge``` \"\"\" y = self . model ( x ) z = self . bridge ( x ) output = y - z if return_bridge : return output , z return output","title":"model_with_bridge"},{"location":"docs/layers/model_with_bridge/#pytorch_adapt.layers.model_with_bridge.ModelWithBridge","text":"Bases: torch . nn . Module Implementation of the bridge architecture described in Gradually Vanishing Bridge for Adversarial Domain Adaptation . Source code in pytorch_adapt\\layers\\model_with_bridge.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class ModelWithBridge ( torch . nn . Module ): \"\"\" Implementation of the bridge architecture described in [Gradually Vanishing Bridge for Adversarial Domain Adaptation](https://arxiv.org/abs/2003.13183). \"\"\" def __init__ ( self , model : torch . nn . Module , bridge : torch . nn . Module = None ): \"\"\" Arguments: model: Any pytorch model. bridge: A model which has the same input/output sizes as ```model```. If ```None```, then the bridge is formed by copying ```model```, and randomly reinitialization all its parameters. \"\"\" super () . __init__ () self . model = model if bridge is None : bridge = c_f . reinit ( copy . deepcopy ( model )) self . bridge = bridge def forward ( self , x : torch . Tensor , return_bridge : bool = False ) -> Union [ torch . Tensor , Tuple [ torch . Tensor , torch . Tensor ]]: \"\"\" Arguments: x: The input to both ```self.model``` and ```self.bridge```. return_bridge: Whether or not to return the bridge output in addition to the ```model - bridge``` output Returns: If ```return_bridge = False```, then return just ```model - bridge```. If ```return_bridge = True```, then return a tuple of ```(model - bridge), bridge``` \"\"\" y = self . model ( x ) z = self . bridge ( x ) output = y - z if return_bridge : return output , z return output","title":"ModelWithBridge"},{"location":"docs/layers/multiple_models/","text":"MultipleModels \u00b6 Bases: torch . nn . Module Wraps a list of models, and returns their outputs as a list of tensors Source code in pytorch_adapt\\layers\\multiple_models.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class MultipleModels ( torch . nn . Module ): \"\"\" Wraps a list of models, and returns their outputs as a list of tensors \"\"\" def __init__ ( self , * models : torch . nn . Module ): \"\"\" Arguments: *models: The models to be wrapped. \"\"\" super () . __init__ () self . models = torch . nn . ModuleList ( models ) def forward ( self , x : torch . Tensor ) -> List [ Any ]: \"\"\" Arguments: x: the input to each model Returns: A list containing the output of each model. \"\"\" outputs = [] for m in self . models : outputs . append ( m ( x )) return outputs __init__ ( * models ) \u00b6 Parameters: Name Type Description Default *models torch . nn . Module The models to be wrapped. () Source code in pytorch_adapt\\layers\\multiple_models.py 11 12 13 14 15 16 17 def __init__ ( self , * models : torch . nn . Module ): \"\"\" Arguments: *models: The models to be wrapped. \"\"\" super () . __init__ () self . models = torch . nn . ModuleList ( models ) forward ( x ) \u00b6 Parameters: Name Type Description Default x torch . Tensor the input to each model required Returns: Type Description List [ Any ] A list containing the output of each model. Source code in pytorch_adapt\\layers\\multiple_models.py 19 20 21 22 23 24 25 26 27 28 29 def forward ( self , x : torch . Tensor ) -> List [ Any ]: \"\"\" Arguments: x: the input to each model Returns: A list containing the output of each model. \"\"\" outputs = [] for m in self . models : outputs . append ( m ( x )) return outputs","title":"multiple_models"},{"location":"docs/layers/multiple_models/#pytorch_adapt.layers.multiple_models.MultipleModels","text":"Bases: torch . nn . Module Wraps a list of models, and returns their outputs as a list of tensors Source code in pytorch_adapt\\layers\\multiple_models.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class MultipleModels ( torch . nn . Module ): \"\"\" Wraps a list of models, and returns their outputs as a list of tensors \"\"\" def __init__ ( self , * models : torch . nn . Module ): \"\"\" Arguments: *models: The models to be wrapped. \"\"\" super () . __init__ () self . models = torch . nn . ModuleList ( models ) def forward ( self , x : torch . Tensor ) -> List [ Any ]: \"\"\" Arguments: x: the input to each model Returns: A list containing the output of each model. \"\"\" outputs = [] for m in self . models : outputs . append ( m ( x )) return outputs","title":"MultipleModels"},{"location":"docs/layers/neighborhood_aggregation/","text":"NeighborhoodAggregation \u00b6 Bases: torch . nn . Module Implementation of the pseudo labeling step in Domain Adaptation with Auxiliary Target Domain-Oriented Classifier . Source code in pytorch_adapt\\layers\\neighborhood_aggregation.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class NeighborhoodAggregation ( torch . nn . Module ): \"\"\" Implementation of the pseudo labeling step in [Domain Adaptation with Auxiliary Target Domain-Oriented Classifier](https://arxiv.org/abs/2007.04171). \"\"\" def __init__ ( self , dataset_size : int , feature_dim : int , num_classes : int , k : int = 5 , T : float = 0.5 , ): \"\"\" Arguments: dataset_size: The number of samples in the target dataset. feature_dim: The feature dimensionality, i.e at each iteration the features should be size ```(N, D)``` where N is batch size and D is ```feature_dim```. num_classes: The number of class labels in the target dataset. k: The number of nearest neighbors used to determine each sample's pseudolabel T: The softmax temperature used when storing predictions in memory. \"\"\" super () . __init__ () self . register_buffer ( \"feat_memory\" , F . normalize ( torch . rand ( dataset_size , feature_dim )) ) self . register_buffer ( \"pred_memory\" , torch . ones ( dataset_size , num_classes ) / num_classes ) self . k = k self . T = T def forward ( self , features : torch . Tensor , logits : torch . Tensor = None , update : bool = False , idx : torch . Tensor = None , ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Arguments: features: The features to compute pseudolabels for. logits: The logits from which predictions will be computed and stored in memory. Required if ```update = True``` update: If True, the current batch of predictions is added to the memory bank. idx: A tensor containing the dataset indices that produced each row of ```features```. \"\"\" # move to device if necessary self . feat_memory = pml_cf . to_device ( self . feat_memory , features ) self . pred_memory = pml_cf . to_device ( self . pred_memory , features ) with torch . no_grad (): features = F . normalize ( features ) pseudo_labels , mean_preds = self . get_pseudo_labels ( features , idx ) if update : self . update_memory ( features , logits , idx ) return pseudo_labels , mean_preds def get_pseudo_labels ( self , normalized_features , idx ): dis = torch . mm ( normalized_features , self . feat_memory . t ()) # set self-comparisons to min similarity for di in range ( dis . size ( 0 )): dis [ di , idx [ di ]] = torch . min ( dis ) _ , indices = torch . topk ( dis , k = self . k , dim = 1 ) preds = torch . mean ( self . pred_memory [ indices ], dim = 1 ) pseudo_labels = torch . argmax ( preds , dim = 1 ) return pseudo_labels , preds def update_memory ( self , normalized_features , logits , idx ): preds = F . softmax ( logits , dim = 1 ) p = 1.0 / self . T preds = ( preds ** p ) / torch . sum ( preds ** p , dim = 0 ) self . feat_memory [ idx ] = normalized_features self . pred_memory [ idx ] = preds def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"k\" , \"T\" ]) __init__ ( dataset_size , feature_dim , num_classes , k = 5 , T = 0.5 ) \u00b6 Parameters: Name Type Description Default dataset_size int The number of samples in the target dataset. required feature_dim int The feature dimensionality, i.e at each iteration the features should be size (N, D) where N is batch size and D is feature_dim . required num_classes int The number of class labels in the target dataset. required k int The number of nearest neighbors used to determine each sample's pseudolabel 5 T float The softmax temperature used when storing predictions in memory. 0.5 Source code in pytorch_adapt\\layers\\neighborhood_aggregation.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , dataset_size : int , feature_dim : int , num_classes : int , k : int = 5 , T : float = 0.5 , ): \"\"\" Arguments: dataset_size: The number of samples in the target dataset. feature_dim: The feature dimensionality, i.e at each iteration the features should be size ```(N, D)``` where N is batch size and D is ```feature_dim```. num_classes: The number of class labels in the target dataset. k: The number of nearest neighbors used to determine each sample's pseudolabel T: The softmax temperature used when storing predictions in memory. \"\"\" super () . __init__ () self . register_buffer ( \"feat_memory\" , F . normalize ( torch . rand ( dataset_size , feature_dim )) ) self . register_buffer ( \"pred_memory\" , torch . ones ( dataset_size , num_classes ) / num_classes ) self . k = k self . T = T forward ( features , logits = None , update = False , idx = None ) \u00b6 Parameters: Name Type Description Default features torch . Tensor The features to compute pseudolabels for. required logits torch . Tensor The logits from which predictions will be computed and stored in memory. Required if update = True None update bool If True, the current batch of predictions is added to the memory bank. False idx torch . Tensor A tensor containing the dataset indices that produced each row of features . None Source code in pytorch_adapt\\layers\\neighborhood_aggregation.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def forward ( self , features : torch . Tensor , logits : torch . Tensor = None , update : bool = False , idx : torch . Tensor = None , ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Arguments: features: The features to compute pseudolabels for. logits: The logits from which predictions will be computed and stored in memory. Required if ```update = True``` update: If True, the current batch of predictions is added to the memory bank. idx: A tensor containing the dataset indices that produced each row of ```features```. \"\"\" # move to device if necessary self . feat_memory = pml_cf . to_device ( self . feat_memory , features ) self . pred_memory = pml_cf . to_device ( self . pred_memory , features ) with torch . no_grad (): features = F . normalize ( features ) pseudo_labels , mean_preds = self . get_pseudo_labels ( features , idx ) if update : self . update_memory ( features , logits , idx ) return pseudo_labels , mean_preds","title":"neighborhood_aggregation"},{"location":"docs/layers/neighborhood_aggregation/#pytorch_adapt.layers.neighborhood_aggregation.NeighborhoodAggregation","text":"Bases: torch . nn . Module Implementation of the pseudo labeling step in Domain Adaptation with Auxiliary Target Domain-Oriented Classifier . Source code in pytorch_adapt\\layers\\neighborhood_aggregation.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class NeighborhoodAggregation ( torch . nn . Module ): \"\"\" Implementation of the pseudo labeling step in [Domain Adaptation with Auxiliary Target Domain-Oriented Classifier](https://arxiv.org/abs/2007.04171). \"\"\" def __init__ ( self , dataset_size : int , feature_dim : int , num_classes : int , k : int = 5 , T : float = 0.5 , ): \"\"\" Arguments: dataset_size: The number of samples in the target dataset. feature_dim: The feature dimensionality, i.e at each iteration the features should be size ```(N, D)``` where N is batch size and D is ```feature_dim```. num_classes: The number of class labels in the target dataset. k: The number of nearest neighbors used to determine each sample's pseudolabel T: The softmax temperature used when storing predictions in memory. \"\"\" super () . __init__ () self . register_buffer ( \"feat_memory\" , F . normalize ( torch . rand ( dataset_size , feature_dim )) ) self . register_buffer ( \"pred_memory\" , torch . ones ( dataset_size , num_classes ) / num_classes ) self . k = k self . T = T def forward ( self , features : torch . Tensor , logits : torch . Tensor = None , update : bool = False , idx : torch . Tensor = None , ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Arguments: features: The features to compute pseudolabels for. logits: The logits from which predictions will be computed and stored in memory. Required if ```update = True``` update: If True, the current batch of predictions is added to the memory bank. idx: A tensor containing the dataset indices that produced each row of ```features```. \"\"\" # move to device if necessary self . feat_memory = pml_cf . to_device ( self . feat_memory , features ) self . pred_memory = pml_cf . to_device ( self . pred_memory , features ) with torch . no_grad (): features = F . normalize ( features ) pseudo_labels , mean_preds = self . get_pseudo_labels ( features , idx ) if update : self . update_memory ( features , logits , idx ) return pseudo_labels , mean_preds def get_pseudo_labels ( self , normalized_features , idx ): dis = torch . mm ( normalized_features , self . feat_memory . t ()) # set self-comparisons to min similarity for di in range ( dis . size ( 0 )): dis [ di , idx [ di ]] = torch . min ( dis ) _ , indices = torch . topk ( dis , k = self . k , dim = 1 ) preds = torch . mean ( self . pred_memory [ indices ], dim = 1 ) pseudo_labels = torch . argmax ( preds , dim = 1 ) return pseudo_labels , preds def update_memory ( self , normalized_features , logits , idx ): preds = F . softmax ( logits , dim = 1 ) p = 1.0 / self . T preds = ( preds ** p ) / torch . sum ( preds ** p , dim = 0 ) self . feat_memory [ idx ] = normalized_features self . pred_memory [ idx ] = preds def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"k\" , \"T\" ])","title":"NeighborhoodAggregation"},{"location":"docs/layers/nll_loss/","text":"NLLLoss \u00b6 Bases: torch . nn . Module Same as torch.nn.NLLLoss but takes in softmax as input Source code in pytorch_adapt\\layers\\nll_loss.py 5 6 7 8 9 10 11 12 13 14 15 16 17 class NLLLoss ( torch . nn . Module ): \"\"\" Same as torch.nn.NLLLoss but takes in softmax as input \"\"\" def __init__ ( self , reduction = \"mean\" ): super () . __init__ () self . reduction = reduction def forward ( self , x : torch . Tensor , y : torch . Tensor ): \"\"\" \"\"\" x = torch . log ( x + pml_cf . small_val ( x . dtype )) return torch . nn . functional . nll_loss ( x , y , reduction = self . reduction )","title":"nll_loss"},{"location":"docs/layers/nll_loss/#pytorch_adapt.layers.nll_loss.NLLLoss","text":"Bases: torch . nn . Module Same as torch.nn.NLLLoss but takes in softmax as input Source code in pytorch_adapt\\layers\\nll_loss.py 5 6 7 8 9 10 11 12 13 14 15 16 17 class NLLLoss ( torch . nn . Module ): \"\"\" Same as torch.nn.NLLLoss but takes in softmax as input \"\"\" def __init__ ( self , reduction = \"mean\" ): super () . __init__ () self . reduction = reduction def forward ( self , x : torch . Tensor , y : torch . Tensor ): \"\"\" \"\"\" x = torch . log ( x + pml_cf . small_val ( x . dtype )) return torch . nn . functional . nll_loss ( x , y , reduction = self . reduction )","title":"NLLLoss"},{"location":"docs/layers/plus_residual/","text":"PlusResidual \u00b6 Bases: torch . nn . Module Wraps a layer such that the forward pass returns x + self.layer(x) Source code in pytorch_adapt\\layers\\plus_residual.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class PlusResidual ( torch . nn . Module ): \"\"\" Wraps a layer such that the forward pass returns ```x + self.layer(x)``` \"\"\" def __init__ ( self , layer : torch . nn . Module ): \"\"\" Arguments: layer: The layer to be wrapped. \"\"\" super () . __init__ () self . layer = layer def forward ( self , x ): \"\"\"\"\"\" return x + self . layer ( x ) __init__ ( layer ) \u00b6 Parameters: Name Type Description Default layer torch . nn . Module The layer to be wrapped. required Source code in pytorch_adapt\\layers\\plus_residual.py 10 11 12 13 14 15 16 def __init__ ( self , layer : torch . nn . Module ): \"\"\" Arguments: layer: The layer to be wrapped. \"\"\" super () . __init__ () self . layer = layer","title":"plus_residual"},{"location":"docs/layers/plus_residual/#pytorch_adapt.layers.plus_residual.PlusResidual","text":"Bases: torch . nn . Module Wraps a layer such that the forward pass returns x + self.layer(x) Source code in pytorch_adapt\\layers\\plus_residual.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class PlusResidual ( torch . nn . Module ): \"\"\" Wraps a layer such that the forward pass returns ```x + self.layer(x)``` \"\"\" def __init__ ( self , layer : torch . nn . Module ): \"\"\" Arguments: layer: The layer to be wrapped. \"\"\" super () . __init__ () self . layer = layer def forward ( self , x ): \"\"\"\"\"\" return x + self . layer ( x )","title":"PlusResidual"},{"location":"docs/layers/randomized_dot_product/","text":"RandomizedDotProduct \u00b6 Bases: torch . nn . Module Implementation of randomized multilinear conditioning from Conditional Adversarial Domain Adaptation . Source code in pytorch_adapt\\layers\\randomized_dot_product.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class RandomizedDotProduct ( torch . nn . Module ): \"\"\" Implementation of randomized multilinear conditioning from [Conditional Adversarial Domain Adaptation](https://arxiv.org/abs/1705.10667). \"\"\" def __init__ ( self , in_dims : List [ int ], out_dim : int = 1024 ): \"\"\" Arguments: in_dims: A list of the feature dims. For example, if the input features have shapes ```(32, 512)``` and ```(32, 64)```, then ```in_dims = [512, 64]```. out_dim: The output feature dim. \"\"\" super () . __init__ () self . in_dims = in_dims for i , d in enumerate ( in_dims ): self . register_buffer ( self . rand_mat_name ( i ), torch . randn ( d , out_dim )) self . out_dim = out_dim self . num_mats = len ( in_dims ) self . divisor = math . pow ( float ( self . out_dim ), 1.0 / self . num_mats ) def forward ( self , * inputs : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: *inputs: The number of inputs must be equal to the length of ```self.in_dims```. \"\"\" for i in range ( self . num_mats ): # move to device if necessary curr = inputs [ i ] self . set_rand_mat ( i , pml_cf . to_device ( self . get_rand_mat ( i ), curr , dtype = curr . dtype ) ) return_list = [ torch . mm ( inputs [ i ], self . get_rand_mat ( i )) for i in range ( self . num_mats ) ] return_tensor = return_list [ 0 ] / self . divisor for single in return_list [ 1 :]: return_tensor = return_tensor * single return return_tensor def set_rand_mat ( self , i , value ): setattr ( self , self . rand_mat_name ( i ), value ) def get_rand_mat ( self , i ): return getattr ( self , self . rand_mat_name ( i )) def rand_mat_name ( self , i ): return f \"rand_mat { i } \" def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"in_dims\" , \"out_dim\" , \"divisor\" ]) __init__ ( in_dims , out_dim = 1024 ) \u00b6 Parameters: Name Type Description Default in_dims List [ int ] A list of the feature dims. For example, if the input features have shapes (32, 512) and (32, 64) , then in_dims = [512, 64] . required out_dim int The output feature dim. 1024 Source code in pytorch_adapt\\layers\\randomized_dot_product.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , in_dims : List [ int ], out_dim : int = 1024 ): \"\"\" Arguments: in_dims: A list of the feature dims. For example, if the input features have shapes ```(32, 512)``` and ```(32, 64)```, then ```in_dims = [512, 64]```. out_dim: The output feature dim. \"\"\" super () . __init__ () self . in_dims = in_dims for i , d in enumerate ( in_dims ): self . register_buffer ( self . rand_mat_name ( i ), torch . randn ( d , out_dim )) self . out_dim = out_dim self . num_mats = len ( in_dims ) self . divisor = math . pow ( float ( self . out_dim ), 1.0 / self . num_mats ) forward ( * inputs ) \u00b6 Parameters: Name Type Description Default *inputs torch . Tensor The number of inputs must be equal to the length of self.in_dims . () Source code in pytorch_adapt\\layers\\randomized_dot_product.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def forward ( self , * inputs : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: *inputs: The number of inputs must be equal to the length of ```self.in_dims```. \"\"\" for i in range ( self . num_mats ): # move to device if necessary curr = inputs [ i ] self . set_rand_mat ( i , pml_cf . to_device ( self . get_rand_mat ( i ), curr , dtype = curr . dtype ) ) return_list = [ torch . mm ( inputs [ i ], self . get_rand_mat ( i )) for i in range ( self . num_mats ) ] return_tensor = return_list [ 0 ] / self . divisor for single in return_list [ 1 :]: return_tensor = return_tensor * single return return_tensor","title":"randomized_dot_product"},{"location":"docs/layers/randomized_dot_product/#pytorch_adapt.layers.randomized_dot_product.RandomizedDotProduct","text":"Bases: torch . nn . Module Implementation of randomized multilinear conditioning from Conditional Adversarial Domain Adaptation . Source code in pytorch_adapt\\layers\\randomized_dot_product.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class RandomizedDotProduct ( torch . nn . Module ): \"\"\" Implementation of randomized multilinear conditioning from [Conditional Adversarial Domain Adaptation](https://arxiv.org/abs/1705.10667). \"\"\" def __init__ ( self , in_dims : List [ int ], out_dim : int = 1024 ): \"\"\" Arguments: in_dims: A list of the feature dims. For example, if the input features have shapes ```(32, 512)``` and ```(32, 64)```, then ```in_dims = [512, 64]```. out_dim: The output feature dim. \"\"\" super () . __init__ () self . in_dims = in_dims for i , d in enumerate ( in_dims ): self . register_buffer ( self . rand_mat_name ( i ), torch . randn ( d , out_dim )) self . out_dim = out_dim self . num_mats = len ( in_dims ) self . divisor = math . pow ( float ( self . out_dim ), 1.0 / self . num_mats ) def forward ( self , * inputs : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: *inputs: The number of inputs must be equal to the length of ```self.in_dims```. \"\"\" for i in range ( self . num_mats ): # move to device if necessary curr = inputs [ i ] self . set_rand_mat ( i , pml_cf . to_device ( self . get_rand_mat ( i ), curr , dtype = curr . dtype ) ) return_list = [ torch . mm ( inputs [ i ], self . get_rand_mat ( i )) for i in range ( self . num_mats ) ] return_tensor = return_list [ 0 ] / self . divisor for single in return_list [ 1 :]: return_tensor = return_tensor * single return return_tensor def set_rand_mat ( self , i , value ): setattr ( self , self . rand_mat_name ( i ), value ) def get_rand_mat ( self , i ): return getattr ( self , self . rand_mat_name ( i )) def rand_mat_name ( self , i ): return f \"rand_mat { i } \" def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"in_dims\" , \"out_dim\" , \"divisor\" ])","title":"RandomizedDotProduct"},{"location":"docs/layers/silhouette_score/","text":"SilhouetteScore \u00b6 Bases: torch . nn . Module A PyTorch implementation of the silhouette score Source code in pytorch_adapt\\layers\\silhouette_score.py 46 47 48 49 50 51 52 53 54 class SilhouetteScore ( torch . nn . Module ): \"\"\" A PyTorch implementation of the [silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) \"\"\" def forward ( self , features : torch . Tensor , labels : torch . Tensor ) -> float : \"\"\" \"\"\" return get_silhouette_score ( features , labels )","title":"silhouette_score"},{"location":"docs/layers/silhouette_score/#pytorch_adapt.layers.silhouette_score.SilhouetteScore","text":"Bases: torch . nn . Module A PyTorch implementation of the silhouette score Source code in pytorch_adapt\\layers\\silhouette_score.py 46 47 48 49 50 51 52 53 54 class SilhouetteScore ( torch . nn . Module ): \"\"\" A PyTorch implementation of the [silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) \"\"\" def forward ( self , features : torch . Tensor , labels : torch . Tensor ) -> float : \"\"\" \"\"\" return get_silhouette_score ( features , labels )","title":"SilhouetteScore"},{"location":"docs/layers/sliced_wasserstein/","text":"SlicedWasserstein \u00b6 Bases: torch . nn . Module Implementation of the loss used in Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation Source code in pytorch_adapt\\layers\\sliced_wasserstein.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class SlicedWasserstein ( torch . nn . Module ): \"\"\" Implementation of the loss used in [Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation](https://arxiv.org/abs/1903.04064) \"\"\" def __init__ ( self , m : int = 128 ): \"\"\" Arguments: m: The dimensionality to project to. \"\"\" super () . __init__ () self . m = 128 def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: a batch of class predictions y: the other batch of class predictions Returns: The discrepancy between the two batches of class predictions. \"\"\" d = x . shape [ 1 ] proj = torch . randn ( d , self . m , device = x . device ) proj = torch . nn . functional . normalize ( proj , dim = 0 ) x = torch . matmul ( x , proj ) y = torch . matmul ( y , proj ) x , _ = torch . sort ( x , dim = 0 ) y , _ = torch . sort ( y , dim = 0 ) return torch . mean (( x - y ) ** 2 ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"m\" ]) __init__ ( m = 128 ) \u00b6 Parameters: Name Type Description Default m int The dimensionality to project to. 128 Source code in pytorch_adapt\\layers\\sliced_wasserstein.py 12 13 14 15 16 17 18 def __init__ ( self , m : int = 128 ): \"\"\" Arguments: m: The dimensionality to project to. \"\"\" super () . __init__ () self . m = 128 forward ( x , y ) \u00b6 Parameters: Name Type Description Default x torch . Tensor a batch of class predictions required y torch . Tensor the other batch of class predictions required Returns: Type Description torch . Tensor The discrepancy between the two batches of class predictions. Source code in pytorch_adapt\\layers\\sliced_wasserstein.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: a batch of class predictions y: the other batch of class predictions Returns: The discrepancy between the two batches of class predictions. \"\"\" d = x . shape [ 1 ] proj = torch . randn ( d , self . m , device = x . device ) proj = torch . nn . functional . normalize ( proj , dim = 0 ) x = torch . matmul ( x , proj ) y = torch . matmul ( y , proj ) x , _ = torch . sort ( x , dim = 0 ) y , _ = torch . sort ( y , dim = 0 ) return torch . mean (( x - y ) ** 2 )","title":"sliced_wasserstein"},{"location":"docs/layers/sliced_wasserstein/#pytorch_adapt.layers.sliced_wasserstein.SlicedWasserstein","text":"Bases: torch . nn . Module Implementation of the loss used in Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation Source code in pytorch_adapt\\layers\\sliced_wasserstein.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class SlicedWasserstein ( torch . nn . Module ): \"\"\" Implementation of the loss used in [Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation](https://arxiv.org/abs/1903.04064) \"\"\" def __init__ ( self , m : int = 128 ): \"\"\" Arguments: m: The dimensionality to project to. \"\"\" super () . __init__ () self . m = 128 def forward ( self , x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\" Arguments: x: a batch of class predictions y: the other batch of class predictions Returns: The discrepancy between the two batches of class predictions. \"\"\" d = x . shape [ 1 ] proj = torch . randn ( d , self . m , device = x . device ) proj = torch . nn . functional . normalize ( proj , dim = 0 ) x = torch . matmul ( x , proj ) y = torch . matmul ( y , proj ) x , _ = torch . sort ( x , dim = 0 ) y , _ = torch . sort ( y , dim = 0 ) return torch . mean (( x - y ) ** 2 ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"m\" ])","title":"SlicedWasserstein"},{"location":"docs/layers/stochastic_linear/","text":"StochasticLinear \u00b6 Bases: nn . Module Implementation of the stochastic layer from Stochastic Classifiers for Unsupervised Domain Adaptation . In train() mode, it uses random weights and biases that are sampled from a learned normal distribution. In eval() mode, the learned mean is used. Source code in pytorch_adapt\\layers\\stochastic_linear.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class StochasticLinear ( nn . Module ): \"\"\" Implementation of the stochastic layer from [Stochastic Classifiers for Unsupervised Domain Adaptation](https://xiatian-zhu.github.io/papers/LuEtAl_CVPR2020.pdf). In ```train()``` mode, it uses random weights and biases that are sampled from a learned normal distribution. In ```eval()``` mode, the learned mean is used. \"\"\" def __init__ ( self , in_features : int , out_features : int , device = None , dtype = None ): \"\"\" Arguments: in_features: size of each input sample out_features: size of each output sample \"\"\" factory_kwargs = { \"device\" : device , \"dtype\" : dtype } super () . __init__ () self . in_features = in_features self . out_features = out_features self . weight_mean = nn . Parameter ( torch . empty ( in_features , out_features , ** factory_kwargs ) ) self . weight_sigma = nn . Parameter ( torch . empty ( in_features , out_features , ** factory_kwargs ) ) self . bias_mean = nn . Parameter ( torch . empty ( out_features , ** factory_kwargs )) self . bias_sigma = nn . Parameter ( torch . empty ( out_features , ** factory_kwargs )) self . reset_parameters () def reset_parameters ( self ): reset_parameters_helper ( self . weight_mean , self . bias_mean ) reset_parameters_helper ( self . weight_sigma , self . bias_sigma ) def random_sample ( self , mean , sigma ): eps = torch . randn ( * sigma . shape , device = sigma . device , dtype = sigma . dtype ) return mean + ( sigma * eps ) def forward ( self , x ): \"\"\"\"\"\" if self . training : weight = self . random_sample ( self . weight_mean , self . weight_sigma ) bias = self . random_sample ( self . bias_mean , self . bias_sigma ) else : weight = self . weight_mean bias = self . bias_mean return torch . matmul ( x , weight ) + bias def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"in_features\" , \"out_features\" ]) __init__ ( in_features , out_features , device = None , dtype = None ) \u00b6 Parameters: Name Type Description Default in_features int size of each input sample required out_features int size of each output sample required Source code in pytorch_adapt\\layers\\stochastic_linear.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , in_features : int , out_features : int , device = None , dtype = None ): \"\"\" Arguments: in_features: size of each input sample out_features: size of each output sample \"\"\" factory_kwargs = { \"device\" : device , \"dtype\" : dtype } super () . __init__ () self . in_features = in_features self . out_features = out_features self . weight_mean = nn . Parameter ( torch . empty ( in_features , out_features , ** factory_kwargs ) ) self . weight_sigma = nn . Parameter ( torch . empty ( in_features , out_features , ** factory_kwargs ) ) self . bias_mean = nn . Parameter ( torch . empty ( out_features , ** factory_kwargs )) self . bias_sigma = nn . Parameter ( torch . empty ( out_features , ** factory_kwargs )) self . reset_parameters ()","title":"stochastic_linear"},{"location":"docs/layers/stochastic_linear/#pytorch_adapt.layers.stochastic_linear.StochasticLinear","text":"Bases: nn . Module Implementation of the stochastic layer from Stochastic Classifiers for Unsupervised Domain Adaptation . In train() mode, it uses random weights and biases that are sampled from a learned normal distribution. In eval() mode, the learned mean is used. Source code in pytorch_adapt\\layers\\stochastic_linear.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class StochasticLinear ( nn . Module ): \"\"\" Implementation of the stochastic layer from [Stochastic Classifiers for Unsupervised Domain Adaptation](https://xiatian-zhu.github.io/papers/LuEtAl_CVPR2020.pdf). In ```train()``` mode, it uses random weights and biases that are sampled from a learned normal distribution. In ```eval()``` mode, the learned mean is used. \"\"\" def __init__ ( self , in_features : int , out_features : int , device = None , dtype = None ): \"\"\" Arguments: in_features: size of each input sample out_features: size of each output sample \"\"\" factory_kwargs = { \"device\" : device , \"dtype\" : dtype } super () . __init__ () self . in_features = in_features self . out_features = out_features self . weight_mean = nn . Parameter ( torch . empty ( in_features , out_features , ** factory_kwargs ) ) self . weight_sigma = nn . Parameter ( torch . empty ( in_features , out_features , ** factory_kwargs ) ) self . bias_mean = nn . Parameter ( torch . empty ( out_features , ** factory_kwargs )) self . bias_sigma = nn . Parameter ( torch . empty ( out_features , ** factory_kwargs )) self . reset_parameters () def reset_parameters ( self ): reset_parameters_helper ( self . weight_mean , self . bias_mean ) reset_parameters_helper ( self . weight_sigma , self . bias_sigma ) def random_sample ( self , mean , sigma ): eps = torch . randn ( * sigma . shape , device = sigma . device , dtype = sigma . dtype ) return mean + ( sigma * eps ) def forward ( self , x ): \"\"\"\"\"\" if self . training : weight = self . random_sample ( self . weight_mean , self . weight_sigma ) bias = self . random_sample ( self . bias_mean , self . bias_sigma ) else : weight = self . weight_mean bias = self . bias_mean return torch . matmul ( x , weight ) + bias def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"in_features\" , \"out_features\" ])","title":"StochasticLinear"},{"location":"docs/layers/sufficient_accuracy/","text":"SufficientAccuracy \u00b6 Bases: torch . nn . Module Determines if a batch of logits has accuracy greater than some threshold. This can be used to control program flow. Example: condition_fn = SufficientAccuracy ( threshold = 0.7 ) if condition_fn ( logits , labels ): ... Source code in pytorch_adapt\\layers\\sufficient_accuracy.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class SufficientAccuracy ( torch . nn . Module ): \"\"\" Determines if a batch of logits has accuracy greater than some threshold. This can be used to control program flow. Example: ```python condition_fn = SufficientAccuracy(threshold=0.7) if condition_fn(logits, labels): ... ``` \"\"\" def __init__ ( self , threshold : float , accuracy_func : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , to_probs_func : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: threshold: The accuracy must be greater than this for the forward pass to return True. accuracy_func: function that takes in ```(to_probs_func(logits), labels)``` and returns accuracy. If ```None```, then classification accuracy is used. to_probs_func: function that processes the logits before they get passed to ```accuracy_func```. If ```None```, then ```torch.nn.Sigmoid``` is used \"\"\" super () . __init__ () self . threshold = threshold self . accuracy_func = c_f . default ( accuracy_func , accuracy ) self . to_probs_func = c_f . default ( to_probs_func , torch . nn . Sigmoid ()) pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"accuracy\" , \"threshold\" ] ) def forward ( self , x : torch . Tensor , labels : torch . Tensor ) -> bool : \"\"\" Arguments: x: logits to compute accuracy for labels: the corresponding labels Returns: ```True``` if the accuracy is greater than ```self.threshold``` \"\"\" with torch . no_grad (): x = self . to_probs_func ( x ) labels = labels . type ( torch . int ) self . accuracy = self . accuracy_func ( x , labels ) . item () return self . accuracy > self . threshold def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"threshold\" ]) __init__ ( threshold , accuracy_func = None , to_probs_func = None ) \u00b6 Parameters: Name Type Description Default threshold float The accuracy must be greater than this for the forward pass to return True. required accuracy_func Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] function that takes in (to_probs_func(logits), labels) and returns accuracy. If None , then classification accuracy is used. None to_probs_func Callable [[ torch . Tensor ], torch . Tensor ] function that processes the logits before they get passed to accuracy_func . If None , then torch.nn.Sigmoid is used None Source code in pytorch_adapt\\layers\\sufficient_accuracy.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , threshold : float , accuracy_func : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , to_probs_func : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: threshold: The accuracy must be greater than this for the forward pass to return True. accuracy_func: function that takes in ```(to_probs_func(logits), labels)``` and returns accuracy. If ```None```, then classification accuracy is used. to_probs_func: function that processes the logits before they get passed to ```accuracy_func```. If ```None```, then ```torch.nn.Sigmoid``` is used \"\"\" super () . __init__ () self . threshold = threshold self . accuracy_func = c_f . default ( accuracy_func , accuracy ) self . to_probs_func = c_f . default ( to_probs_func , torch . nn . Sigmoid ()) pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"accuracy\" , \"threshold\" ] ) forward ( x , labels ) \u00b6 Parameters: Name Type Description Default x torch . Tensor logits to compute accuracy for required labels torch . Tensor the corresponding labels required Returns: Type Description bool True if the accuracy is greater than self.threshold Source code in pytorch_adapt\\layers\\sufficient_accuracy.py 48 49 50 51 52 53 54 55 56 57 58 59 60 def forward ( self , x : torch . Tensor , labels : torch . Tensor ) -> bool : \"\"\" Arguments: x: logits to compute accuracy for labels: the corresponding labels Returns: ```True``` if the accuracy is greater than ```self.threshold``` \"\"\" with torch . no_grad (): x = self . to_probs_func ( x ) labels = labels . type ( torch . int ) self . accuracy = self . accuracy_func ( x , labels ) . item () return self . accuracy > self . threshold","title":"sufficient_accuracy"},{"location":"docs/layers/sufficient_accuracy/#pytorch_adapt.layers.sufficient_accuracy.SufficientAccuracy","text":"Bases: torch . nn . Module Determines if a batch of logits has accuracy greater than some threshold. This can be used to control program flow. Example: condition_fn = SufficientAccuracy ( threshold = 0.7 ) if condition_fn ( logits , labels ): ... Source code in pytorch_adapt\\layers\\sufficient_accuracy.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class SufficientAccuracy ( torch . nn . Module ): \"\"\" Determines if a batch of logits has accuracy greater than some threshold. This can be used to control program flow. Example: ```python condition_fn = SufficientAccuracy(threshold=0.7) if condition_fn(logits, labels): ... ``` \"\"\" def __init__ ( self , threshold : float , accuracy_func : Callable [[ torch . Tensor , torch . Tensor ], torch . Tensor ] = None , to_probs_func : Callable [[ torch . Tensor ], torch . Tensor ] = None , ): \"\"\" Arguments: threshold: The accuracy must be greater than this for the forward pass to return True. accuracy_func: function that takes in ```(to_probs_func(logits), labels)``` and returns accuracy. If ```None```, then classification accuracy is used. to_probs_func: function that processes the logits before they get passed to ```accuracy_func```. If ```None```, then ```torch.nn.Sigmoid``` is used \"\"\" super () . __init__ () self . threshold = threshold self . accuracy_func = c_f . default ( accuracy_func , accuracy ) self . to_probs_func = c_f . default ( to_probs_func , torch . nn . Sigmoid ()) pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"accuracy\" , \"threshold\" ] ) def forward ( self , x : torch . Tensor , labels : torch . Tensor ) -> bool : \"\"\" Arguments: x: logits to compute accuracy for labels: the corresponding labels Returns: ```True``` if the accuracy is greater than ```self.threshold``` \"\"\" with torch . no_grad (): x = self . to_probs_func ( x ) labels = labels . type ( torch . int ) self . accuracy = self . accuracy_func ( x , labels ) . item () return self . accuracy > self . threshold def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"threshold\" ])","title":"SufficientAccuracy"},{"location":"docs/layers/uniform_distribution_loss/","text":"UniformDistributionLoss \u00b6 Bases: torch . nn . Module Implementation of the confusion loss from Simultaneous Deep Transfer Across Domains and Tasks . Source code in pytorch_adapt\\layers\\uniform_distribution_loss.py 6 7 8 9 10 11 12 13 14 15 16 17 class UniformDistributionLoss ( torch . nn . Module ): \"\"\" Implementation of the confusion loss from [Simultaneous Deep Transfer Across Domains and Tasks](https://arxiv.org/abs/1510.02192). \"\"\" # *args to make it work as a drop in replacement for CrossEntropyLoss def forward ( self , x , * args ): \"\"\"\"\"\" probs = F . log_softmax ( x , dim = 1 ) avg_probs = torch . mean ( probs , dim = 1 ) return - torch . mean ( avg_probs )","title":"uniform_distribution_loss"},{"location":"docs/layers/uniform_distribution_loss/#pytorch_adapt.layers.uniform_distribution_loss.UniformDistributionLoss","text":"Bases: torch . nn . Module Implementation of the confusion loss from Simultaneous Deep Transfer Across Domains and Tasks . Source code in pytorch_adapt\\layers\\uniform_distribution_loss.py 6 7 8 9 10 11 12 13 14 15 16 17 class UniformDistributionLoss ( torch . nn . Module ): \"\"\" Implementation of the confusion loss from [Simultaneous Deep Transfer Across Domains and Tasks](https://arxiv.org/abs/1510.02192). \"\"\" # *args to make it work as a drop in replacement for CrossEntropyLoss def forward ( self , x , * args ): \"\"\"\"\"\" probs = F . log_softmax ( x , dim = 1 ) avg_probs = torch . mean ( probs , dim = 1 ) return - torch . mean ( avg_probs )","title":"UniformDistributionLoss"},{"location":"docs/layers/vat_loss/","text":"VATLoss \u00b6 Bases: torch . nn . Module Implementation of the loss used in Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning A DIRT-T Approach to Unsupervised Domain Adaptation Source code in pytorch_adapt\\layers\\vat_loss.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class VATLoss ( torch . nn . Module ): \"\"\" Implementation of the loss used in - [Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning](https://arxiv.org/abs/1704.03976) - [A DIRT-T Approach to Unsupervised Domain Adaptation](https://arxiv.org/abs/1802.08735) \"\"\" def __init__ ( self , num_power_iterations : int = 1 , xi : float = 1e-6 , epsilon : float = 8.0 ): \"\"\" Arguments: num_power_iterations: The number of iterations for computing the approximation of the adversarial perturbation. xi: The L2 norm of the the generated noise which is used in the process of creating the perturbation. epsilon: The L2 norm of the generated perturbation. \"\"\" super () . __init__ () self . num_power_iterations = num_power_iterations self . xi = xi self . epsilon = epsilon self . kl_div = torch . nn . KLDivLoss ( reduction = \"batchmean\" ) pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"num_power_iterations\" , \"xi\" , \"epsilon\" ] ) def forward ( self , imgs : torch . Tensor , logits : torch . Tensor , model : torch . nn . Module ) -> torch . Tensor : \"\"\" Arguments: imgs: The input to the model logits: The model's logits computed from ```imgs``` model: The aforementioned model \"\"\" logits = logits . detach () model . apply ( c_f . set_layers_mode ( \"eval\" , c_f . batchnorm_types ())) perturbation = self . get_perturbation ( imgs , logits , model ) new_logits = model ( imgs + perturbation ) preds = F . softmax ( logits , dim = 1 ) new_preds = F . log_softmax ( new_logits , dim = 1 ) model . apply ( c_f . set_layers_mode ( \"train\" , c_f . batchnorm_types ())) return self . kl_div ( new_preds , preds ) def get_perturbation ( self , imgs , original_logits , model ): noise = torch . randn ( * imgs . shape , device = original_logits . device ) original_preds = F . softmax ( original_logits , dim = 1 ) for _ in range ( self . num_power_iterations ): noise . requires_grad = True noise = self . xi * get_normalized_noise ( noise ) noise . retain_grad () new_preds = F . log_softmax ( model ( imgs + noise ), dim = 1 ) dist = self . kl_div ( new_preds , original_preds ) dist . backward ( retain_graph = True ) noise = noise . grad . detach () model . zero_grad () return self . epsilon * get_normalized_noise ( noise ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"num_power_iterations\" , \"xi\" , \"epsilon\" ]) __init__ ( num_power_iterations = 1 , xi = 1e-06 , epsilon = 8.0 ) \u00b6 Parameters: Name Type Description Default num_power_iterations int The number of iterations for computing the approximation of the adversarial perturbation. 1 xi float The L2 norm of the the generated noise which is used in the process of creating the perturbation. 1e-06 epsilon float The L2 norm of the generated perturbation. 8.0 Source code in pytorch_adapt\\layers\\vat_loss.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , num_power_iterations : int = 1 , xi : float = 1e-6 , epsilon : float = 8.0 ): \"\"\" Arguments: num_power_iterations: The number of iterations for computing the approximation of the adversarial perturbation. xi: The L2 norm of the the generated noise which is used in the process of creating the perturbation. epsilon: The L2 norm of the generated perturbation. \"\"\" super () . __init__ () self . num_power_iterations = num_power_iterations self . xi = xi self . epsilon = epsilon self . kl_div = torch . nn . KLDivLoss ( reduction = \"batchmean\" ) pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"num_power_iterations\" , \"xi\" , \"epsilon\" ] ) forward ( imgs , logits , model ) \u00b6 Parameters: Name Type Description Default imgs torch . Tensor The input to the model required logits torch . Tensor The model's logits computed from imgs required model torch . nn . Module The aforementioned model required Source code in pytorch_adapt\\layers\\vat_loss.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def forward ( self , imgs : torch . Tensor , logits : torch . Tensor , model : torch . nn . Module ) -> torch . Tensor : \"\"\" Arguments: imgs: The input to the model logits: The model's logits computed from ```imgs``` model: The aforementioned model \"\"\" logits = logits . detach () model . apply ( c_f . set_layers_mode ( \"eval\" , c_f . batchnorm_types ())) perturbation = self . get_perturbation ( imgs , logits , model ) new_logits = model ( imgs + perturbation ) preds = F . softmax ( logits , dim = 1 ) new_preds = F . log_softmax ( new_logits , dim = 1 ) model . apply ( c_f . set_layers_mode ( \"train\" , c_f . batchnorm_types ())) return self . kl_div ( new_preds , preds )","title":"vat_loss"},{"location":"docs/layers/vat_loss/#pytorch_adapt.layers.vat_loss.VATLoss","text":"Bases: torch . nn . Module Implementation of the loss used in Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning A DIRT-T Approach to Unsupervised Domain Adaptation Source code in pytorch_adapt\\layers\\vat_loss.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class VATLoss ( torch . nn . Module ): \"\"\" Implementation of the loss used in - [Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning](https://arxiv.org/abs/1704.03976) - [A DIRT-T Approach to Unsupervised Domain Adaptation](https://arxiv.org/abs/1802.08735) \"\"\" def __init__ ( self , num_power_iterations : int = 1 , xi : float = 1e-6 , epsilon : float = 8.0 ): \"\"\" Arguments: num_power_iterations: The number of iterations for computing the approximation of the adversarial perturbation. xi: The L2 norm of the the generated noise which is used in the process of creating the perturbation. epsilon: The L2 norm of the generated perturbation. \"\"\" super () . __init__ () self . num_power_iterations = num_power_iterations self . xi = xi self . epsilon = epsilon self . kl_div = torch . nn . KLDivLoss ( reduction = \"batchmean\" ) pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"num_power_iterations\" , \"xi\" , \"epsilon\" ] ) def forward ( self , imgs : torch . Tensor , logits : torch . Tensor , model : torch . nn . Module ) -> torch . Tensor : \"\"\" Arguments: imgs: The input to the model logits: The model's logits computed from ```imgs``` model: The aforementioned model \"\"\" logits = logits . detach () model . apply ( c_f . set_layers_mode ( \"eval\" , c_f . batchnorm_types ())) perturbation = self . get_perturbation ( imgs , logits , model ) new_logits = model ( imgs + perturbation ) preds = F . softmax ( logits , dim = 1 ) new_preds = F . log_softmax ( new_logits , dim = 1 ) model . apply ( c_f . set_layers_mode ( \"train\" , c_f . batchnorm_types ())) return self . kl_div ( new_preds , preds ) def get_perturbation ( self , imgs , original_logits , model ): noise = torch . randn ( * imgs . shape , device = original_logits . device ) original_preds = F . softmax ( original_logits , dim = 1 ) for _ in range ( self . num_power_iterations ): noise . requires_grad = True noise = self . xi * get_normalized_noise ( noise ) noise . retain_grad () new_preds = F . log_softmax ( model ( imgs + noise ), dim = 1 ) dist = self . kl_div ( new_preds , original_preds ) dist . backward ( retain_graph = True ) noise = noise . grad . detach () model . zero_grad () return self . epsilon * get_normalized_noise ( noise ) def extra_repr ( self ): \"\"\"\"\"\" return c_f . extra_repr ( self , [ \"num_power_iterations\" , \"xi\" , \"epsilon\" ])","title":"VATLoss"},{"location":"docs/meta_validators/","text":"The following can be imported like this (using ForwardOnlyValidator as an example): from pytorch_adapt.meta_validators import ForwardOnlyValidator Direct module members \u00b6 ForwardOnlyValidator ReverseValidator","title":"meta_validators"},{"location":"docs/meta_validators/#direct-module-members","text":"ForwardOnlyValidator ReverseValidator","title":"Direct module members"},{"location":"docs/meta_validators/forward_only_validator/","text":"ForwardOnlyValidator \u00b6 This is basically a pass-through function. It returns the best score and best epoch that is returned by the inner adapter. Source code in pytorch_adapt\\meta_validators\\forward_only_validator.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class ForwardOnlyValidator : \"\"\" This is basically a pass-through function. It returns the best score and best epoch that is returned by the inner adapter. \"\"\" def run ( self , adapter , ** kwargs ) -> Tuple [ float , int ]: \"\"\" Arguments: adapter: the framework-wrapped adapter. **kwargs: keyword arguments to be passed into adapter.run() Returns: the best score and best epoch \"\"\" if not adapter . validator : raise KeyError ( \"An adapter validator is required when using ForwardOnlyValidator\" ) return adapter . run ( ** kwargs ) run ( adapter , ** kwargs ) \u00b6 Parameters: Name Type Description Default adapter the framework-wrapped adapter. required **kwargs keyword arguments to be passed into adapter.run() {} Returns: Type Description Tuple [ float , int ] the best score and best epoch Source code in pytorch_adapt\\meta_validators\\forward_only_validator.py 11 12 13 14 15 16 17 18 19 20 21 22 23 def run ( self , adapter , ** kwargs ) -> Tuple [ float , int ]: \"\"\" Arguments: adapter: the framework-wrapped adapter. **kwargs: keyword arguments to be passed into adapter.run() Returns: the best score and best epoch \"\"\" if not adapter . validator : raise KeyError ( \"An adapter validator is required when using ForwardOnlyValidator\" ) return adapter . run ( ** kwargs )","title":"forward_only_validator"},{"location":"docs/meta_validators/forward_only_validator/#pytorch_adapt.meta_validators.forward_only_validator.ForwardOnlyValidator","text":"This is basically a pass-through function. It returns the best score and best epoch that is returned by the inner adapter. Source code in pytorch_adapt\\meta_validators\\forward_only_validator.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class ForwardOnlyValidator : \"\"\" This is basically a pass-through function. It returns the best score and best epoch that is returned by the inner adapter. \"\"\" def run ( self , adapter , ** kwargs ) -> Tuple [ float , int ]: \"\"\" Arguments: adapter: the framework-wrapped adapter. **kwargs: keyword arguments to be passed into adapter.run() Returns: the best score and best epoch \"\"\" if not adapter . validator : raise KeyError ( \"An adapter validator is required when using ForwardOnlyValidator\" ) return adapter . run ( ** kwargs )","title":"ForwardOnlyValidator"},{"location":"docs/meta_validators/reverse_validator/","text":"ReverseValidator \u00b6 Reverse validation consists of three steps. Train a model on the labeled source and unlabeled target Use the trained model to create pseudolabels for the target dataset. Train a new model on the labeled target and \"unlabeled\" source. The final score is the accuracy of the model from step 3. Source code in pytorch_adapt\\meta_validators\\reverse_validator.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class ReverseValidator : \"\"\" Reverse validation consists of three steps. 1. Train a model on the labeled source and unlabeled target 2. Use the trained model to create pseudolabels for the target dataset. 3. Train a new model on the labeled target and \"unlabeled\" source. The final score is the accuracy of the model from step 3. \"\"\" def __init__ ( self ): self . pseudo_train = None self . pseudo_val = None def run ( self , forward_adapter , reverse_adapter , forward_kwargs , reverse_kwargs , pl_dataloader_creator = None , ) -> Tuple [ float , int ]: \"\"\" Arguments: forward_adapter: the framework-wrapped adapter for step 1. reverse_adapter: the framework-wrapped adapter for step 3. forward_kwargs: a dict of keyword arguments to be passed to forward_adapter.run() reverse_kwargs: a dict of keyword arguments to be passed to reverse_adapter.run() pl_dataloader_creator: An optional DataloaderCreator for obtaining pseudolabels in step 2. Returns: the best score and best epoch of the reverse model \"\"\" if \"datasets\" in reverse_kwargs : raise KeyError ( \"'datasets' should not be in reverse_kwargs because the reverse datasets will be pseudo labeled.\" ) if not reverse_adapter . validator : raise KeyError ( \"reverse_adapter must include 'validator'\" ) forward_adapter . run ( ** forward_kwargs ) if all ( getattr ( forward_adapter , x ) for x in [ \"validator\" , \"checkpoint_fn\" ]): forward_adapter . checkpoint_fn . load_best_checkpoint ( { \"models\" : forward_adapter . adapter . models }, ) datasets = forward_kwargs [ \"datasets\" ] pl_dataloader_creator = c_f . default ( pl_dataloader_creator , DataloaderCreator , { \"all_val\" : True } ) d = {} d [ \"src_train\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_train\" , pl_dataloader_creator ) d [ \"src_val\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_val\" , pl_dataloader_creator ) d [ \"target_train\" ] = TargetDataset ( datasets [ \"src_train\" ] . dataset ) d [ \"target_val\" ] = TargetDataset ( datasets [ \"src_val\" ] . dataset ) d [ \"train\" ] = CombinedSourceAndTargetDataset ( d [ \"src_train\" ], d [ \"target_train\" ]) self . pseudo_train = d [ \"src_train\" ] self . pseudo_val = d [ \"src_val\" ] reverse_kwargs [ \"datasets\" ] = d return reverse_adapter . run ( ** reverse_kwargs ) run ( forward_adapter , reverse_adapter , forward_kwargs , reverse_kwargs , pl_dataloader_creator = None ) \u00b6 Parameters: Name Type Description Default forward_adapter the framework-wrapped adapter for step 1. required reverse_adapter the framework-wrapped adapter for step 3. required forward_kwargs a dict of keyword arguments to be passed to forward_adapter.run() required reverse_kwargs a dict of keyword arguments to be passed to reverse_adapter.run() required pl_dataloader_creator An optional DataloaderCreator for obtaining pseudolabels in step 2. None Returns: Type Description Tuple [ float , int ] the best score and best epoch of the reverse model Source code in pytorch_adapt\\meta_validators\\reverse_validator.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def run ( self , forward_adapter , reverse_adapter , forward_kwargs , reverse_kwargs , pl_dataloader_creator = None , ) -> Tuple [ float , int ]: \"\"\" Arguments: forward_adapter: the framework-wrapped adapter for step 1. reverse_adapter: the framework-wrapped adapter for step 3. forward_kwargs: a dict of keyword arguments to be passed to forward_adapter.run() reverse_kwargs: a dict of keyword arguments to be passed to reverse_adapter.run() pl_dataloader_creator: An optional DataloaderCreator for obtaining pseudolabels in step 2. Returns: the best score and best epoch of the reverse model \"\"\" if \"datasets\" in reverse_kwargs : raise KeyError ( \"'datasets' should not be in reverse_kwargs because the reverse datasets will be pseudo labeled.\" ) if not reverse_adapter . validator : raise KeyError ( \"reverse_adapter must include 'validator'\" ) forward_adapter . run ( ** forward_kwargs ) if all ( getattr ( forward_adapter , x ) for x in [ \"validator\" , \"checkpoint_fn\" ]): forward_adapter . checkpoint_fn . load_best_checkpoint ( { \"models\" : forward_adapter . adapter . models }, ) datasets = forward_kwargs [ \"datasets\" ] pl_dataloader_creator = c_f . default ( pl_dataloader_creator , DataloaderCreator , { \"all_val\" : True } ) d = {} d [ \"src_train\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_train\" , pl_dataloader_creator ) d [ \"src_val\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_val\" , pl_dataloader_creator ) d [ \"target_train\" ] = TargetDataset ( datasets [ \"src_train\" ] . dataset ) d [ \"target_val\" ] = TargetDataset ( datasets [ \"src_val\" ] . dataset ) d [ \"train\" ] = CombinedSourceAndTargetDataset ( d [ \"src_train\" ], d [ \"target_train\" ]) self . pseudo_train = d [ \"src_train\" ] self . pseudo_val = d [ \"src_val\" ] reverse_kwargs [ \"datasets\" ] = d return reverse_adapter . run ( ** reverse_kwargs )","title":"reverse_validator"},{"location":"docs/meta_validators/reverse_validator/#pytorch_adapt.meta_validators.reverse_validator.ReverseValidator","text":"Reverse validation consists of three steps. Train a model on the labeled source and unlabeled target Use the trained model to create pseudolabels for the target dataset. Train a new model on the labeled target and \"unlabeled\" source. The final score is the accuracy of the model from step 3. Source code in pytorch_adapt\\meta_validators\\reverse_validator.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class ReverseValidator : \"\"\" Reverse validation consists of three steps. 1. Train a model on the labeled source and unlabeled target 2. Use the trained model to create pseudolabels for the target dataset. 3. Train a new model on the labeled target and \"unlabeled\" source. The final score is the accuracy of the model from step 3. \"\"\" def __init__ ( self ): self . pseudo_train = None self . pseudo_val = None def run ( self , forward_adapter , reverse_adapter , forward_kwargs , reverse_kwargs , pl_dataloader_creator = None , ) -> Tuple [ float , int ]: \"\"\" Arguments: forward_adapter: the framework-wrapped adapter for step 1. reverse_adapter: the framework-wrapped adapter for step 3. forward_kwargs: a dict of keyword arguments to be passed to forward_adapter.run() reverse_kwargs: a dict of keyword arguments to be passed to reverse_adapter.run() pl_dataloader_creator: An optional DataloaderCreator for obtaining pseudolabels in step 2. Returns: the best score and best epoch of the reverse model \"\"\" if \"datasets\" in reverse_kwargs : raise KeyError ( \"'datasets' should not be in reverse_kwargs because the reverse datasets will be pseudo labeled.\" ) if not reverse_adapter . validator : raise KeyError ( \"reverse_adapter must include 'validator'\" ) forward_adapter . run ( ** forward_kwargs ) if all ( getattr ( forward_adapter , x ) for x in [ \"validator\" , \"checkpoint_fn\" ]): forward_adapter . checkpoint_fn . load_best_checkpoint ( { \"models\" : forward_adapter . adapter . models }, ) datasets = forward_kwargs [ \"datasets\" ] pl_dataloader_creator = c_f . default ( pl_dataloader_creator , DataloaderCreator , { \"all_val\" : True } ) d = {} d [ \"src_train\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_train\" , pl_dataloader_creator ) d [ \"src_val\" ] = get_pseudo_labeled_dataset ( forward_adapter , datasets , \"target_val\" , pl_dataloader_creator ) d [ \"target_train\" ] = TargetDataset ( datasets [ \"src_train\" ] . dataset ) d [ \"target_val\" ] = TargetDataset ( datasets [ \"src_val\" ] . dataset ) d [ \"train\" ] = CombinedSourceAndTargetDataset ( d [ \"src_train\" ], d [ \"target_train\" ]) self . pseudo_train = d [ \"src_train\" ] self . pseudo_val = d [ \"src_val\" ] reverse_kwargs [ \"datasets\" ] = d return reverse_adapter . run ( ** reverse_kwargs )","title":"ReverseValidator"},{"location":"docs/models/","text":"The following can be imported like this (using Classifier as an example): from pytorch_adapt.models import Classifier Direct module members \u00b6 Classifier Discriminator MNISTFeatures mnistC mnistG office31C office31G officehomeC officehomeG","title":"models"},{"location":"docs/models/#direct-module-members","text":"Classifier Discriminator MNISTFeatures mnistC mnistG office31C office31G officehomeC officehomeG","title":"Direct module members"},{"location":"docs/models/classifier/","text":"Classifier \u00b6 Bases: nn . Module A 3-layer MLP for classification. Source code in pytorch_adapt\\models\\classifier.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Classifier ( nn . Module ): \"\"\" A 3-layer MLP for classification. \"\"\" def __init__ ( self , num_classes , in_size = 2048 , h = 1024 ): \"\"\" Arguments: num_classes: size of the output in_size: size of the input h: hidden layer size \"\"\" super () . __init__ () self . h = h self . net = nn . Sequential ( nn . Linear ( in_size , h ), nn . ReLU (), nn . Dropout (), nn . Linear ( h , h // 2 ), nn . ReLU (), nn . Dropout (), nn . Linear ( h // 2 , num_classes ), ) def forward ( self , x ): \"\"\"\"\"\" return self . net ( x ) __init__ ( num_classes , in_size = 2048 , h = 1024 ) \u00b6 Parameters: Name Type Description Default num_classes size of the output required in_size size of the input 2048 h hidden layer size 1024 Source code in pytorch_adapt\\models\\classifier.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , num_classes , in_size = 2048 , h = 1024 ): \"\"\" Arguments: num_classes: size of the output in_size: size of the input h: hidden layer size \"\"\" super () . __init__ () self . h = h self . net = nn . Sequential ( nn . Linear ( in_size , h ), nn . ReLU (), nn . Dropout (), nn . Linear ( h , h // 2 ), nn . ReLU (), nn . Dropout (), nn . Linear ( h // 2 , num_classes ), )","title":"classifier"},{"location":"docs/models/classifier/#pytorch_adapt.models.classifier.Classifier","text":"Bases: nn . Module A 3-layer MLP for classification. Source code in pytorch_adapt\\models\\classifier.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Classifier ( nn . Module ): \"\"\" A 3-layer MLP for classification. \"\"\" def __init__ ( self , num_classes , in_size = 2048 , h = 1024 ): \"\"\" Arguments: num_classes: size of the output in_size: size of the input h: hidden layer size \"\"\" super () . __init__ () self . h = h self . net = nn . Sequential ( nn . Linear ( in_size , h ), nn . ReLU (), nn . Dropout (), nn . Linear ( h , h // 2 ), nn . ReLU (), nn . Dropout (), nn . Linear ( h // 2 , num_classes ), ) def forward ( self , x ): \"\"\"\"\"\" return self . net ( x )","title":"Classifier"},{"location":"docs/models/discriminator/","text":"Discriminator \u00b6 Bases: nn . Module A 3-layer MLP for domain classification. Source code in pytorch_adapt\\models\\discriminator.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Discriminator ( nn . Module ): \"\"\" A 3-layer MLP for domain classification. \"\"\" def __init__ ( self , in_size = 2048 , h = 2048 , out_size = 1 ): \"\"\" Arguments: in_size: size of the input h: hidden layer size out_size: size of the output \"\"\" super () . __init__ () self . h = h self . net = nn . Sequential ( nn . Linear ( in_size , h ), nn . ReLU (), nn . Linear ( h , h ), nn . ReLU (), nn . Linear ( h , out_size ), ) self . out_size = out_size def forward ( self , x ): \"\"\"\"\"\" return self . net ( x ) . squeeze ( 1 ) __init__ ( in_size = 2048 , h = 2048 , out_size = 1 ) \u00b6 Parameters: Name Type Description Default in_size size of the input 2048 h hidden layer size 2048 out_size size of the output 1 Source code in pytorch_adapt\\models\\discriminator.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , in_size = 2048 , h = 2048 , out_size = 1 ): \"\"\" Arguments: in_size: size of the input h: hidden layer size out_size: size of the output \"\"\" super () . __init__ () self . h = h self . net = nn . Sequential ( nn . Linear ( in_size , h ), nn . ReLU (), nn . Linear ( h , h ), nn . ReLU (), nn . Linear ( h , out_size ), ) self . out_size = out_size","title":"discriminator"},{"location":"docs/models/discriminator/#pytorch_adapt.models.discriminator.Discriminator","text":"Bases: nn . Module A 3-layer MLP for domain classification. Source code in pytorch_adapt\\models\\discriminator.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Discriminator ( nn . Module ): \"\"\" A 3-layer MLP for domain classification. \"\"\" def __init__ ( self , in_size = 2048 , h = 2048 , out_size = 1 ): \"\"\" Arguments: in_size: size of the input h: hidden layer size out_size: size of the output \"\"\" super () . __init__ () self . h = h self . net = nn . Sequential ( nn . Linear ( in_size , h ), nn . ReLU (), nn . Linear ( h , h ), nn . ReLU (), nn . Linear ( h , out_size ), ) self . out_size = out_size def forward ( self , x ): \"\"\"\"\"\" return self . net ( x ) . squeeze ( 1 )","title":"Discriminator"},{"location":"docs/models/mnist/","text":"MNISTFeatures \u00b6 Bases: nn . Module A small convnet for extracting features from MNIST. Source code in pytorch_adapt\\models\\mnist.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class MNISTFeatures ( nn . Module ): \"\"\" A small convnet for extracting features from MNIST. \"\"\" def __init__ ( self ): \"\"\" \"\"\" super () . __init__ () self . conv1 = nn . Conv2d ( 3 , 32 , 5 , 1 ) self . conv2 = nn . Conv2d ( 32 , 48 , 5 , 1 ) self . fc = nn . Identity () def forward ( self , x ): \"\"\" \"\"\" x = self . conv1 ( x ) x = F . relu ( x ) x = F . max_pool2d ( x , kernel_size = 2 , stride = 2 ) x = self . conv2 ( x ) x = F . relu ( x ) x = F . max_pool2d ( x , kernel_size = 2 , stride = 2 ) x = torch . flatten ( x , start_dim = 1 ) return self . fc ( x )","title":"mnist"},{"location":"docs/models/mnist/#pytorch_adapt.models.mnist.MNISTFeatures","text":"Bases: nn . Module A small convnet for extracting features from MNIST. Source code in pytorch_adapt\\models\\mnist.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class MNISTFeatures ( nn . Module ): \"\"\" A small convnet for extracting features from MNIST. \"\"\" def __init__ ( self ): \"\"\" \"\"\" super () . __init__ () self . conv1 = nn . Conv2d ( 3 , 32 , 5 , 1 ) self . conv2 = nn . Conv2d ( 32 , 48 , 5 , 1 ) self . fc = nn . Identity () def forward ( self , x ): \"\"\" \"\"\" x = self . conv1 ( x ) x = F . relu ( x ) x = F . max_pool2d ( x , kernel_size = 2 , stride = 2 ) x = self . conv2 ( x ) x = F . relu ( x ) x = F . max_pool2d ( x , kernel_size = 2 , stride = 2 ) x = torch . flatten ( x , start_dim = 1 ) return self . fc ( x )","title":"MNISTFeatures"},{"location":"docs/models/pretrained/","text":"mnistC ( num_classes = 10 , in_size = 1200 , h = 256 , pretrained = False , progress = True , ** kwargs ) \u00b6 Returns: Type Description A Classifier model trained on the MNIST dataset, if pretrained == True . Source code in pytorch_adapt\\models\\pretrained.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def mnistC ( num_classes = 10 , in_size = 1200 , h = 256 , pretrained = False , progress = True , ** kwargs ): \"\"\" Returns: A [```Classifier```][pytorch_adapt.models.Classifier] model trained on the MNIST dataset, if ```pretrained == True```. \"\"\" model = Classifier ( num_classes = num_classes , in_size = in_size , h = h ) url = \"https://cornell.box.com/shared/static/j4zrogronmievq1csulrkai7zjm27gcq\" h = \"ac7b5a13df2ef3522b6550a147eb44dde8ff4fead3ddedc540d9fe63c9d597c1\" file_name = f \"mnistC- { h [: 8 ] } .pth\" return download_weights ( model , url , pretrained , progress = progress , file_name = file_name , ** kwargs ) mnistG ( pretrained = False , progress = True , ** kwargs ) \u00b6 Returns: Type Description An MNISTFeatures model trained on the MNIST dataset, if pretrained == True . Source code in pytorch_adapt\\models\\pretrained.py 6 7 8 9 10 11 12 13 14 15 16 17 18 def mnistG ( pretrained = False , progress = True , ** kwargs ): \"\"\" Returns: An [```MNISTFeatures```][pytorch_adapt.models.MNISTFeatures] model trained on the MNIST dataset, if ```pretrained == True```. \"\"\" model = MNISTFeatures () url = \"https://cornell.box.com/shared/static/tdx0ts24e273j7mf3r2ox7a12xh4fdfy\" h = \"68ee79452f1d5301be2329dfa542ac6fa18de99e09d6540838606d9d700b09c8\" file_name = f \"mnistG- { h [: 8 ] } .pth\" return download_weights ( model , url , pretrained , progress = progress , file_name = file_name , ** kwargs ) office31C ( domain = None , num_classes = 31 , in_size = 2048 , h = 256 , pretrained = False , progress = True , ** kwargs ) \u00b6 Returns: Type Description A Classifier model trained on the specified domain of the Office31 dataset, if pretrained == True . For example ```python model = office31(domain=\"amazon\", pretrained=True) ``` Source code in pytorch_adapt\\models\\pretrained.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def office31C ( domain = None , num_classes = 31 , in_size = 2048 , h = 256 , pretrained = False , progress = True , ** kwargs , ): \"\"\" Returns: A [```Classifier```][pytorch_adapt.models.Classifier] model trained on the specified ```domain``` of the [Office31][pytorch_adapt.datasets.Office31] dataset, if ```pretrained == True```. For example ```python model = office31(domain=\"amazon\", pretrained=True) ``` \"\"\" if pretrained and not domain : raise ValueError ( \"if pretrained, domain must be specified\" ) model = Classifier ( num_classes = num_classes , in_size = in_size , h = h ) if not pretrained : return model url = { \"amazon\" : \"https://cornell.box.com/shared/static/6h165jqlxcpo16jbs3a7vpvslb6u9vaq\" , \"dslr\" : \"https://cornell.box.com/shared/static/t97sedzf4wrto3yfvr8hxivyblqkljiq\" , \"webcam\" : \"https://cornell.box.com/shared/static/zuv7be39v8bijwggrvfzlyw1h0pfwrb4\" , }[ domain ] h = { \"amazon\" : \"6e2fb6f392538172515c2c673a8b3ead7aad8b88b44aad6468c7e9b11761b667\" , \"dslr\" : \"fc0acd7a71eb5f12d4af619e5c63bcc42e5a23441bbd105fe0f7a37c26f37d80\" , \"webcam\" : \"b2bb55978380fa9ca6452cba30e0ac2a19b7166d8348bcc1554fdabd185e4cdd\" , }[ domain ] file_name = f \"office31C { domain } - { h [: 8 ] } .pth\" return download_weights ( model , url , pretrained , progress = progress , file_name = file_name , ** kwargs ) office31G ( * args , ** kwargs ) \u00b6 Returns: Type Description A ResNet50 model trained on ImageNet, if pretrained == True . Source code in pytorch_adapt\\models\\pretrained.py 53 54 55 56 57 58 def office31G ( * args , ** kwargs ): \"\"\" Returns: A ResNet50 model trained on ImageNet, if ```pretrained == True```. \"\"\" return resnet50 ( * args , ** kwargs ) officehomeC ( domain = None , num_classes = 65 , in_size = 2048 , h = 256 , pretrained = False , progress = True , ** kwargs ) \u00b6 Returns: Type Description A Classifier model trained on the specified domain of the OfficeHome dataset, if pretrained == True . For example ```python model = officehomeC(domain=\"art\", pretrained=True) ``` Source code in pytorch_adapt\\models\\pretrained.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def officehomeC ( domain = None , num_classes = 65 , in_size = 2048 , h = 256 , pretrained = False , progress = True , ** kwargs , ): \"\"\" Returns: A [```Classifier```][pytorch_adapt.models.Classifier] model trained on the specified ```domain``` of the [OfficeHome][pytorch_adapt.datasets.OfficeHome] dataset, if ```pretrained == True```. For example ```python model = officehomeC(domain=\"art\", pretrained=True) ``` \"\"\" if pretrained and not domain : raise ValueError ( \"if pretrained, domain must be specified\" ) model = Classifier ( num_classes = num_classes , in_size = in_size , h = h ) if not pretrained : return model url = { \"art\" : \"https://cornell.box.com/shared/static/wxg7v32e2m0jcmq53amhdipty9veb2xx\" , \"clipart\" : \"https://cornell.box.com/shared/static/4dhwhj6fkzg9lfgu0mfskt2kby8mznez\" , \"product\" : \"https://cornell.box.com/shared/static/r6f3ltgve5g2lrcdtoykj84rlyzqs6ga\" , \"real\" : \"https://cornell.box.com/shared/static/1lf1foq65m77pdpc50isdgsc8k71ei29\" , }[ domain ] h = { \"art\" : \"8db546ff250d2b54899f92e482e80a68411cbe525134d429987b57d3b0571e4b\" , \"clipart\" : \"8e145cc6d2df3ff428aeafa43066bbde97d56e9f844b34408bdca74125e62590\" , \"product\" : \"472ff36fdf13ec6c1fa1236d1d0800e2a5cf2e3d366b6b63ff5807dff6a761d8\" , \"real\" : \"f0c8d6e941d4f488ff2438eb5cccdc59e78f35961e48f03d2186752e5878c697\" , }[ domain ] file_name = f \"officehomeC { domain } - { h [: 8 ] } .pth\" return download_weights ( model , url , pretrained , progress = progress , file_name = file_name , ** kwargs ) officehomeG ( * args , ** kwargs ) \u00b6 Returns: Type Description A ResNet50 model trained on ImageNet, if pretrained == True . Source code in pytorch_adapt\\models\\pretrained.py 102 103 104 105 106 107 def officehomeG ( * args , ** kwargs ): \"\"\" Returns: A ResNet50 model trained on ImageNet, if ```pretrained == True```. \"\"\" return resnet50 ( * args , ** kwargs )","title":"pretrained"},{"location":"docs/models/pretrained/#pytorch_adapt.models.pretrained.mnistC","text":"Returns: Type Description A Classifier model trained on the MNIST dataset, if pretrained == True . Source code in pytorch_adapt\\models\\pretrained.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def mnistC ( num_classes = 10 , in_size = 1200 , h = 256 , pretrained = False , progress = True , ** kwargs ): \"\"\" Returns: A [```Classifier```][pytorch_adapt.models.Classifier] model trained on the MNIST dataset, if ```pretrained == True```. \"\"\" model = Classifier ( num_classes = num_classes , in_size = in_size , h = h ) url = \"https://cornell.box.com/shared/static/j4zrogronmievq1csulrkai7zjm27gcq\" h = \"ac7b5a13df2ef3522b6550a147eb44dde8ff4fead3ddedc540d9fe63c9d597c1\" file_name = f \"mnistC- { h [: 8 ] } .pth\" return download_weights ( model , url , pretrained , progress = progress , file_name = file_name , ** kwargs )","title":"mnistC()"},{"location":"docs/models/pretrained/#pytorch_adapt.models.pretrained.mnistG","text":"Returns: Type Description An MNISTFeatures model trained on the MNIST dataset, if pretrained == True . Source code in pytorch_adapt\\models\\pretrained.py 6 7 8 9 10 11 12 13 14 15 16 17 18 def mnistG ( pretrained = False , progress = True , ** kwargs ): \"\"\" Returns: An [```MNISTFeatures```][pytorch_adapt.models.MNISTFeatures] model trained on the MNIST dataset, if ```pretrained == True```. \"\"\" model = MNISTFeatures () url = \"https://cornell.box.com/shared/static/tdx0ts24e273j7mf3r2ox7a12xh4fdfy\" h = \"68ee79452f1d5301be2329dfa542ac6fa18de99e09d6540838606d9d700b09c8\" file_name = f \"mnistG- { h [: 8 ] } .pth\" return download_weights ( model , url , pretrained , progress = progress , file_name = file_name , ** kwargs )","title":"mnistG()"},{"location":"docs/models/pretrained/#pytorch_adapt.models.pretrained.office31C","text":"Returns: Type Description A Classifier model trained on the specified domain of the Office31 dataset, if pretrained == True . For example ```python model = office31(domain=\"amazon\", pretrained=True) ``` Source code in pytorch_adapt\\models\\pretrained.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def office31C ( domain = None , num_classes = 31 , in_size = 2048 , h = 256 , pretrained = False , progress = True , ** kwargs , ): \"\"\" Returns: A [```Classifier```][pytorch_adapt.models.Classifier] model trained on the specified ```domain``` of the [Office31][pytorch_adapt.datasets.Office31] dataset, if ```pretrained == True```. For example ```python model = office31(domain=\"amazon\", pretrained=True) ``` \"\"\" if pretrained and not domain : raise ValueError ( \"if pretrained, domain must be specified\" ) model = Classifier ( num_classes = num_classes , in_size = in_size , h = h ) if not pretrained : return model url = { \"amazon\" : \"https://cornell.box.com/shared/static/6h165jqlxcpo16jbs3a7vpvslb6u9vaq\" , \"dslr\" : \"https://cornell.box.com/shared/static/t97sedzf4wrto3yfvr8hxivyblqkljiq\" , \"webcam\" : \"https://cornell.box.com/shared/static/zuv7be39v8bijwggrvfzlyw1h0pfwrb4\" , }[ domain ] h = { \"amazon\" : \"6e2fb6f392538172515c2c673a8b3ead7aad8b88b44aad6468c7e9b11761b667\" , \"dslr\" : \"fc0acd7a71eb5f12d4af619e5c63bcc42e5a23441bbd105fe0f7a37c26f37d80\" , \"webcam\" : \"b2bb55978380fa9ca6452cba30e0ac2a19b7166d8348bcc1554fdabd185e4cdd\" , }[ domain ] file_name = f \"office31C { domain } - { h [: 8 ] } .pth\" return download_weights ( model , url , pretrained , progress = progress , file_name = file_name , ** kwargs )","title":"office31C()"},{"location":"docs/models/pretrained/#pytorch_adapt.models.pretrained.office31G","text":"Returns: Type Description A ResNet50 model trained on ImageNet, if pretrained == True . Source code in pytorch_adapt\\models\\pretrained.py 53 54 55 56 57 58 def office31G ( * args , ** kwargs ): \"\"\" Returns: A ResNet50 model trained on ImageNet, if ```pretrained == True```. \"\"\" return resnet50 ( * args , ** kwargs )","title":"office31G()"},{"location":"docs/models/pretrained/#pytorch_adapt.models.pretrained.officehomeC","text":"Returns: Type Description A Classifier model trained on the specified domain of the OfficeHome dataset, if pretrained == True . For example ```python model = officehomeC(domain=\"art\", pretrained=True) ``` Source code in pytorch_adapt\\models\\pretrained.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def officehomeC ( domain = None , num_classes = 65 , in_size = 2048 , h = 256 , pretrained = False , progress = True , ** kwargs , ): \"\"\" Returns: A [```Classifier```][pytorch_adapt.models.Classifier] model trained on the specified ```domain``` of the [OfficeHome][pytorch_adapt.datasets.OfficeHome] dataset, if ```pretrained == True```. For example ```python model = officehomeC(domain=\"art\", pretrained=True) ``` \"\"\" if pretrained and not domain : raise ValueError ( \"if pretrained, domain must be specified\" ) model = Classifier ( num_classes = num_classes , in_size = in_size , h = h ) if not pretrained : return model url = { \"art\" : \"https://cornell.box.com/shared/static/wxg7v32e2m0jcmq53amhdipty9veb2xx\" , \"clipart\" : \"https://cornell.box.com/shared/static/4dhwhj6fkzg9lfgu0mfskt2kby8mznez\" , \"product\" : \"https://cornell.box.com/shared/static/r6f3ltgve5g2lrcdtoykj84rlyzqs6ga\" , \"real\" : \"https://cornell.box.com/shared/static/1lf1foq65m77pdpc50isdgsc8k71ei29\" , }[ domain ] h = { \"art\" : \"8db546ff250d2b54899f92e482e80a68411cbe525134d429987b57d3b0571e4b\" , \"clipart\" : \"8e145cc6d2df3ff428aeafa43066bbde97d56e9f844b34408bdca74125e62590\" , \"product\" : \"472ff36fdf13ec6c1fa1236d1d0800e2a5cf2e3d366b6b63ff5807dff6a761d8\" , \"real\" : \"f0c8d6e941d4f488ff2438eb5cccdc59e78f35961e48f03d2186752e5878c697\" , }[ domain ] file_name = f \"officehomeC { domain } - { h [: 8 ] } .pth\" return download_weights ( model , url , pretrained , progress = progress , file_name = file_name , ** kwargs )","title":"officehomeC()"},{"location":"docs/models/pretrained/#pytorch_adapt.models.pretrained.officehomeG","text":"Returns: Type Description A ResNet50 model trained on ImageNet, if pretrained == True . Source code in pytorch_adapt\\models\\pretrained.py 102 103 104 105 106 107 def officehomeG ( * args , ** kwargs ): \"\"\" Returns: A ResNet50 model trained on ImageNet, if ```pretrained == True```. \"\"\" return resnet50 ( * args , ** kwargs )","title":"officehomeG()"},{"location":"docs/utils/","text":"","title":"utils"},{"location":"docs/validators/","text":"The following can be imported like this (using AccuracyValidator as an example): from pytorch_adapt.validators import AccuracyValidator Direct module members \u00b6 AccuracyValidator BaseValidator ClassClusterValidator DeepEmbeddedValidator DiversityValidator EntropyValidator ErrorValidator IMValidator ISTValidator KNNValidator MMDValidator MultipleValidators PerClassValidator SNDValidator ScoreHistories ScoreHistory TargetKNNValidator","title":"validators"},{"location":"docs/validators/#direct-module-members","text":"AccuracyValidator BaseValidator ClassClusterValidator DeepEmbeddedValidator DiversityValidator EntropyValidator ErrorValidator IMValidator ISTValidator KNNValidator MMDValidator MultipleValidators PerClassValidator SNDValidator ScoreHistories ScoreHistory TargetKNNValidator","title":"Direct module members"},{"location":"docs/validators/accuracy_validator/","text":"AccuracyValidator \u00b6 Bases: BaseValidator Returns accuracy using the torchmetrics accuracy function . The required dataset splits are [\"src_val\"] . This can be changed using key_map . Source code in pytorch_adapt\\validators\\accuracy_validator.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class AccuracyValidator ( BaseValidator ): \"\"\" Returns accuracy using the [torchmetrics accuracy function](https://torchmetrics.readthedocs.io/en/latest/references/functional.html#accuracy-func). The required dataset splits are ```[\"src_val\"]```. This can be changed using [```key_map```][pytorch_adapt.validators.BaseValidator.__init__]. \"\"\" def __init__ ( self , layer = \"preds\" , torchmetric_kwargs = None , ** kwargs ): \"\"\" Arguments: layer: The name of the layer to pass into the accuracy function. For example, ```\"preds\"``` refers to the softmaxed logits. torchmetric_kwargs: A dictionary of keyword arguments to be passed into the [torchmetrics accuracy function](https://torchmetrics.readthedocs.io/en/latest/references/functional.html#accuracy-func). \"\"\" super () . __init__ ( ** kwargs ) self . layer = layer self . torchmetric_kwargs = c_f . default ( torchmetric_kwargs , {}) self . accuracy_func = accuracy def compute_score ( self , src_val ): return self . accuracy_func ( src_val [ self . layer ], src_val [ \"labels\" ], ** self . torchmetric_kwargs ) . item () __init__ ( layer = 'preds' , torchmetric_kwargs = None , ** kwargs ) \u00b6 Parameters: Name Type Description Default layer The name of the layer to pass into the accuracy function. For example, \"preds\" refers to the softmaxed logits. 'preds' torchmetric_kwargs A dictionary of keyword arguments to be passed into the torchmetrics accuracy function . None Source code in pytorch_adapt\\validators\\accuracy_validator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 def __init__ ( self , layer = \"preds\" , torchmetric_kwargs = None , ** kwargs ): \"\"\" Arguments: layer: The name of the layer to pass into the accuracy function. For example, ```\"preds\"``` refers to the softmaxed logits. torchmetric_kwargs: A dictionary of keyword arguments to be passed into the [torchmetrics accuracy function](https://torchmetrics.readthedocs.io/en/latest/references/functional.html#accuracy-func). \"\"\" super () . __init__ ( ** kwargs ) self . layer = layer self . torchmetric_kwargs = c_f . default ( torchmetric_kwargs , {}) self . accuracy_func = accuracy","title":"accuracy_validator"},{"location":"docs/validators/accuracy_validator/#pytorch_adapt.validators.accuracy_validator.AccuracyValidator","text":"Bases: BaseValidator Returns accuracy using the torchmetrics accuracy function . The required dataset splits are [\"src_val\"] . This can be changed using key_map . Source code in pytorch_adapt\\validators\\accuracy_validator.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class AccuracyValidator ( BaseValidator ): \"\"\" Returns accuracy using the [torchmetrics accuracy function](https://torchmetrics.readthedocs.io/en/latest/references/functional.html#accuracy-func). The required dataset splits are ```[\"src_val\"]```. This can be changed using [```key_map```][pytorch_adapt.validators.BaseValidator.__init__]. \"\"\" def __init__ ( self , layer = \"preds\" , torchmetric_kwargs = None , ** kwargs ): \"\"\" Arguments: layer: The name of the layer to pass into the accuracy function. For example, ```\"preds\"``` refers to the softmaxed logits. torchmetric_kwargs: A dictionary of keyword arguments to be passed into the [torchmetrics accuracy function](https://torchmetrics.readthedocs.io/en/latest/references/functional.html#accuracy-func). \"\"\" super () . __init__ ( ** kwargs ) self . layer = layer self . torchmetric_kwargs = c_f . default ( torchmetric_kwargs , {}) self . accuracy_func = accuracy def compute_score ( self , src_val ): return self . accuracy_func ( src_val [ self . layer ], src_val [ \"labels\" ], ** self . torchmetric_kwargs ) . item ()","title":"AccuracyValidator"},{"location":"docs/validators/base_validator/","text":"BaseValidator \u00b6 Bases: ABC The parent class of all validators. The main purpose of validators is to give an estimate of target domain accuracy, usually without having access to class labels. Source code in pytorch_adapt\\validators\\base_validator.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class BaseValidator ( ABC ): \"\"\" The parent class of all validators. The main purpose of validators is to give an estimate of target domain accuracy, usually without having access to class labels. \"\"\" def __init__ ( self , key_map : Dict [ str , str ] = None ): \"\"\" Arguments: key_map: A mapping from ```<new_split_names>``` to ```<original_split_names>```. For example, [```AccuracyValidator```][pytorch_adapt.validators.AccuracyValidator] expects ```src_val``` by default. When used with one of the [```frameworks```](../frameworks/index.md), this default indicates that data related to the ```src_val``` split should be retrieved. If you instead want to compute accuracy for the ```src_train``` split, you would set the ```key_map``` to ```{\"src_train\": \"src_val\"}```. \"\"\" self . key_map = c_f . default ( key_map , {}) def _required_data ( self ): args = inspect . getfullargspec ( self . compute_score ) . args args . remove ( \"self\" ) return args @property def required_data ( self ) -> List [ str ]: \"\"\" Returns: A list of dataset split names. \"\"\" output = set ( self . _required_data ()) - set ( self . key_map . values ()) output = list ( output ) for k , v in self . key_map . items (): output . append ( k ) return output @abstractmethod def compute_score ( self ): pass def __call__ ( self , ** kwargs ) -> float : \"\"\" Arguments: **kwargs: A mapping from dataset split name to dictionaries containing: - ```\"features\"``` - ```\"logits\"``` - ```\"preds\"``` - ```\"domain\"``` - ```\"labels\"``` (if available) Returns: The validation score. \"\"\" kwargs = self . kwargs_check ( kwargs ) return self . compute_score ( ** kwargs ) def kwargs_check ( self , kwargs ): if kwargs . keys () != set ( self . required_data ): raise ValueError ( f \"Input to compute_score has keys = { kwargs . keys () } but should have keys { self . required_data } \" ) return c_f . map_keys ( kwargs , self . key_map ) def __repr__ ( self ): return c_f . nice_repr ( self , self . extra_repr (), {}) def extra_repr ( self ): return c_f . extra_repr ( self , [ \"required_data\" ]) __call__ ( ** kwargs ) \u00b6 Parameters: Name Type Description Default **kwargs A mapping from dataset split name to dictionaries containing: \"features\" \"logits\" \"preds\" \"domain\" \"labels\" (if available) {} Returns: Type Description float The validation score. Source code in pytorch_adapt\\validators\\base_validator.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def __call__ ( self , ** kwargs ) -> float : \"\"\" Arguments: **kwargs: A mapping from dataset split name to dictionaries containing: - ```\"features\"``` - ```\"logits\"``` - ```\"preds\"``` - ```\"domain\"``` - ```\"labels\"``` (if available) Returns: The validation score. \"\"\" kwargs = self . kwargs_check ( kwargs ) return self . compute_score ( ** kwargs ) __init__ ( key_map = None ) \u00b6 Parameters: Name Type Description Default key_map Dict [ str , str ] A mapping from <new_split_names> to <original_split_names> . For example, AccuracyValidator expects src_val by default. When used with one of the frameworks , this default indicates that data related to the src_val split should be retrieved. If you instead want to compute accuracy for the src_train split, you would set the key_map to {\"src_train\": \"src_val\"} . None Source code in pytorch_adapt\\validators\\base_validator.py 17 18 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , key_map : Dict [ str , str ] = None ): \"\"\" Arguments: key_map: A mapping from ```<new_split_names>``` to ```<original_split_names>```. For example, [```AccuracyValidator```][pytorch_adapt.validators.AccuracyValidator] expects ```src_val``` by default. When used with one of the [```frameworks```](../frameworks/index.md), this default indicates that data related to the ```src_val``` split should be retrieved. If you instead want to compute accuracy for the ```src_train``` split, you would set the ```key_map``` to ```{\"src_train\": \"src_val\"}```. \"\"\" self . key_map = c_f . default ( key_map , {}) required_data () property \u00b6 Returns: Type Description List [ str ] A list of dataset split names. Source code in pytorch_adapt\\validators\\base_validator.py 36 37 38 39 40 41 42 43 44 45 46 @property def required_data ( self ) -> List [ str ]: \"\"\" Returns: A list of dataset split names. \"\"\" output = set ( self . _required_data ()) - set ( self . key_map . values ()) output = list ( output ) for k , v in self . key_map . items (): output . append ( k ) return output","title":"base_validator"},{"location":"docs/validators/base_validator/#pytorch_adapt.validators.base_validator.BaseValidator","text":"Bases: ABC The parent class of all validators. The main purpose of validators is to give an estimate of target domain accuracy, usually without having access to class labels. Source code in pytorch_adapt\\validators\\base_validator.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class BaseValidator ( ABC ): \"\"\" The parent class of all validators. The main purpose of validators is to give an estimate of target domain accuracy, usually without having access to class labels. \"\"\" def __init__ ( self , key_map : Dict [ str , str ] = None ): \"\"\" Arguments: key_map: A mapping from ```<new_split_names>``` to ```<original_split_names>```. For example, [```AccuracyValidator```][pytorch_adapt.validators.AccuracyValidator] expects ```src_val``` by default. When used with one of the [```frameworks```](../frameworks/index.md), this default indicates that data related to the ```src_val``` split should be retrieved. If you instead want to compute accuracy for the ```src_train``` split, you would set the ```key_map``` to ```{\"src_train\": \"src_val\"}```. \"\"\" self . key_map = c_f . default ( key_map , {}) def _required_data ( self ): args = inspect . getfullargspec ( self . compute_score ) . args args . remove ( \"self\" ) return args @property def required_data ( self ) -> List [ str ]: \"\"\" Returns: A list of dataset split names. \"\"\" output = set ( self . _required_data ()) - set ( self . key_map . values ()) output = list ( output ) for k , v in self . key_map . items (): output . append ( k ) return output @abstractmethod def compute_score ( self ): pass def __call__ ( self , ** kwargs ) -> float : \"\"\" Arguments: **kwargs: A mapping from dataset split name to dictionaries containing: - ```\"features\"``` - ```\"logits\"``` - ```\"preds\"``` - ```\"domain\"``` - ```\"labels\"``` (if available) Returns: The validation score. \"\"\" kwargs = self . kwargs_check ( kwargs ) return self . compute_score ( ** kwargs ) def kwargs_check ( self , kwargs ): if kwargs . keys () != set ( self . required_data ): raise ValueError ( f \"Input to compute_score has keys = { kwargs . keys () } but should have keys { self . required_data } \" ) return c_f . map_keys ( kwargs , self . key_map ) def __repr__ ( self ): return c_f . nice_repr ( self , self . extra_repr (), {}) def extra_repr ( self ): return c_f . extra_repr ( self , [ \"required_data\" ])","title":"BaseValidator"},{"location":"docs/validators/deep_embedded_validator/","text":"DeepEmbeddedValidator \u00b6 Bases: BaseValidator Implementation of Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation Source code in pytorch_adapt\\validators\\deep_embedded_validator.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class DeepEmbeddedValidator ( BaseValidator ): \"\"\" Implementation of [Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation](http://proceedings.mlr.press/v97/you19a.html) \"\"\" def __init__ ( self , temp_folder , layer = \"features\" , num_workers = 0 , batch_size = 32 , error_fn = None , error_layer = \"logits\" , normalization = None , framework_fn = None , ** kwargs , ): super () . __init__ ( ** kwargs ) self . temp_folder = temp_folder self . layer = layer self . num_workers = num_workers self . batch_size = batch_size self . error_fn = c_f . default ( error_fn , torch . nn . CrossEntropyLoss ( reduction = \"none\" ) ) self . error_layer = error_layer check_normalization ( normalization ) self . normalization = normalization self . framework_fn = c_f . default ( framework_fn , default_framework_fn ) self . D_accuracy_val = None self . D_accuracy_test = None self . mean_error = None self . _DEV_recordable = [ \"D_accuracy_val\" , \"D_accuracy_test\" , \"mean_error\" ] pml_cf . add_to_recordable_attributes ( self , list_of_names = self . _DEV_recordable ) def compute_score ( self , src_train , src_val , target_train ): init_logging_level = c_f . LOGGER . level c_f . LOGGER . setLevel ( logging . WARNING ) weights , self . D_accuracy_val , self . D_accuracy_test = get_weights ( src_train [ self . layer ], src_val [ self . layer ], target_train [ self . layer ], self . num_workers , self . batch_size , self . temp_folder , self . framework_fn , ) error_per_sample = self . error_fn ( src_val [ self . error_layer ], src_val [ \"labels\" ]) output = get_dev_risk ( weights , error_per_sample [:, None ], self . normalization ) self . mean_error = torch . mean ( error_per_sample ) . item () c_f . LOGGER . setLevel ( init_logging_level ) return - output def extra_repr ( self ): x = super () . extra_repr () x += f \" \\n { c_f . extra_repr ( self , self . _DEV_recordable ) } \" return x","title":"deep_embedded_validator"},{"location":"docs/validators/deep_embedded_validator/#pytorch_adapt.validators.deep_embedded_validator.DeepEmbeddedValidator","text":"Bases: BaseValidator Implementation of Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation Source code in pytorch_adapt\\validators\\deep_embedded_validator.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class DeepEmbeddedValidator ( BaseValidator ): \"\"\" Implementation of [Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation](http://proceedings.mlr.press/v97/you19a.html) \"\"\" def __init__ ( self , temp_folder , layer = \"features\" , num_workers = 0 , batch_size = 32 , error_fn = None , error_layer = \"logits\" , normalization = None , framework_fn = None , ** kwargs , ): super () . __init__ ( ** kwargs ) self . temp_folder = temp_folder self . layer = layer self . num_workers = num_workers self . batch_size = batch_size self . error_fn = c_f . default ( error_fn , torch . nn . CrossEntropyLoss ( reduction = \"none\" ) ) self . error_layer = error_layer check_normalization ( normalization ) self . normalization = normalization self . framework_fn = c_f . default ( framework_fn , default_framework_fn ) self . D_accuracy_val = None self . D_accuracy_test = None self . mean_error = None self . _DEV_recordable = [ \"D_accuracy_val\" , \"D_accuracy_test\" , \"mean_error\" ] pml_cf . add_to_recordable_attributes ( self , list_of_names = self . _DEV_recordable ) def compute_score ( self , src_train , src_val , target_train ): init_logging_level = c_f . LOGGER . level c_f . LOGGER . setLevel ( logging . WARNING ) weights , self . D_accuracy_val , self . D_accuracy_test = get_weights ( src_train [ self . layer ], src_val [ self . layer ], target_train [ self . layer ], self . num_workers , self . batch_size , self . temp_folder , self . framework_fn , ) error_per_sample = self . error_fn ( src_val [ self . error_layer ], src_val [ \"labels\" ]) output = get_dev_risk ( weights , error_per_sample [:, None ], self . normalization ) self . mean_error = torch . mean ( error_per_sample ) . item () c_f . LOGGER . setLevel ( init_logging_level ) return - output def extra_repr ( self ): x = super () . extra_repr () x += f \" \\n { c_f . extra_repr ( self , self . _DEV_recordable ) } \" return x","title":"DeepEmbeddedValidator"},{"location":"docs/validators/diversity_validator/","text":"DiversityValidator \u00b6 Bases: BaseValidator Returns the negative of the diversity of all logits. Source code in pytorch_adapt\\validators\\diversity_validator.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class DiversityValidator ( BaseValidator ): \"\"\" Returns the negative of the [diversity][pytorch_adapt.layers.diversity_loss.DiversityLoss] of all logits. \"\"\" def __init__ ( self , layer = \"logits\" , ** kwargs ): super () . __init__ ( ** kwargs ) self . layer = layer self . loss_fn = DiversityLoss ( after_softmax = self . layer == \"preds\" ) def compute_score ( self , target_train ): return - self . loss_fn ( target_train [ self . layer ]) . item ()","title":"diversity_validator"},{"location":"docs/validators/diversity_validator/#pytorch_adapt.validators.diversity_validator.DiversityValidator","text":"Bases: BaseValidator Returns the negative of the diversity of all logits. Source code in pytorch_adapt\\validators\\diversity_validator.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class DiversityValidator ( BaseValidator ): \"\"\" Returns the negative of the [diversity][pytorch_adapt.layers.diversity_loss.DiversityLoss] of all logits. \"\"\" def __init__ ( self , layer = \"logits\" , ** kwargs ): super () . __init__ ( ** kwargs ) self . layer = layer self . loss_fn = DiversityLoss ( after_softmax = self . layer == \"preds\" ) def compute_score ( self , target_train ): return - self . loss_fn ( target_train [ self . layer ]) . item ()","title":"DiversityValidator"},{"location":"docs/validators/entropy_validator/","text":"EntropyValidator \u00b6 Bases: BaseValidator Returns the negative of the entropy of all logits. Source code in pytorch_adapt\\validators\\entropy_validator.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class EntropyValidator ( BaseValidator ): \"\"\" Returns the negative of the [entropy][pytorch_adapt.layers.entropy_loss.EntropyLoss] of all logits. \"\"\" def __init__ ( self , layer = \"logits\" , ** kwargs ): super () . __init__ ( ** kwargs ) self . layer = layer self . loss_fn = EntropyLoss ( after_softmax = self . layer == \"preds\" ) def compute_score ( self , target_train ): return - self . loss_fn ( target_train [ self . layer ]) . item ()","title":"entropy_validator"},{"location":"docs/validators/entropy_validator/#pytorch_adapt.validators.entropy_validator.EntropyValidator","text":"Bases: BaseValidator Returns the negative of the entropy of all logits. Source code in pytorch_adapt\\validators\\entropy_validator.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class EntropyValidator ( BaseValidator ): \"\"\" Returns the negative of the [entropy][pytorch_adapt.layers.entropy_loss.EntropyLoss] of all logits. \"\"\" def __init__ ( self , layer = \"logits\" , ** kwargs ): super () . __init__ ( ** kwargs ) self . layer = layer self . loss_fn = EntropyLoss ( after_softmax = self . layer == \"preds\" ) def compute_score ( self , target_train ): return - self . loss_fn ( target_train [ self . layer ]) . item ()","title":"EntropyValidator"},{"location":"docs/validators/error_validator/","text":"ErrorValidator \u00b6 Bases: AccuracyValidator Returns -(1-accuracy) Source code in pytorch_adapt\\validators\\error_validator.py 4 5 6 7 8 9 10 class ErrorValidator ( AccuracyValidator ): \"\"\" Returns ```-(1-accuracy)``` \"\"\" def compute_score ( self , src_val ): return - ( 1 - super () . compute_score ( src_val ))","title":"error_validator"},{"location":"docs/validators/error_validator/#pytorch_adapt.validators.error_validator.ErrorValidator","text":"Bases: AccuracyValidator Returns -(1-accuracy) Source code in pytorch_adapt\\validators\\error_validator.py 4 5 6 7 8 9 10 class ErrorValidator ( AccuracyValidator ): \"\"\" Returns ```-(1-accuracy)``` \"\"\" def compute_score ( self , src_val ): return - ( 1 - super () . compute_score ( src_val ))","title":"ErrorValidator"},{"location":"docs/validators/im_validator/","text":"IMValidator \u00b6 Bases: MultipleValidators The sum of EntropyValidator and DiversityValidator Source code in pytorch_adapt\\validators\\im_validator.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class IMValidator ( MultipleValidators ): \"\"\" The sum of [EntropyValidator][pytorch_adapt.validators.EntropyValidator] and [DiversityValidator][pytorch_adapt.validators.DiversityValidator] \"\"\" def __init__ ( self , weights = None , ** kwargs ): layer = kwargs . pop ( \"layer\" , None ) inner_kwargs = {} if not layer else { \"layer\" : layer } validators = { \"entropy\" : EntropyValidator ( ** inner_kwargs ), \"diversity\" : DiversityValidator ( ** inner_kwargs ), } super () . __init__ ( validators = validators , weights = weights , ** kwargs )","title":"im_validator"},{"location":"docs/validators/im_validator/#pytorch_adapt.validators.im_validator.IMValidator","text":"Bases: MultipleValidators The sum of EntropyValidator and DiversityValidator Source code in pytorch_adapt\\validators\\im_validator.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class IMValidator ( MultipleValidators ): \"\"\" The sum of [EntropyValidator][pytorch_adapt.validators.EntropyValidator] and [DiversityValidator][pytorch_adapt.validators.DiversityValidator] \"\"\" def __init__ ( self , weights = None , ** kwargs ): layer = kwargs . pop ( \"layer\" , None ) inner_kwargs = {} if not layer else { \"layer\" : layer } validators = { \"entropy\" : EntropyValidator ( ** inner_kwargs ), \"diversity\" : DiversityValidator ( ** inner_kwargs ), } super () . __init__ ( validators = validators , weights = weights , ** kwargs )","title":"IMValidator"},{"location":"docs/validators/multiple_validators/","text":"MultipleValidators \u00b6 Bases: BaseValidator Wraps multiple validators and returns the sum of their scores. Source code in pytorch_adapt\\validators\\multiple_validators.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class MultipleValidators ( BaseValidator ): \"\"\" Wraps multiple validators and returns the sum of their scores. \"\"\" def __init__ ( self , validators : Union [ List [ \"BaseValidator\" ], Dict [ str , \"BaseValidator\" ]], weights : Union [ List [ float ], Dict [ str , float ]] = None , return_sub_scores = False , ** kwargs , ): \"\"\" Arguments: validators: A list of validators or a dictionary mapping from strings to validators. weights: A list of floats or a dictionary mapping from validator names to floats. If ```None```, then the validators will be equally weighted. return_sub_scores: If ```True```, then return the score of each validator, in addition to their summed value. \"\"\" super () . __init__ ( ** kwargs ) self . validators = c_f . enumerate_to_dict ( validators ) self . weights = c_f . default ( weights , { k : 1 for k in self . validators . keys ()}) self . weights = c_f . enumerate_to_dict ( self . weights ) if self . validators . keys () != self . weights . keys (): raise KeyError ( \"validator keys and weight keys must be the same\" ) self . return_sub_scores = return_sub_scores pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"weights\" ]) def _required_data ( self ): output = [ v . required_data for v in self . validators . values ()] output = list ( itertools . chain ( * output )) return list ( set ( output )) def compute_score ( self ): pass def __call__ ( self , ** kwargs ) -> Union [ float , Tuple [ float , Dict [ str , float ]]]: \"\"\" Returns: The sum of the validator scores. If ```self.return_sub_scores``` then it also returns a dictionary containing each validator's weighted score. \"\"\" kwargs = self . kwargs_check ( kwargs ) outputs = {} for k , v in self . validators . items (): score = v ( ** c_f . filter_kwargs ( kwargs , v . required_data )) outputs [ k ] = score * self . weights [ k ] final = sum ( outputs . values ()) if self . return_sub_scores : return final , outputs return final def __repr__ ( self ): return c_f . nice_repr ( self , self . extra_repr (), self . validators ) def extra_repr ( self ): x = super () . extra_repr () x += f \" \\n { c_f . extra_repr ( self , [ 'weights' ]) } \" return x __call__ ( ** kwargs ) \u00b6 Returns: Type Description Union [ float , Tuple [ float , Dict [ str , float ]]] The sum of the validator scores. If self.return_sub_scores then Union [ float , Tuple [ float , Dict [ str , float ]]] it also returns a dictionary containing each validator's weighted score. Source code in pytorch_adapt\\validators\\multiple_validators.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def __call__ ( self , ** kwargs ) -> Union [ float , Tuple [ float , Dict [ str , float ]]]: \"\"\" Returns: The sum of the validator scores. If ```self.return_sub_scores``` then it also returns a dictionary containing each validator's weighted score. \"\"\" kwargs = self . kwargs_check ( kwargs ) outputs = {} for k , v in self . validators . items (): score = v ( ** c_f . filter_kwargs ( kwargs , v . required_data )) outputs [ k ] = score * self . weights [ k ] final = sum ( outputs . values ()) if self . return_sub_scores : return final , outputs return final __init__ ( validators , weights = None , return_sub_scores = False , ** kwargs ) \u00b6 Parameters: Name Type Description Default validators Union [ List ['BaseValidator'], Dict [ str , 'BaseValidator']] A list of validators or a dictionary mapping from strings to validators. required weights Union [ List [ float ], Dict [ str , float ]] A list of floats or a dictionary mapping from validator names to floats. If None , then the validators will be equally weighted. None return_sub_scores If True , then return the score of each validator, in addition to their summed value. False Source code in pytorch_adapt\\validators\\multiple_validators.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , validators : Union [ List [ \"BaseValidator\" ], Dict [ str , \"BaseValidator\" ]], weights : Union [ List [ float ], Dict [ str , float ]] = None , return_sub_scores = False , ** kwargs , ): \"\"\" Arguments: validators: A list of validators or a dictionary mapping from strings to validators. weights: A list of floats or a dictionary mapping from validator names to floats. If ```None```, then the validators will be equally weighted. return_sub_scores: If ```True```, then return the score of each validator, in addition to their summed value. \"\"\" super () . __init__ ( ** kwargs ) self . validators = c_f . enumerate_to_dict ( validators ) self . weights = c_f . default ( weights , { k : 1 for k in self . validators . keys ()}) self . weights = c_f . enumerate_to_dict ( self . weights ) if self . validators . keys () != self . weights . keys (): raise KeyError ( \"validator keys and weight keys must be the same\" ) self . return_sub_scores = return_sub_scores pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"weights\" ])","title":"multiple_validators"},{"location":"docs/validators/multiple_validators/#pytorch_adapt.validators.multiple_validators.MultipleValidators","text":"Bases: BaseValidator Wraps multiple validators and returns the sum of their scores. Source code in pytorch_adapt\\validators\\multiple_validators.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class MultipleValidators ( BaseValidator ): \"\"\" Wraps multiple validators and returns the sum of their scores. \"\"\" def __init__ ( self , validators : Union [ List [ \"BaseValidator\" ], Dict [ str , \"BaseValidator\" ]], weights : Union [ List [ float ], Dict [ str , float ]] = None , return_sub_scores = False , ** kwargs , ): \"\"\" Arguments: validators: A list of validators or a dictionary mapping from strings to validators. weights: A list of floats or a dictionary mapping from validator names to floats. If ```None```, then the validators will be equally weighted. return_sub_scores: If ```True```, then return the score of each validator, in addition to their summed value. \"\"\" super () . __init__ ( ** kwargs ) self . validators = c_f . enumerate_to_dict ( validators ) self . weights = c_f . default ( weights , { k : 1 for k in self . validators . keys ()}) self . weights = c_f . enumerate_to_dict ( self . weights ) if self . validators . keys () != self . weights . keys (): raise KeyError ( \"validator keys and weight keys must be the same\" ) self . return_sub_scores = return_sub_scores pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"weights\" ]) def _required_data ( self ): output = [ v . required_data for v in self . validators . values ()] output = list ( itertools . chain ( * output )) return list ( set ( output )) def compute_score ( self ): pass def __call__ ( self , ** kwargs ) -> Union [ float , Tuple [ float , Dict [ str , float ]]]: \"\"\" Returns: The sum of the validator scores. If ```self.return_sub_scores``` then it also returns a dictionary containing each validator's weighted score. \"\"\" kwargs = self . kwargs_check ( kwargs ) outputs = {} for k , v in self . validators . items (): score = v ( ** c_f . filter_kwargs ( kwargs , v . required_data )) outputs [ k ] = score * self . weights [ k ] final = sum ( outputs . values ()) if self . return_sub_scores : return final , outputs return final def __repr__ ( self ): return c_f . nice_repr ( self , self . extra_repr (), self . validators ) def extra_repr ( self ): x = super () . extra_repr () x += f \" \\n { c_f . extra_repr ( self , [ 'weights' ]) } \" return x","title":"MultipleValidators"},{"location":"docs/validators/score_history/","text":"ScoreHistories \u00b6 Bases: ScoreHistory Like ScoreHistory , but it wraps a MultipleValidators object and keeps track of sub-validator scores histories. Source code in pytorch_adapt\\validators\\score_history.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 class ScoreHistories ( ScoreHistory ): \"\"\" Like [ScoreHistory][pytorch_adapt.validators.ScoreHistory], but it wraps a [MultipleValidators][pytorch_adapt.validators.MultipleValidators] object and keeps track of sub-validator scores histories. \"\"\" def __init__ ( self , validator , ** kwargs ): super () . __init__ ( validator = validator , ** kwargs ) if not isinstance ( validator , MultipleValidators ): raise TypeError ( \"validator must be of type MultipleValidators\" ) validator . return_sub_scores = True self . histories = { k : ScoreHistory ( v ) for k , v in validator . validators . items ()} pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"histories\" ]) def __call__ ( self , epoch : int , ** kwargs : Dict [ str , torch . Tensor ]) -> float : score , sub_scores = super () . __call__ ( epoch , ** kwargs ) for k , v in self . histories . items (): v . append_to_history_and_normalize ( sub_scores [ k ], epoch ) return score def extra_repr ( self ): x = super () . extra_repr () x += f \" \\n { c_f . extra_repr ( self , [ 'histories' ]) } \" return x def state_dict ( self ): output = super () . state_dict () output . update ( { \"histories\" : { k : v . state_dict () for k , v in self . histories . items ()}} ) return output def load_state_dict ( self , state_dict ): histories = state_dict . pop ( \"histories\" ) super () . load_state_dict ( state_dict ) c_f . assert_state_dict_keys ( histories , self . histories . keys ()) for k , v in histories . items (): self . histories [ k ] . load_state_dict ( v ) ScoreHistory \u00b6 Bases: ABC Wraps a validator and keeps track of the validator's score history as the epochs progress. Source code in pytorch_adapt\\validators\\score_history.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 class ScoreHistory ( ABC ): \"\"\" Wraps a [validator][pytorch_adapt.validators.BaseValidator] and keeps track of the validator's score history as the epochs progress. \"\"\" def __init__ ( self , validator , normalizer : Callable [[ np . ndarray ], np . ndarray ] = None , ignore_epoch : int = None , ): \"\"\" Arguments: normalizer: A function that receives the current unnormalized score history, and returns a normalized version of the score history. If ```None```, then it defaults to no normalization. ignore_epoch: This epoch will ignored when determining the best scoring epoch. If ```None```, then no epoch will be ignored. \"\"\" self . validator = validator self . normalizer = c_f . default ( normalizer , return_raw ) self . score_history = np . array ([]) self . raw_score_history = np . array ([]) self . epochs = np . array ([], dtype = int ) self . ignore_epoch = ignore_epoch pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"latest_score\" , \"best_score\" , \"latest_epoch\" , \"best_epoch\" ], ) def __call__ ( self , epoch : int , ** kwargs : Dict [ str , torch . Tensor ]) -> float : \"\"\" Arguments: epoch: The epoch to be scored. **kwargs: Keyword arguments that get passed into the wrapped validator's [```__call__```][pytorch_adapt.validators.BaseValidator.__call__] method. \"\"\" if epoch in self . epochs : raise ValueError ( f \"Epoch { epoch } has already been evaluated\" ) score = self . validator ( ** kwargs ) sub_scores = None if isinstance ( score , ( list , tuple )): score , sub_scores = score self . append_to_history_and_normalize ( score , epoch ) if sub_scores : return self . latest_score , sub_scores return self . latest_score def append_to_history_and_normalize ( self , score , epoch ): self . raw_score_history = np . append ( self . raw_score_history , score ) self . epochs = np . append ( self . epochs , epoch ) self . score_history = self . normalizer ( self . raw_score_history ) @property def score_history_ignore_epoch ( self ): return remove_ignore_epoch ( self . score_history , self . epochs , self . ignore_epoch ) @property def epochs_ignore_epoch ( self ): return remove_ignore_epoch ( self . epochs , self . epochs , self . ignore_epoch ) def has_valid_history ( self , ignore_ignore_epoch = True ): x = ( self . score_history_ignore_epoch if ignore_ignore_epoch else self . score_history ) return len ( x ) > 0 and np . isfinite ( x ) . any () @property def best_score ( self ) -> float : \"\"\" Returns: The best score, ignoring ```self.ignore_epoch```. Returns ```None``` if no valid scores are available. \"\"\" if self . has_valid_history (): return self . score_history_ignore_epoch [ self . best_idx ] @property def best_epoch ( self ) -> int : \"\"\" Returns: The best epoch, ignoring ```self.ignore_epoch```. Returns ```None``` if no valid epochs are available. \"\"\" if self . has_valid_history (): return self . epochs_ignore_epoch [ self . best_idx ] @property def best_idx ( self ) -> int : \"\"\" Returns: The index of the best score in ```self.score_history_ignore_epoch```. Returns ```None``` if no valid epochs are available. \"\"\" if self . has_valid_history (): return np . nanargmax ( self . score_history_ignore_epoch ) @property def latest_epoch ( self ) -> int : \"\"\" Returns: The latest epoch, including ```self.ignore_epoch```. Returns ```None``` if no epochs have been scored. \"\"\" if self . has_valid_history ( False ): return self . epochs [ - 1 ] @property def latest_score ( self ) -> float : \"\"\" Returns: The latest score, including ```self.ignore_epoch```. Returns ```None``` if no epochs have been scored. \"\"\" if self . has_valid_history ( False ): return self . score_history [ - 1 ] @property def latest_is_best ( self ) -> bool : \"\"\" Returns: ```False``` if the latest epoch was not the best scoring epoch, or if the latest epoch is ```ignore_epoch```. ```True``` otherwise. \"\"\" if self . has_valid_history ( False ): return self . best_epoch == self . latest_epoch return False @property def required_data ( self ): return self . validator . required_data def __repr__ ( self ): return c_f . nice_repr ( self , self . extra_repr (), {}) def extra_repr ( self ): return c_f . extra_repr ( self , [ \"validator\" , \"latest_score\" , \"best_score\" , \"best_epoch\" ] ) def state_dict ( self ): return { k : getattr ( self , k ) for k in state_dict_keys ()} def load_state_dict ( self , state_dict ): c_f . assert_state_dict_keys ( state_dict , set ( state_dict_keys ())) for k , v in state_dict . items (): if not isinstance ( getattr ( self . __class__ , k , None ), property ): setattr ( self , k , v ) __call__ ( epoch , ** kwargs ) \u00b6 Parameters: Name Type Description Default epoch int The epoch to be scored. required **kwargs Dict [ str , torch . Tensor ] Keyword arguments that get passed into the wrapped validator's __call__ method. {} Source code in pytorch_adapt\\validators\\score_history.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __call__ ( self , epoch : int , ** kwargs : Dict [ str , torch . Tensor ]) -> float : \"\"\" Arguments: epoch: The epoch to be scored. **kwargs: Keyword arguments that get passed into the wrapped validator's [```__call__```][pytorch_adapt.validators.BaseValidator.__call__] method. \"\"\" if epoch in self . epochs : raise ValueError ( f \"Epoch { epoch } has already been evaluated\" ) score = self . validator ( ** kwargs ) sub_scores = None if isinstance ( score , ( list , tuple )): score , sub_scores = score self . append_to_history_and_normalize ( score , epoch ) if sub_scores : return self . latest_score , sub_scores return self . latest_score __init__ ( validator , normalizer = None , ignore_epoch = None ) \u00b6 Parameters: Name Type Description Default normalizer Callable [[ np . ndarray ], np . ndarray ] A function that receives the current unnormalized score history, and returns a normalized version of the score history. If None , then it defaults to no normalization. None ignore_epoch int This epoch will ignored when determining the best scoring epoch. If None , then no epoch will be ignored. None Source code in pytorch_adapt\\validators\\score_history.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , validator , normalizer : Callable [[ np . ndarray ], np . ndarray ] = None , ignore_epoch : int = None , ): \"\"\" Arguments: normalizer: A function that receives the current unnormalized score history, and returns a normalized version of the score history. If ```None```, then it defaults to no normalization. ignore_epoch: This epoch will ignored when determining the best scoring epoch. If ```None```, then no epoch will be ignored. \"\"\" self . validator = validator self . normalizer = c_f . default ( normalizer , return_raw ) self . score_history = np . array ([]) self . raw_score_history = np . array ([]) self . epochs = np . array ([], dtype = int ) self . ignore_epoch = ignore_epoch pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"latest_score\" , \"best_score\" , \"latest_epoch\" , \"best_epoch\" ], ) best_epoch () property \u00b6 Returns: Type Description int The best epoch, ignoring self.ignore_epoch . int Returns None if no valid epochs are available. Source code in pytorch_adapt\\validators\\score_history.py 93 94 95 96 97 98 99 100 101 @property def best_epoch ( self ) -> int : \"\"\" Returns: The best epoch, ignoring ```self.ignore_epoch```. Returns ```None``` if no valid epochs are available. \"\"\" if self . has_valid_history (): return self . epochs_ignore_epoch [ self . best_idx ] best_idx () property \u00b6 Returns: Type Description int The index of the best score in self.score_history_ignore_epoch . int Returns None if no valid epochs are available. Source code in pytorch_adapt\\validators\\score_history.py 103 104 105 106 107 108 109 110 111 @property def best_idx ( self ) -> int : \"\"\" Returns: The index of the best score in ```self.score_history_ignore_epoch```. Returns ```None``` if no valid epochs are available. \"\"\" if self . has_valid_history (): return np . nanargmax ( self . score_history_ignore_epoch ) best_score () property \u00b6 Returns: Type Description float The best score, ignoring self.ignore_epoch . float Returns None if no valid scores are available. Source code in pytorch_adapt\\validators\\score_history.py 83 84 85 86 87 88 89 90 91 @property def best_score ( self ) -> float : \"\"\" Returns: The best score, ignoring ```self.ignore_epoch```. Returns ```None``` if no valid scores are available. \"\"\" if self . has_valid_history (): return self . score_history_ignore_epoch [ self . best_idx ] latest_epoch () property \u00b6 Returns: Type Description int The latest epoch, including self.ignore_epoch . int Returns None if no epochs have been scored. Source code in pytorch_adapt\\validators\\score_history.py 113 114 115 116 117 118 119 120 121 @property def latest_epoch ( self ) -> int : \"\"\" Returns: The latest epoch, including ```self.ignore_epoch```. Returns ```None``` if no epochs have been scored. \"\"\" if self . has_valid_history ( False ): return self . epochs [ - 1 ] latest_is_best () property \u00b6 Returns: Type Description bool False if the latest epoch was not the best bool scoring epoch, or if the latest epoch is ignore_epoch . bool True otherwise. Source code in pytorch_adapt\\validators\\score_history.py 133 134 135 136 137 138 139 140 141 142 143 @property def latest_is_best ( self ) -> bool : \"\"\" Returns: ```False``` if the latest epoch was not the best scoring epoch, or if the latest epoch is ```ignore_epoch```. ```True``` otherwise. \"\"\" if self . has_valid_history ( False ): return self . best_epoch == self . latest_epoch return False latest_score () property \u00b6 Returns: Type Description float The latest score, including self.ignore_epoch . float Returns None if no epochs have been scored. Source code in pytorch_adapt\\validators\\score_history.py 123 124 125 126 127 128 129 130 131 @property def latest_score ( self ) -> float : \"\"\" Returns: The latest score, including ```self.ignore_epoch```. Returns ```None``` if no epochs have been scored. \"\"\" if self . has_valid_history ( False ): return self . score_history [ - 1 ]","title":"score_history"},{"location":"docs/validators/score_history/#pytorch_adapt.validators.score_history.ScoreHistories","text":"Bases: ScoreHistory Like ScoreHistory , but it wraps a MultipleValidators object and keeps track of sub-validator scores histories. Source code in pytorch_adapt\\validators\\score_history.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 class ScoreHistories ( ScoreHistory ): \"\"\" Like [ScoreHistory][pytorch_adapt.validators.ScoreHistory], but it wraps a [MultipleValidators][pytorch_adapt.validators.MultipleValidators] object and keeps track of sub-validator scores histories. \"\"\" def __init__ ( self , validator , ** kwargs ): super () . __init__ ( validator = validator , ** kwargs ) if not isinstance ( validator , MultipleValidators ): raise TypeError ( \"validator must be of type MultipleValidators\" ) validator . return_sub_scores = True self . histories = { k : ScoreHistory ( v ) for k , v in validator . validators . items ()} pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"histories\" ]) def __call__ ( self , epoch : int , ** kwargs : Dict [ str , torch . Tensor ]) -> float : score , sub_scores = super () . __call__ ( epoch , ** kwargs ) for k , v in self . histories . items (): v . append_to_history_and_normalize ( sub_scores [ k ], epoch ) return score def extra_repr ( self ): x = super () . extra_repr () x += f \" \\n { c_f . extra_repr ( self , [ 'histories' ]) } \" return x def state_dict ( self ): output = super () . state_dict () output . update ( { \"histories\" : { k : v . state_dict () for k , v in self . histories . items ()}} ) return output def load_state_dict ( self , state_dict ): histories = state_dict . pop ( \"histories\" ) super () . load_state_dict ( state_dict ) c_f . assert_state_dict_keys ( histories , self . histories . keys ()) for k , v in histories . items (): self . histories [ k ] . load_state_dict ( v )","title":"ScoreHistories"},{"location":"docs/validators/score_history/#pytorch_adapt.validators.score_history.ScoreHistory","text":"Bases: ABC Wraps a validator and keeps track of the validator's score history as the epochs progress. Source code in pytorch_adapt\\validators\\score_history.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 class ScoreHistory ( ABC ): \"\"\" Wraps a [validator][pytorch_adapt.validators.BaseValidator] and keeps track of the validator's score history as the epochs progress. \"\"\" def __init__ ( self , validator , normalizer : Callable [[ np . ndarray ], np . ndarray ] = None , ignore_epoch : int = None , ): \"\"\" Arguments: normalizer: A function that receives the current unnormalized score history, and returns a normalized version of the score history. If ```None```, then it defaults to no normalization. ignore_epoch: This epoch will ignored when determining the best scoring epoch. If ```None```, then no epoch will be ignored. \"\"\" self . validator = validator self . normalizer = c_f . default ( normalizer , return_raw ) self . score_history = np . array ([]) self . raw_score_history = np . array ([]) self . epochs = np . array ([], dtype = int ) self . ignore_epoch = ignore_epoch pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"latest_score\" , \"best_score\" , \"latest_epoch\" , \"best_epoch\" ], ) def __call__ ( self , epoch : int , ** kwargs : Dict [ str , torch . Tensor ]) -> float : \"\"\" Arguments: epoch: The epoch to be scored. **kwargs: Keyword arguments that get passed into the wrapped validator's [```__call__```][pytorch_adapt.validators.BaseValidator.__call__] method. \"\"\" if epoch in self . epochs : raise ValueError ( f \"Epoch { epoch } has already been evaluated\" ) score = self . validator ( ** kwargs ) sub_scores = None if isinstance ( score , ( list , tuple )): score , sub_scores = score self . append_to_history_and_normalize ( score , epoch ) if sub_scores : return self . latest_score , sub_scores return self . latest_score def append_to_history_and_normalize ( self , score , epoch ): self . raw_score_history = np . append ( self . raw_score_history , score ) self . epochs = np . append ( self . epochs , epoch ) self . score_history = self . normalizer ( self . raw_score_history ) @property def score_history_ignore_epoch ( self ): return remove_ignore_epoch ( self . score_history , self . epochs , self . ignore_epoch ) @property def epochs_ignore_epoch ( self ): return remove_ignore_epoch ( self . epochs , self . epochs , self . ignore_epoch ) def has_valid_history ( self , ignore_ignore_epoch = True ): x = ( self . score_history_ignore_epoch if ignore_ignore_epoch else self . score_history ) return len ( x ) > 0 and np . isfinite ( x ) . any () @property def best_score ( self ) -> float : \"\"\" Returns: The best score, ignoring ```self.ignore_epoch```. Returns ```None``` if no valid scores are available. \"\"\" if self . has_valid_history (): return self . score_history_ignore_epoch [ self . best_idx ] @property def best_epoch ( self ) -> int : \"\"\" Returns: The best epoch, ignoring ```self.ignore_epoch```. Returns ```None``` if no valid epochs are available. \"\"\" if self . has_valid_history (): return self . epochs_ignore_epoch [ self . best_idx ] @property def best_idx ( self ) -> int : \"\"\" Returns: The index of the best score in ```self.score_history_ignore_epoch```. Returns ```None``` if no valid epochs are available. \"\"\" if self . has_valid_history (): return np . nanargmax ( self . score_history_ignore_epoch ) @property def latest_epoch ( self ) -> int : \"\"\" Returns: The latest epoch, including ```self.ignore_epoch```. Returns ```None``` if no epochs have been scored. \"\"\" if self . has_valid_history ( False ): return self . epochs [ - 1 ] @property def latest_score ( self ) -> float : \"\"\" Returns: The latest score, including ```self.ignore_epoch```. Returns ```None``` if no epochs have been scored. \"\"\" if self . has_valid_history ( False ): return self . score_history [ - 1 ] @property def latest_is_best ( self ) -> bool : \"\"\" Returns: ```False``` if the latest epoch was not the best scoring epoch, or if the latest epoch is ```ignore_epoch```. ```True``` otherwise. \"\"\" if self . has_valid_history ( False ): return self . best_epoch == self . latest_epoch return False @property def required_data ( self ): return self . validator . required_data def __repr__ ( self ): return c_f . nice_repr ( self , self . extra_repr (), {}) def extra_repr ( self ): return c_f . extra_repr ( self , [ \"validator\" , \"latest_score\" , \"best_score\" , \"best_epoch\" ] ) def state_dict ( self ): return { k : getattr ( self , k ) for k in state_dict_keys ()} def load_state_dict ( self , state_dict ): c_f . assert_state_dict_keys ( state_dict , set ( state_dict_keys ())) for k , v in state_dict . items (): if not isinstance ( getattr ( self . __class__ , k , None ), property ): setattr ( self , k , v )","title":"ScoreHistory"},{"location":"docs/validators/snd_validator/","text":"SNDValidator \u00b6 Bases: BaseValidator Implementation of Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density Source code in pytorch_adapt\\validators\\snd_validator.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class SNDValidator ( BaseValidator ): \"\"\" Implementation of [Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density](https://arxiv.org/abs/2108.10860) \"\"\" def __init__ ( self , layer = \"preds\" , T = 0.05 , batch_size = 1024 , ** kwargs ): super () . __init__ ( ** kwargs ) self . layer = layer self . T = T self . entropy_fn = EntropyLoss ( after_softmax = True , return_mean = False ) self . dist_fn = BatchedDistance ( CosineSimilarity (), batch_size = batch_size ) def compute_score ( self , target_train ): features = target_train [ self . layer ] # all_entropies is modified via self.iter_fn all_entropies = [] self . dist_fn . iter_fn = get_iter_fn ( all_entropies , self . entropy_fn , self . T ) self . dist_fn ( features ) all_entropies = torch . cat ( all_entropies , dim = 0 ) if len ( all_entropies ) != len ( features ): raise ValueError ( \"all_entropies should have same length as input features\" ) return torch . mean ( all_entropies ) . item ()","title":"snd_validator"},{"location":"docs/validators/snd_validator/#pytorch_adapt.validators.snd_validator.SNDValidator","text":"Bases: BaseValidator Implementation of Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density Source code in pytorch_adapt\\validators\\snd_validator.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class SNDValidator ( BaseValidator ): \"\"\" Implementation of [Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density](https://arxiv.org/abs/2108.10860) \"\"\" def __init__ ( self , layer = \"preds\" , T = 0.05 , batch_size = 1024 , ** kwargs ): super () . __init__ ( ** kwargs ) self . layer = layer self . T = T self . entropy_fn = EntropyLoss ( after_softmax = True , return_mean = False ) self . dist_fn = BatchedDistance ( CosineSimilarity (), batch_size = batch_size ) def compute_score ( self , target_train ): features = target_train [ self . layer ] # all_entropies is modified via self.iter_fn all_entropies = [] self . dist_fn . iter_fn = get_iter_fn ( all_entropies , self . entropy_fn , self . T ) self . dist_fn ( features ) all_entropies = torch . cat ( all_entropies , dim = 0 ) if len ( all_entropies ) != len ( features ): raise ValueError ( \"all_entropies should have same length as input features\" ) return torch . mean ( all_entropies ) . item ()","title":"SNDValidator"},{"location":"docs/weighters/","text":"The following can be imported like this (using BaseWeighter as an example): from pytorch_adapt.weighters import BaseWeighter Direct module members \u00b6 BaseWeighter MeanWeighter SumWeighter","title":"weighters"},{"location":"docs/weighters/#direct-module-members","text":"BaseWeighter MeanWeighter SumWeighter","title":"Direct module members"},{"location":"docs/weighters/base_weighter/","text":"BaseWeighter \u00b6 Multiplies losses by scalar values, and then reduces them to a single value. Source code in pytorch_adapt\\weighters\\base_weighter.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class BaseWeighter : \"\"\" Multiplies losses by scalar values, and then reduces them to a single value. \"\"\" def __init__ ( self , reduction : Callable [[ List [ torch . Tensor ]], torch . Tensor ], weights : Dict [ str , float ] = None , scale : float = 1 , ): \"\"\" Arguments: reduction: A function that takes in a list of losses and returns a single loss value. weights: A mapping from loss names to weight values. If ```None```, weights are assumed to be 1. scale: A scalar that every loss gets multiplied by. \"\"\" self . reduction = reduction self . weights = c_f . default ( weights , {}) self . scale = scale pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"weights\" , \"scale\" ]) def __call__ ( self , loss_dict : Dict [ str , torch . Tensor ] ) -> Tuple [ torch . Tensor , Dict [ str , float ]]: \"\"\" Arguments: loss_dict: A mapping from loss names to loss values. Returns: A tuple consisting of - the loss that .backward() can be called on - a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. \"\"\" return weight_losses ( self . reduction , self . weights , self . scale , loss_dict ) def __repr__ ( self ): return c_f . nice_repr ( self , c_f . extra_repr ( self , [ \"weights\" , \"scale\" ]), {}) __call__ ( loss_dict ) \u00b6 Parameters: Name Type Description Default loss_dict Dict [ str , torch . Tensor ] A mapping from loss names to loss values. required Returns: Type Description Tuple [ torch . Tensor , Dict [ str , float ]] A tuple consisting of the loss that .backward() can be called on a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. Source code in pytorch_adapt\\weighters\\base_weighter.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __call__ ( self , loss_dict : Dict [ str , torch . Tensor ] ) -> Tuple [ torch . Tensor , Dict [ str , float ]]: \"\"\" Arguments: loss_dict: A mapping from loss names to loss values. Returns: A tuple consisting of - the loss that .backward() can be called on - a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. \"\"\" return weight_losses ( self . reduction , self . weights , self . scale , loss_dict ) __init__ ( reduction , weights = None , scale = 1 ) \u00b6 Parameters: Name Type Description Default reduction Callable [[ List [ torch . Tensor ]], torch . Tensor ] A function that takes in a list of losses and returns a single loss value. required weights Dict [ str , float ] A mapping from loss names to weight values. If None , weights are assumed to be 1. None scale float A scalar that every loss gets multiplied by. 1 Source code in pytorch_adapt\\weighters\\base_weighter.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , reduction : Callable [[ List [ torch . Tensor ]], torch . Tensor ], weights : Dict [ str , float ] = None , scale : float = 1 , ): \"\"\" Arguments: reduction: A function that takes in a list of losses and returns a single loss value. weights: A mapping from loss names to weight values. If ```None```, weights are assumed to be 1. scale: A scalar that every loss gets multiplied by. \"\"\" self . reduction = reduction self . weights = c_f . default ( weights , {}) self . scale = scale pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"weights\" , \"scale\" ])","title":"base_weighter"},{"location":"docs/weighters/base_weighter/#pytorch_adapt.weighters.base_weighter.BaseWeighter","text":"Multiplies losses by scalar values, and then reduces them to a single value. Source code in pytorch_adapt\\weighters\\base_weighter.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class BaseWeighter : \"\"\" Multiplies losses by scalar values, and then reduces them to a single value. \"\"\" def __init__ ( self , reduction : Callable [[ List [ torch . Tensor ]], torch . Tensor ], weights : Dict [ str , float ] = None , scale : float = 1 , ): \"\"\" Arguments: reduction: A function that takes in a list of losses and returns a single loss value. weights: A mapping from loss names to weight values. If ```None```, weights are assumed to be 1. scale: A scalar that every loss gets multiplied by. \"\"\" self . reduction = reduction self . weights = c_f . default ( weights , {}) self . scale = scale pml_cf . add_to_recordable_attributes ( self , list_of_names = [ \"weights\" , \"scale\" ]) def __call__ ( self , loss_dict : Dict [ str , torch . Tensor ] ) -> Tuple [ torch . Tensor , Dict [ str , float ]]: \"\"\" Arguments: loss_dict: A mapping from loss names to loss values. Returns: A tuple consisting of - the loss that .backward() can be called on - a dictionary of floats (detached from the autograd graph) that contains the weighted loss components. \"\"\" return weight_losses ( self . reduction , self . weights , self . scale , loss_dict ) def __repr__ ( self ): return c_f . nice_repr ( self , c_f . extra_repr ( self , [ \"weights\" , \"scale\" ]), {})","title":"BaseWeighter"},{"location":"docs/weighters/mean_weighter/","text":"MeanWeighter \u00b6 Bases: BaseWeighter Weights the losses and then returns the mean of the weighted losses. Source code in pytorch_adapt\\weighters\\mean_weighter.py 12 13 14 15 16 17 18 class MeanWeighter ( BaseWeighter ): \"\"\" Weights the losses and then returns the **mean** of the weighted losses. \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( reduction = mean , ** kwargs )","title":"mean_weighter"},{"location":"docs/weighters/mean_weighter/#pytorch_adapt.weighters.mean_weighter.MeanWeighter","text":"Bases: BaseWeighter Weights the losses and then returns the mean of the weighted losses. Source code in pytorch_adapt\\weighters\\mean_weighter.py 12 13 14 15 16 17 18 class MeanWeighter ( BaseWeighter ): \"\"\" Weights the losses and then returns the **mean** of the weighted losses. \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( reduction = mean , ** kwargs )","title":"MeanWeighter"},{"location":"docs/weighters/sum_weighter/","text":"SumWeighter \u00b6 Bases: BaseWeighter Weights the losses and then returns the sum of the weighted losses. Source code in pytorch_adapt\\weighters\\sum_weighter.py 4 5 6 7 8 9 10 class SumWeighter ( BaseWeighter ): \"\"\" Weights the losses and then returns the **sum** of the weighted losses. \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( reduction = sum , ** kwargs )","title":"sum_weighter"},{"location":"docs/weighters/sum_weighter/#pytorch_adapt.weighters.sum_weighter.SumWeighter","text":"Bases: BaseWeighter Weights the losses and then returns the sum of the weighted losses. Source code in pytorch_adapt\\weighters\\sum_weighter.py 4 5 6 7 8 9 10 class SumWeighter ( BaseWeighter ): \"\"\" Weights the losses and then returns the **sum** of the weighted losses. \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( reduction = sum , ** kwargs )","title":"SumWeighter"}]}